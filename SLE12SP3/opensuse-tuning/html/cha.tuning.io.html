<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Tuning I/O Performance | System Analysis and Tuning Guide | openSUSE Leap 42.3</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><meta name="product-name" content="openSUSE Leap" /><meta name="product-number" content="42.3" /><meta name="book-title" content="System Analysis and Tuning Guide" /><meta name="chapter-title" content="Chapter 12. Tuning I/O Performance" /><meta name="tracker-url" content="https://bugzilla.opensuse.org/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="fs@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="openSUSE Distribution" /><meta name="tracker-bsc-version" content="Leap 42.2" /><link rel="home" href="index.html" title="openSUSE Leap Documentation" /><link rel="up" href="part.tuning.kernel.html" title="Part V. Kernel Tuning" /><link rel="prev" href="part.tuning.kernel.html" title="Part V. Kernel Tuning" /><link rel="next" href="cha.tuning.taskscheduler.html" title="Chapter 13. Tuning the Task Scheduler" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="openSUSE Leap Documentation"><span class="book-icon">openSUSE Leap Documentation</span></a><span> › </span><a class="crumb" href="book.sle.tuning.html">System Analysis and Tuning Guide</a><span> › </span><a class="crumb" href="part.tuning.kernel.html">Kernel Tuning</a><span> › </span><a class="crumb" href="cha.tuning.io.html">Tuning I/O Performance</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>System Analysis and Tuning Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="preface.tuning.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="part.tuning.basics.html"><span class="number">I </span><span class="name">Basics</span></a><ol><li class="inactive"><a href="cha.tuning.basics.html"><span class="number">1 </span><span class="name">General Notes on System Tuning</span></a></li></ol></li><li class="inactive"><a href="part.tuning.monitoring.html"><span class="number">II </span><span class="name">System Monitoring</span></a><ol><li class="inactive"><a href="cha.util.html"><span class="number">2 </span><span class="name">System Monitoring Utilities</span></a></li><li class="inactive"><a href="cha.tuning.logfiles.html"><span class="number">3 </span><span class="name">Analyzing and Managing System Log Files</span></a></li></ol></li><li class="inactive"><a href="part.tuning.kerneltrace.html"><span class="number">III </span><span class="name">Kernel Monitoring</span></a><ol><li class="inactive"><a href="cha.tuning.systemtap.html"><span class="number">4 </span><span class="name">SystemTap—Filtering and Analyzing System Data</span></a></li><li class="inactive"><a href="cha.tuning.kprobes.html"><span class="number">5 </span><span class="name">Kernel Probes</span></a></li><li class="inactive"><a href="cha.perf.html"><span class="number">6 </span><span class="name">Hardware-Based Performance Monitoring with Perf</span></a></li><li class="inactive"><a href="cha.tuning.oprofile.html"><span class="number">7 </span><span class="name">OProfile—System-Wide Profiler</span></a></li></ol></li><li class="inactive"><a href="part.tuning.resources.html"><span class="number">IV </span><span class="name">Resource Management</span></a><ol><li class="inactive"><a href="cha.tuning.resources.html"><span class="number">8 </span><span class="name">General System Resource Management</span></a></li><li class="inactive"><a href="cha.tuning.cgroups.html"><span class="number">9 </span><span class="name">Kernel Control Groups</span></a></li><li class="inactive"><a href="cha.tuning.numactl.html"><span class="number">10 </span><span class="name">Automatic Non-Uniform Memory Access (NUMA) Balancing</span></a></li><li class="inactive"><a href="cha.tuning.power.html"><span class="number">11 </span><span class="name">Power Management</span></a></li></ol></li><li class="inactive"><a href="part.tuning.kernel.html"><span class="number">V </span><span class="name">Kernel Tuning</span></a><ol><li class="inactive"><a href="cha.tuning.io.html"><span class="number">12 </span><span class="name">Tuning I/O Performance</span></a></li><li class="inactive"><a href="cha.tuning.taskscheduler.html"><span class="number">13 </span><span class="name">Tuning the Task Scheduler</span></a></li><li class="inactive"><a href="cha.tuning.memory.html"><span class="number">14 </span><span class="name">Tuning the Memory Management Subsystem</span></a></li><li class="inactive"><a href="cha.tuning.network.html"><span class="number">15 </span><span class="name">Tuning the Network</span></a></li></ol></li><li class="inactive"><a href="part.tuning.dumps.html"><span class="number">VI </span><span class="name">Handling System Dumps</span></a><ol><li class="inactive"><a href="cha.tuning.tracing.html"><span class="number">16 </span><span class="name">Tracing Tools</span></a></li><li class="inactive"><a href="cha.tuning.kexec.html"><span class="number">17 </span><span class="name">Kexec and Kdump</span></a></li></ol></li><li class="inactive"><a href="part.tuning.ptp.html"><span class="number">VII </span><span class="name">Synchronized Clocks with Precision Time Protocol</span></a><ol><li class="inactive"><a href="cha.tuning.ptp.html"><span class="number">18 </span><span class="name">Precision Time Protocol</span></a></li></ol></li><li class="inactive"><a href="bk05apa.html"><span class="number">A </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Part V. Kernel Tuning" href="part.tuning.kernel.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Tuning the Task Scheduler" href="cha.tuning.taskscheduler.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="openSUSE Leap Documentation"><span class="book-icon">openSUSE Leap Documentation</span></a><span> › </span><a class="crumb" href="book.sle.tuning.html">System Analysis and Tuning Guide</a><span> › </span><a class="crumb" href="part.tuning.kernel.html">Kernel Tuning</a><span> › </span><a class="crumb" href="cha.tuning.io.html">Tuning I/O Performance</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Part V. Kernel Tuning" href="part.tuning.kernel.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Tuning the Task Scheduler" href="cha.tuning.taskscheduler.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="cha.tuning.io"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">openSUSE Leap</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">42.3</span></span></span></div><div><h2 class="title"><span class="number">12 </span><span class="name">Tuning I/O Performance</span> </h2><div class="doc-status"><ul><li><span class="ds-label">Filename: </span>tuning_storagescheduler.xml</li><li><span class="ds-label">ID: </span>cha.tuning.io</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha.tuning.io.html#cha.tuning.io.switch"><span class="number">12.1 </span><span class="name">Switching I/O Scheduling</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.io.html#cha.tuning.io.schedulers"><span class="number">12.2 </span><span class="name">Available I/O Elevators</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.io.html#cha.tuning.io.barrier"><span class="number">12.3 </span><span class="name">I/O Barrier Tuning</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.io.html#cha.tuning.io.scsimq"><span class="number">12.4 </span><span class="name">Enable blk-mq I/O Path for SCSI by Default</span></a></span></dt></dl></div></div><p>
  I/O scheduling controls how input/output operations will be submitted to
  storage. <span class="productname"><span class="phrase">openSUSE Leap</span></span> offers various I/O algorithms—called
  <code class="literal">elevators</code>—suiting different workloads.
  Elevators can help to reduce seek operations and can prioritize I/O requests.
 </p><p>
  Choosing the best suited I/O elevator not only depends on the workload,
  but on the hardware, too. Single ATA disk systems, SSDs, RAID arrays, or
  network storage systems, for example, each require different tuning
  strategies.
 </p><div class="sect1 " id="cha.tuning.io.switch"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.io.switch"><span class="number">12.1 </span><span class="name">Switching I/O Scheduling</span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#cha.tuning.io.switch">#</a></h2></div></div></div><p>
   <span class="productname"><span class="phrase">openSUSE Leap</span></span> picks a default I/O scheduler at boot-time, which can be
   changed on the fly per block device. This makes it possible to set different
   algorithms, for example, for the device hosting the system partition and the
   device hosting a database.
  </p><p>
   The default I/O scheduler is chosen for each device based on whether the
   device reports to be rotational disk or not. For non-rotational disks
   <code class="systemitem">DEADLINE</code> I/O scheduler is picked.
   Other devices default to
   <code class="systemitem">CFQ</code> (Completely Fair Queuing).
   To change this default, use the following boot parameter:
  </p><div class="verbatim-wrap"><pre class="screen">elevator=<em class="replaceable ">SCHEDULER</em></pre></div><p>
   Replace <em class="replaceable ">SCHEDULER</em> with one of the values
   <code class="option">cfq</code>, <code class="option">noop</code>, or
   <code class="option">deadline</code>. See <a class="xref" href="cha.tuning.io.html#cha.tuning.io.schedulers" title="12.2. Available I/O Elevators">Section 12.2, “Available I/O Elevators”</a>
   for details.
  </p><p>
   To change the elevator for a specific device in the running system, run
   the following command:
  </p><div class="verbatim-wrap"><pre class="screen">echo <em class="replaceable ">SCHEDULER</em> &gt; /sys/block/<em class="replaceable ">DEVICE</em>/queue/scheduler</pre></div><p>
   Here, <em class="replaceable ">SCHEDULER</em> is one of
   <code class="option">cfq</code>, <code class="option">noop</code>, or <code class="option">deadline</code>.
   <em class="replaceable ">DEVICE</em> is the block device
   (<code class="systemitem">sda</code> for example). Note that this change will not
   persist during reboot. For permanent I/O scheduler change for a particular
   device either place the command switching the I/O scheduler into init
   scripts or add appropriate udev rule into
   <code class="filename">/lib/udev/rules.d/</code>. See
   <code class="filename">/lib/udev/rules.d/60-ssd-scheduler.rules</code> for an example
   of such tuning.
  </p></div><div class="sect1 " id="cha.tuning.io.schedulers"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.io.schedulers"><span class="number">12.2 </span><span class="name">Available I/O Elevators</span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#cha.tuning.io.schedulers">#</a></h2></div></div></div><p>
   In the following elevators available on <span class="productname"><span class="phrase">openSUSE Leap</span></span> are listed. Each
   elevator has a set of tunable parameters, which can be set with the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen">echo <em class="replaceable ">VALUE</em> &gt; /sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/<em class="replaceable ">TUNABLE</em></pre></div><p>
   where <em class="replaceable ">VALUE</em> is the desired value for the
   <em class="replaceable ">TUNABLE</em> and <em class="replaceable ">DEVICE</em>
   the block device.
  </p><p>
   To find out which elevator is the current default, run the following
   command. The currently selected scheduler is listed in brackets:
  </p><div class="verbatim-wrap"><pre class="screen">jupiter:~ # cat /sys/block/sda/queue/scheduler
noop deadline [cfq]</pre></div><p>
  This file can also contain the string <code class="literal">none</code> meaning that
  I/O scheduling does not happen for this device. This is usually because the
  device uses multi-queue queueing mechanism (refer to <a class="xref" href="cha.tuning.io.html#cha.tuning.io.scsimq" title="12.4. Enable blk-mq I/O Path for SCSI by Default">Section 12.4, “Enable blk-mq I/O Path for SCSI by Default”</a>).
  </p><div class="sect2 " id="sec.tuning.io.schedulers.cfq"><div class="titlepage"><div><div><h3 class="title" id="sec.tuning.io.schedulers.cfq"><span class="number">12.2.1 </span><span class="name"><code class="systemitem">CFQ</code> (Completely Fair Queuing)</span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#sec.tuning.io.schedulers.cfq">#</a></h3></div></div></div><p>
    <code class="systemitem">CFQ</code> is a fairness-oriented
    scheduler and is used by default on <span class="productname"><span class="phrase">openSUSE Leap</span></span>. The algorithm
    assigns each thread a time slice in which it is allowed to submit I/O to
    disk. This way each thread gets a fair share of I/O throughput. It also
    allows assigning tasks I/O priorities which are taken into account
    during scheduling decisions (see
    <a class="xref" href="cha.tuning.resources.html#cha.tuning.resources.disk.ionice" title="8.3.3. Prioritizing Disk Access with ionice">Section 8.3.3, “Prioritizing Disk Access with <code class="command">ionice</code>”</a>). The
    <code class="systemitem">CFQ</code> scheduler has the
    following tunable parameters:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm140085915180288"><span class="term "><code class="filename">
      /sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/slice_idle_us
     </code>
     </span></dt><dd><p>
       When a task has no more I/O to submit in its time slice, the I/O
       scheduler waits for a while before scheduling the next thread.
       The <code class="filename">slice_idle_us</code> is
       the time in microseconds the I/O scheduler waits. File
       <code class="filename">slice_idle</code> controls the same tunable but in
       millisecond units. Waiting for more I/O from a thread can
       improve locality of I/O. Additionally, it avoids starving processes
       doing dependent I/O.
       A process does dependent I/O if it needs a result of one I/O
       to submit another I/O. For example, if you first need to read an index
       block to find out a data block to read, these two reads form
       a dependent I/O.
      </p><p>For media where locality does not play a big role (SSDs, SANs
       with lots of disks) setting  <code class="filename">/sys/block/<em class="replaceable ">&lt;device&gt;</em>/queue/iosched/slice_idle_us</code>
       to <code class="literal">0</code> can improve the throughput considerably.
      </p></dd><dt id="idm140085915174288"><span class="term "><code class="filename">
      /sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/quantum
     </code>
     </span></dt><dd><p>
       This option limits the maximum number of requests that are being
       processed at once by the device. The default value is
       <code class="literal">4</code>. For a storage with several disks, this setting
       can unnecessarily limit parallel processing of requests. Therefore,
       increasing the value can improve performance. However, it can also
       cause latency of certain I/O operations to increase because more
       requests are buffered inside the storage. When changing this value,
       you can also consider tuning
       <code class="filename">/sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/slice_async_rq</code>
       (the default value is <code class="literal">2</code>). This limits the maximum
       number of asynchronous requests—usually write
       requests—that are submitted in one time slice.
      </p></dd><dt id="idm140085915169296"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/low_latency</code>
     </span></dt><dd><p>
       When enabled (which is the default on <span class="productname"><span class="phrase">openSUSE Leap</span></span>) the scheduler
       may dynamically adjust the length of the time slice by aiming to meet
       a tuning parameter called the <code class="literal">target_latency</code>. Time
       slices are recomputed to meet this <code class="literal">target_latency</code>
       and ensure that processes get fair access within a bounded length of
       time.
      </p></dd><dt id="idm140085915164480"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/target_latency</code>
     </span></dt><dd><p>
       Contains an estimated latency time for the
       <code class="systemitem">CFQ</code>.
       <code class="systemitem">CFQ</code> will use it to
       calculate the time slice used for every task.
      </p></dd><dt id="idm140085915160384"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">DEVICE</em>/queue/iosched/group_idle_us</code></span></dt><dd><p>To avoid starving of blkio cgroups doing dependent I/O, CFQ
           waits a bit after completion of I/O for one blkio cgroup before
           scheduling I/O for a different blkio cgroup. When
           <code class="literal">slice_idle_us</code> is set, this parameter does not
           have a big impact. However, for fast media, the overhead of
           <code class="literal">slice_idle_us</code> is generally undesirable.
           Disabling <code class="literal">slice_idle_us</code> and setting
           <code class="literal">group_idle_us</code> is a method to avoid starvation
           of blkio cgroups doing dependent I/O with lower overhead. Note that
           the file <code class="filename">group_idle</code> controls the same tunable
           however with millisecond granularity.
         </p></dd></dl></div><div class="complex-example"><div class="example" id="idm140085915155088"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12.1: </span><span class="name">Increasing individual thread throughput using <code class="systemitem">CFQ</code> </span><a title="Permalink" class="permalink" href="cha.tuning.io.html#idm140085915155088">#</a></h6></div><div class="example-contents"><p>
     In <span class="productname"><span class="phrase">openSUSE Leap</span></span> <span class="productnumber"><span class="phrase">42.3</span></span>, the <code class="literal">low_latency</code>
     tuning parameter is enabled by default to ensure that processes get fair
     access within a bounded length of time. (Note that this parameter was not
     enabled in versions prior to <span class="phrase">openSUSE Leap</span>.)
    </p><p>
     This is usually preferred in a server scenario where processes are
     executing I/O as part of transactions, as it makes the time needed for each
     transaction predictable. However, there are scenarios where that is not the
     desired behavior:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       If the performance metric of interest is the peak performance of a single
       process when there is I/O contention.
      </p></li><li class="listitem "><p>
       If a workload must complete as quickly as possible and there are multiple
       sources of I/O. In this case, unfair treatment from the I/O scheduler may
       allow the transactions to complete faster: Processes take their full
       slice and exit quickly, resulting in reduced overall contention.
      </p></li></ul></div><p>
     To address this, there are two options—increase
     <code class="literal">target_latency</code> or disable
     <code class="literal">low_latency</code>. As with all tuning parameters it is
     important to verify your workload behaves as expected before and after
     the tuning modification. Take careful note of whether your workload
     depends on individual process peak performance or scales better with
     fairness. It should also be noted that the performance will depend on
     the underlying storage and the correct tuning option for one
     installation may not be universally true.
    </p><p>
     Find below an example that does not control when I/O starts but is
     simple enough to demonstrate the point. 32 processes are writing a
     small amount of data to disk in parallel. Using the <span class="productname"><span class="phrase">openSUSE Leap</span></span>
     default (enabling <code class="literal">low_latency</code>), the result looks as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>echo 1 &gt; /sys/block/sda/queue/iosched/low_latency
<code class="prompt root">root # </code>time ./dd-test.sh
10485760 bytes (10 MB) copied, 2.62464 s, 4.0 MB/s
10485760 bytes (10 MB) copied, 3.29624 s, 3.2 MB/s
10485760 bytes (10 MB) copied, 3.56341 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.56908 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.53043 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.57511 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.53672 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.5433 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.65474 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.63694 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.90122 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.88507 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.86135 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.84553 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.88871 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.94943 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 4.12731 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.15106 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.21601 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.35004 s, 2.4 MB/s
10485760 bytes (10 MB) copied, 4.33387 s, 2.4 MB/s
10485760 bytes (10 MB) copied, 4.55434 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.52283 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.52682 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.56176 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.62727 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.78958 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.79772 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.78004 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.77994 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.86114 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.88062 s, 2.1 MB/s

real    0m4.978s
user    0m0.112s
sys     0m1.544s</pre></div><p>
     Note that each process completes in similar times. This is the
     <code class="systemitem">CFQ</code> scheduler meeting its
     <code class="literal">target_latency</code>: Each process has fair access
     to storage.
    </p><p>
     Note that the earlier processes complete somewhat faster.
     This happens because the start time of the processes is not identical.
     In a more complicated example, it is possible to control for this.
    </p><p>
     This is what happens when low_latency is disabled:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>echo 0 &gt; /sys/block/sda/queue/iosched/low_latency
<code class="prompt root">root # </code>time ./dd-test.sh
10485760 bytes (10 MB) copied, 0.813519 s, 12.9 MB/s
10485760 bytes (10 MB) copied, 0.788106 s, 13.3 MB/s
10485760 bytes (10 MB) copied, 0.800404 s, 13.1 MB/s
10485760 bytes (10 MB) copied, 0.816398 s, 12.8 MB/s
10485760 bytes (10 MB) copied, 0.959087 s, 10.9 MB/s
10485760 bytes (10 MB) copied, 1.09563 s, 9.6 MB/s
10485760 bytes (10 MB) copied, 1.18716 s, 8.8 MB/s
10485760 bytes (10 MB) copied, 1.27661 s, 8.2 MB/s
10485760 bytes (10 MB) copied, 1.46312 s, 7.2 MB/s
10485760 bytes (10 MB) copied, 1.55489 s, 6.7 MB/s
10485760 bytes (10 MB) copied, 1.64277 s, 6.4 MB/s
10485760 bytes (10 MB) copied, 1.78196 s, 5.9 MB/s
10485760 bytes (10 MB) copied, 1.87496 s, 5.6 MB/s
10485760 bytes (10 MB) copied, 1.9461 s, 5.4 MB/s
10485760 bytes (10 MB) copied, 2.08351 s, 5.0 MB/s
10485760 bytes (10 MB) copied, 2.28003 s, 4.6 MB/s
10485760 bytes (10 MB) copied, 2.42979 s, 4.3 MB/s
10485760 bytes (10 MB) copied, 2.54564 s, 4.1 MB/s
10485760 bytes (10 MB) copied, 2.6411 s, 4.0 MB/s
10485760 bytes (10 MB) copied, 2.75171 s, 3.8 MB/s
10485760 bytes (10 MB) copied, 2.86162 s, 3.7 MB/s
10485760 bytes (10 MB) copied, 2.98453 s, 3.5 MB/s
10485760 bytes (10 MB) copied, 3.13723 s, 3.3 MB/s
10485760 bytes (10 MB) copied, 3.36399 s, 3.1 MB/s
10485760 bytes (10 MB) copied, 3.60018 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.58151 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.67385 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.69471 s, 2.8 MB/s
10485760 bytes (10 MB) copied, 3.66658 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.81495 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 4.10172 s, 2.6 MB/s
10485760 bytes (10 MB) copied, 4.0966 s, 2.6 MB/s

real    0m3.505s
user    0m0.160s
sys     0m1.516s</pre></div><p>
     Note that the time processes take to complete is spread much wider as
     processes are not getting fair access. Some processes complete faster
     and exit, allowing the total workload to complete faster, and some
     processes measure higher apparent I/O performance. It is also important
     to note that this example may not behave similarly on all systems as
     the results depend on the resources of the machine and the underlying
     storage.
    </p><p>
     It is important to emphasize that neither tuning option is inherently
     better than the other. Both are best in different circumstances and it
     is important to understand the requirements of your workload and tune
     accordingly.
    </p></div></div></div></div><div class="sect2 " id="sec.tuning.io.schedulers.noop"><div class="titlepage"><div><div><h3 class="title" id="sec.tuning.io.schedulers.noop"><span class="number">12.2.2 </span><span class="name"><code class="systemitem">NOOP</code></span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#sec.tuning.io.schedulers.noop">#</a></h3></div></div></div><p>
    A trivial scheduler that only passes down the I/O that comes to it.
    Useful for checking whether complex I/O scheduling decisions of other
    schedulers are causing I/O performance regressions.
   </p><p>
    This scheduler is recommended for setups with devices that do I/O scheduling
    themselves, such as intelligent storage or in multipathing environments.
    If you choose a more complicated scheduler on the host, the scheduler of
    the host and the scheduler of the storage device compete with each other.
    This can decrease performance.
    The storage device can usually determine best how to schedule I/O.
   </p><p>
    For similar reasons, this scheduler is also recommended for use within
    virtual machines.
   </p><p>
    The <code class="systemitem">NOOP</code> scheduler can be
    useful for devices that do not depend on mechanical movement, like SSDs.
    Usually, the
    <code class="systemitem">DEADLINE</code> I/O scheduler is a
    better choice for these devices. However,
    <code class="systemitem">NOOP</code> creates less overhead and
    thus can on certain workloads increase performance.
   </p></div><div class="sect2 " id="sec.tuning.io.schedulers.deadline"><div class="titlepage"><div><div><h3 class="title" id="sec.tuning.io.schedulers.deadline"><span class="number">12.2.3 </span><span class="name"><code class="systemitem">DEADLINE</code></span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#sec.tuning.io.schedulers.deadline">#</a></h3></div></div></div><p>
    <code class="systemitem">DEADLINE</code> is a latency-oriented
    I/O scheduler. Each I/O request is assigned a deadline. Usually,
    requests are stored in queues (read and write) sorted by sector numbers.
    The <code class="systemitem">DEADLINE</code> algorithm
    maintains two additional queues (read and write) in which requests are
    sorted by deadline. As long as no request has timed out, the
    <span class="quote">“<span class="quote">sector</span>”</span> queue is used. When timeouts occur, requests from
    the <span class="quote">“<span class="quote">deadline</span>”</span> queue are served until there are no more
    expired requests. Generally, the algorithm prefers reads over writes.
   </p><p>
    This scheduler can provide a superior throughput over the
    <code class="systemitem">CFQ</code> I/O scheduler in cases
    where several threads read and write and fairness is not an issue. For
    example, for several parallel readers from a SAN and for databases
    (especially when using <span class="quote">“<span class="quote">TCQ</span>”</span> disks). The
    <code class="systemitem">DEADLINE</code> scheduler has the
    following tunable parameters:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm140085915116256"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">&lt;device&gt;</em>/queue/iosched/writes_starved</code>
     </span></dt><dd><p>
       Controls how many reads can be sent to disk before it is possible to
       send writes. A value of <code class="literal">3</code> means, that three read
       operations are carried out for one write operation.
      </p></dd><dt id="idm140085915113136"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">&lt;device&gt;</em>/queue/iosched/read_expire</code>
     </span></dt><dd><p>
       Sets the deadline (current time plus the read_expire value) for read
       operations in milliseconds. The default is 500.
      </p></dd><dt id="idm140085915110464"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">&lt;device&gt;</em>/queue/iosched/write_expire</code>
     </span></dt><dd><p>
       <code class="filename">/sys/block/<em class="replaceable ">&lt;device&gt;</em>/queue/iosched/read_expire</code>
       Sets the deadline (current time plus the read_expire value) for read
       operations in milliseconds. The default is 500.
      </p></dd></dl></div></div></div><div class="sect1 " id="cha.tuning.io.barrier"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.io.barrier"><span class="number">12.3 </span><span class="name">I/O Barrier Tuning</span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#cha.tuning.io.barrier">#</a></h2></div></div></div><p>
   
   Most file systems (such as XFS, Ext3, Ext4, or reiserfs) send write
   barriers to disk after fsync or during transaction commits. Write
   barriers enforce proper ordering of writes, making volatile disk write
   caches safe to use (at some performance penalty). If your disks are
   battery-backed in one way or another, disabling barriers can safely
   improve performance.
  </p><p>
   Sending write barriers can be disabled using the
   <code class="option">nobarrier</code> mount option.
  </p><div id="idm140085915102976" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Disabling Barriers Can Lead to Data Loss</h6><p>
    Disabling barriers when disks cannot guarantee caches are properly
    written in case of power failure can lead to severe file system
    corruption and data loss.
   </p></div></div><div class="sect1 " id="cha.tuning.io.scsimq"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.io.scsimq"><span class="number">12.4 </span><span class="name">Enable blk-mq I/O Path for SCSI by Default</span> <a title="Permalink" class="permalink" href="cha.tuning.io.html#cha.tuning.io.scsimq">#</a></h2></div></div></div><p>
   Block multiqueue (blk-mq) is a multi-queue block I/O queueing
   mechanism. Blk-mq uses per-cpu software queues to queue I/O
   requests. The software queues are mapped to one or more hardware
   submission queues. Blk-mq significantly reduces lock contention. In
   particular blk-mq improves performance for devices that support a
   high number of input/output operations per second (IOPS).  Blk-mq
   is already the default for some devices, for example, NVM Express devices.
  </p><p>
   Currently blk-mq has no I/O scheduling support (no CFQ, no deadline
   I/O scheduler). This lack of I/O scheduling can cause significant
   performance degradation when spinning disks are used. Therefore
   blk-mq is not enabled by default for SCSI devices.
  </p><p>
   If you have fast SCSI devices (for example, SSDs) instead of SCSI
   hard disks attached to your system, consider switching to
   blk-mq for SCSI. This is done using the kernel command line option
   <code class="literal">scsi_mod.use_blk_mq=1</code>.
  </p></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha.tuning.taskscheduler.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 13 </span>Tuning the Task Scheduler</span></a><a class="nav-link" href="part.tuning.kernel.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Part V </span>Kernel Tuning</span></a></div><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>