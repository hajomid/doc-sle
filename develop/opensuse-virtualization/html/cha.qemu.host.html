<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Setting Up a KVM VM Host Server | Virtualization Guide | openSUSE Leap 42.3</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 2.4.0 using SUSE XSL Stylesheets 2.0.8 (based on DocBook XSL Stylesheets 1.78.1) - chunked" /><meta name="product-name" content="openSUSE Leap" /><meta name="product-number" content="42.3" /><meta name="book-title" content="Virtualization Guide" /><meta name="chapter-title" content="Chapter 26. Setting Up a KVM VM Host Server" /><meta name="tracker-url" content="https://bugzilla.opensuse.org/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="fs@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="openSUSE Distribution" /><meta name="tracker-bsc-version" content="Leap 42.2" /><link rel="home" href="index.html" title="openSUSE Leap Documentation" /><link rel="up" href="part.virt.qemu.html" title="Part V. Managing Virtual Machines with QEMU" /><link rel="prev" href="cha.qemu.overview.html" title="Chapter 25. QEMU Overview" /><link rel="next" href="cha.qemu.guest_inst.html" title="Chapter 27. Guest Installation" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="openSUSE Leap Documentation"><span class="book-icon">openSUSE Leap Documentation</span></a><span> › </span><a class="crumb" href="book.virt.html">Virtualization Guide</a><span> › </span><a class="crumb" href="part.virt.qemu.html">Managing Virtual Machines with QEMU</a><span> › </span><a class="crumb" href="cha.qemu.host.html">Setting Up a KVM VM Host Server</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Virtualization Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="cha.kvm.html"><span class="number"> </span><span class="name">About This Manual</span></a></li><li class="inactive"><a href="part.virt.intro.html"><span class="number">I </span><span class="name">Introduction</span></a><ol><li class="inactive"><a href="chap.virtualization.introduction.html"><span class="number">1 </span><span class="name">Virtualization Technology</span></a></li><li class="inactive"><a href="cha.xen.basics.html"><span class="number">2 </span><span class="name">Introduction to Xen Virtualization</span></a></li><li class="inactive"><a href="cha.kvm.intro.html"><span class="number">3 </span><span class="name">Introduction to KVM Virtualization</span></a></li><li class="inactive"><a href="cha.containers.intro.html"><span class="number">4 </span><span class="name">Introduction to Linux Containers</span></a></li><li class="inactive"><a href="cha.tools.intro.html"><span class="number">5 </span><span class="name">Virtualization Tools</span></a></li><li class="inactive"><a href="cha.vt.installation.html"><span class="number">6 </span><span class="name">Installation of Virtualization Components</span></a></li></ol></li><li class="inactive"><a href="part.virt.libvirt.html"><span class="number">II </span><span class="name">Managing Virtual Machines with <code class="systemitem">libvirt</code></span></a><ol><li class="inactive"><a href="cha.libvirt.overview.html"><span class="number">7 </span><span class="name">Starting and Stopping <code class="systemitem">libvirtd</code></span></a></li><li class="inactive"><a href="cha.kvm.inst.html"><span class="number">8 </span><span class="name">Guest Installation</span></a></li><li class="inactive"><a href="cha.libvirt.managing.html"><span class="number">9 </span><span class="name">Basic VM Guest Management</span></a></li><li class="inactive"><a href="cha.libvirt.connect.html"><span class="number">10 </span><span class="name">Connecting and Authorizing</span></a></li><li class="inactive"><a href="cha.libvirt.storage.html"><span class="number">11 </span><span class="name">Managing Storage</span></a></li><li class="inactive"><a href="cha.libvirt.networks.html"><span class="number">12 </span><span class="name">Managing Networks</span></a></li><li class="inactive"><a href="cha.libvirt.config.html"><span class="number">13 </span><span class="name">Configuring Virtual Machines</span></a></li></ol></li><li class="inactive"><a href="part.virt.common.html"><span class="number">III </span><span class="name">Hypervisor-Independent Features</span></a><ol><li class="inactive"><a href="cha.cachemodes.html"><span class="number">14 </span><span class="name">Disk Cache Modes</span></a></li><li class="inactive"><a href="sec.kvm.managing.clock.html"><span class="number">15 </span><span class="name">VM Guest Clock Settings</span></a></li><li class="inactive"><a href="chap.guestfs.html"><span class="number">16 </span><span class="name">libguestfs</span></a></li></ol></li><li class="inactive"><a href="part.virt.xen.html"><span class="number">IV </span><span class="name">Managing Virtual Machines with Xen</span></a><ol><li class="inactive"><a href="cha.xen.vhost.html"><span class="number">17 </span><span class="name">Setting Up a Virtual Machine Host</span></a></li><li class="inactive"><a href="cha.xen.network.html"><span class="number">18 </span><span class="name">Virtual Networking</span></a></li><li class="inactive"><a href="cha.xen.manage.html"><span class="number">19 </span><span class="name">Managing a Virtualization Environment</span></a></li><li class="inactive"><a href="cha.xen.vbd.html"><span class="number">20 </span><span class="name">Block Devices in Xen</span></a></li><li class="inactive"><a href="cha.xen.config.html"><span class="number">21 </span><span class="name">Virtualization: Configuration Options and Settings</span></a></li><li class="inactive"><a href="cha.xen.admin.html"><span class="number">22 </span><span class="name">Administrative Tasks</span></a></li><li class="inactive"><a href="cha.xen.xenstore.html"><span class="number">23 </span><span class="name">XenStore: Configuration Database Shared between Domains</span></a></li><li class="inactive"><a href="cha.xen.ha.html"><span class="number">24 </span><span class="name">Xen as a High-Availability Virtualization Host</span></a></li></ol></li><li class="inactive"><a href="part.virt.qemu.html"><span class="number">V </span><span class="name">Managing Virtual Machines with QEMU</span></a><ol><li class="inactive"><a href="cha.qemu.overview.html"><span class="number">25 </span><span class="name">QEMU Overview</span></a></li><li class="inactive"><a href="cha.qemu.host.html"><span class="number">26 </span><span class="name">Setting Up a KVM VM Host Server</span></a></li><li class="inactive"><a href="cha.qemu.guest_inst.html"><span class="number">27 </span><span class="name">Guest Installation</span></a></li><li class="inactive"><a href="cha.qemu.running.html"><span class="number">28 </span><span class="name">Running Virtual Machines with qemu-system-ARCH</span></a></li><li class="inactive"><a href="cha.qemu.monitor.html"><span class="number">29 </span><span class="name">Virtual Machine Administration Using QEMU Monitor</span></a></li></ol></li><li class="inactive"><a href="part.virt.lxc.html"><span class="number">VI </span><span class="name">Managing Virtual Machines with LXC</span></a><ol><li class="inactive"><a href="cha.lxc.html"><span class="number">30 </span><span class="name">Linux Containers</span></a></li><li class="inactive"><a href="cha.lxc2libvirt.html"><span class="number">31 </span><span class="name">Migration from LXC to <code class="systemitem">libvirt-lxc</code></span></a></li></ol></li><li class="inactive"><a href="gloss.vt.glossary.html"><span class="number"> </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="app.kvm.html"><span class="number">A </span><span class="name">Appendix</span></a></li><li class="inactive"><a href="cha.xmtoxl.html"><span class="number">B </span><span class="name">XM, XL Toolstacks and Libvirt framework</span></a></li><li class="inactive"><a href="bk06apc.html"><span class="number">C </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 25. QEMU Overview" href="cha.qemu.overview.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 27. Guest Installation" href="cha.qemu.guest_inst.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="openSUSE Leap Documentation"><span class="book-icon">openSUSE Leap Documentation</span></a><span> › </span><a class="crumb" href="book.virt.html">Virtualization Guide</a><span> › </span><a class="crumb" href="part.virt.qemu.html">Managing Virtual Machines with QEMU</a><span> › </span><a class="crumb" href="cha.qemu.host.html">Setting Up a KVM VM Host Server</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 25. QEMU Overview" href="cha.qemu.overview.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 27. Guest Installation" href="cha.qemu.guest_inst.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="cha.qemu.host"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">openSUSE Leap</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">42.3</span></span></span></div><div><h2 class="title"><span class="number">26 </span><span class="name">Setting Up a KVM VM Host Server</span> </h2><div class="doc-status"><ul><li><span class="ds-label">Filename: </span>qemu_host_installation.xml</li><li><span class="ds-label">ID: </span>cha.qemu.host</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha.qemu.host.html#kvm.host.cpu"><span class="number">26.1 </span><span class="name">CPU Support for Virtualization</span></a></span></dt><dt><span class="sect1"><a href="cha.qemu.host.html#kvm.host.soft"><span class="number">26.2 </span><span class="name">Required Software</span></a></span></dt><dt><span class="sect1"><a href="cha.qemu.host.html#kvm.host.virtio"><span class="number">26.3 </span><span class="name">KVM Host-Specific Features</span></a></span></dt></dl></div></div><p>
  This section documents how to set up and use <span class="productname"><span class="phrase">openSUSE Leap</span></span> <span class="productnumber"><span class="phrase">42.3</span></span>
  as a QEMU-KVM based virtual machine host.
 </p><div id="idm139724851969104" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Resources</h6><p>
   In general, the virtual guest system needs the same hardware resources as
   when installed on a physical machine. The more guests you plan to run on
   the host system, the more hardware resources—CPU, disk, memory, and
   network—you need to add to the VM Host Server.
  </p></div><div class="sect1 " id="kvm.host.cpu"><div class="titlepage"><div><div><h2 class="title" id="kvm.host.cpu"><span class="number">26.1 </span><span class="name">CPU Support for Virtualization</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.host.cpu">#</a></h2></div></div></div><p>
   To run KVM, your CPU must support virtualization, and
   virtualization needs to be enabled in BIOS. The file
   <code class="filename">/proc/cpuinfo</code> includes information about your CPU
   features.
  </p></div><div class="sect1 " id="kvm.host.soft"><div class="titlepage"><div><div><h2 class="title" id="kvm.host.soft"><span class="number">26.2 </span><span class="name">Required Software</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.host.soft">#</a></h2></div></div></div><p>
   The KVM host requires several packages to be installed. To install all
   necessary packages, do the following:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run <span class="guimenu">YaST</span> › <span class="guimenu">
     Virtualization</span> › <span class="guimenu">Install Hypervisor and
     Tools</span>.
    </p><div class="figure" id="idm139724851961280"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast2_hypervisors_kde.png"><img src="images/yast2_hypervisors_kde.png" width="" alt="Installing the KVM Hypervisor and Tools" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 26.1: </span><span class="name">Installing the KVM Hypervisor and Tools </span><a title="Permalink" class="permalink" href="cha.qemu.host.html#idm139724851961280">#</a></h6></div></div></li><li class="step "><p>
     Select <span class="guimenu">KVM server</span> and preferably also <span class="guimenu">KVM
     tools</span>, and confirm with <span class="guimenu">Accept</span>.
    </p></li><li class="step "><p>
     During the installation process, you can optionally let YaST create a
     <span class="guimenu">Network Bridge</span> for you automatically. If you do not
     plan to dedicate an additional physical network card to your virtual
     guests, network bridge is a standard way to connect the guest machines
     to the network.
    </p><div class="figure" id="idm139724851952128"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast2_netbridge.png"><img src="images/yast2_netbridge.png" width="" alt="Network Bridge" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 26.2: </span><span class="name">Network Bridge </span><a title="Permalink" class="permalink" href="cha.qemu.host.html#idm139724851952128">#</a></h6></div></div></li><li class="step "><p>
     After all the required packages are installed (and new network setup
     activated), try to load the KVM kernel module relevant for your CPU
     type—<code class="systemitem">kvm-intel</code> or
     <code class="systemitem">kvm-amd</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>modprobe kvm-intel</pre></div><p>
     Check if the module is loaded into memory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>lsmod | grep kvm
kvm_intel              64835  6
kvm                   411041  1 kvm_intel</pre></div><p>
     Now the KVM host is ready to serve KVM VM Guests. For more
     information, see <a class="xref" href="cha.qemu.running.html" title="Chapter 28. Running Virtual Machines with qemu-system-ARCH">Chapter 28, <em>Running Virtual Machines with qemu-system-ARCH</em></a>.
    </p></li></ol></div></div></div><div class="sect1 " id="kvm.host.virtio"><div class="titlepage"><div><div><h2 class="title" id="kvm.host.virtio"><span class="number">26.3 </span><span class="name">KVM Host-Specific Features</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.host.virtio">#</a></h2></div></div></div><p>
   You can improve the performance of KVM-based VM Guests by letting them
   fully use specific features of the VM Host Server's hardware
   (<span class="emphasis"><em>paravirtualization</em></span>). This section introduces
   techniques to make the guests access the physical host's hardware
   directly—without the emulation layer—to make the most use of
   it.
  </p><div id="idm139724851938976" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
    Examples included in this section assume basic knowledge of the
    <code class="command">qemu-system-<em class="replaceable ">ARCH</em></code> command
    line options. For more information, see
    <a class="xref" href="cha.qemu.running.html" title="Chapter 28. Running Virtual Machines with qemu-system-ARCH">Chapter 28, <em>Running Virtual Machines with qemu-system-ARCH</em></a>.
   </p></div><div class="sect2 " id="kvm.virtio-scsi"><div class="titlepage"><div><div><h3 class="title" id="kvm.virtio-scsi"><span class="number">26.3.1 </span><span class="name">Using the Host Storage with <code class="systemitem">virtio-scsi</code></span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.virtio-scsi">#</a></h3></div></div></div><p>
    <code class="systemitem">virtio-scsi</code> is an advanced storage stack for
    KVM. It replaces the former <code class="systemitem">virtio-blk</code> stack
    for SCSI devices pass-through. It has several advantages over
    <code class="systemitem">virtio-blk</code>:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm139724851933008"><span class="term ">Improved scalability</span></dt><dd><p>
       KVM guests have a limited number of PCI controllers, which results
       in a limited number of possibly attached devices.
       <code class="systemitem">virtio-scsi</code> solves this limitation by
       grouping multiple storage devices on a single controller. Each device
       on a <code class="systemitem">virtio-scsi</code> controller is represented
       as a logical unit, or <span class="emphasis"><em>LUN</em></span>.
      </p></dd><dt id="idm139724851929600"><span class="term ">Standard command set</span></dt><dd><p>
       <code class="systemitem">virtio-blk</code> uses a small set of
       commands that need to be known to both the
       <code class="systemitem">virtio-blk</code> driver and the virtual machine
       monitor, and so introducing a new command requires updating both the
       driver and the monitor.
      </p><p>
       By comparison, <code class="systemitem">virtio-scsi</code> does not define
       commands, but rather a transport protocol for these commands following
       the industry-standard SCSI specification. This approach is shared with
       other technologies, such as Fibre Channel, ATAPI, and USB devices.
      </p></dd><dt id="idm139724851925584"><span class="term ">Device naming</span></dt><dd><p>
       <code class="systemitem">virtio-blk</code> devices are presented inside the
       guest as <code class="filename">/dev/vd<em class="replaceable ">X</em></code>,
       which is different from device
       names in physical systems and may cause migration problems.
      </p><p>
       <code class="systemitem">virtio-scsi</code> keeps the device names identical
       to those on physical systems, making the virtual machines easily
       relocatable.
      </p></dd><dt id="idm139724851921472"><span class="term ">SCSI device pass-through</span></dt><dd><p>
       For virtual disks backed by a whole LUN on the host, it is preferable
       for the guest to send SCSI commands directly to the LUN
       (pass-through). This is limited in
       <code class="systemitem">virtio-blk</code>, as guests need to use the
       virtio-blk protocol instead of SCSI command pass-through, and,
       moreover, it is not available for Windows guests.
       <code class="systemitem">virtio-scsi</code> natively removes these
       limitations.
      </p></dd></dl></div><div class="sect3 " id="idm139724851918256"><div class="titlepage"><div><div><h4 class="title" id="idm139724851918256"><span class="number">26.3.1.1 </span><span class="name"><code class="systemitem">virtio-scsi</code> Usage</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#idm139724851918256">#</a></h4></div></div></div><p>
     KVM supports the SCSI pass-through feature with the
     <code class="systemitem">virtio-scsi-pci</code> device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-system-x86_64 [...] \
-device virtio-scsi-pci,id=scsi</pre></div></div></div><div class="sect2 " id="kvm.qemu.vnet"><div class="titlepage"><div><div><h3 class="title" id="kvm.qemu.vnet"><span class="number">26.3.2 </span><span class="name">Accelerated Networking with <code class="systemitem">vhost-net</code></span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.vnet">#</a></h3></div></div></div><p>
    The <code class="systemitem">vhost-net</code> module is used to accelerate
    KVM's paravirtualized network drivers. It provides better latency and
    greater network throughput. Use the <code class="literal">vhost-net</code>
    driver by starting the guest with the following example command line:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>qemu-system-x86_64 [...] \
-netdev tap,id=guest0,vhost=on,script=no \
-net nic,model=virtio,netdev=guest0,macaddr=00:16:35:AF:94:4B</pre></div><p>
    Note that <code class="literal">guest0</code> is an identification string of the
    vhost-driven device.
   </p></div><div class="sect2 " id="kvm.qemu.multiqueue"><div class="titlepage"><div><div><h3 class="title" id="kvm.qemu.multiqueue"><span class="number">26.3.3 </span><span class="name">Scaling Network Performance with Multiqueue virtio-net</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.multiqueue">#</a></h3></div></div></div><p>
    As the number of virtual CPUs increases in VM Guests, QEMU offers a
    way of improving the network performance using
    <span class="emphasis"><em>multiqueue</em></span>. Multiqueue virtio-net scales the
    network performance by allowing VM Guest virtual CPUs to transfer
    packets in parallel. Multiqueue support is required on both the VM Host Server
    and VM Guest sides.
   </p><div id="idm139724851907280" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Performance Benefit</h6><p>
     The multiqueue virtio-net solution is most beneficial in the following
     cases:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Network traffic packets are large.
      </p></li><li class="listitem "><p>
       VM Guest has many connections active at the same time, mainly
       between the guest systems, or between the guest and the host, or
       between the guest and an external system.
      </p></li><li class="listitem "><p>
       The number of active queues is equal to the number of virtual CPUs in
       the VM Guest.
      </p></li></ul></div></div><div id="idm139724851901872" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     While multiqueue virtio-net increases the total network throughput, it
     increases CPU consumption as it uses of the virtual CPU's power.
    </p></div><div class="procedure " id="kvm.qemu.mq.enable"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 26.1: </span><span class="name">How to Enable Multiqueue virtio-net </span><a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.mq.enable">#</a></h6></div><div class="procedure-contents"><p>
     The following procedure lists important steps to enable the multiqueue
     feature with <code class="command">qemu-system-ARCH</code>. It assumes that a tap
     network device with multiqueue capability (supported since kernel
     version 3.8) is set up on the VM Host Server.
    </p><ol class="procedure" type="1"><li class="step "><p>
      In <code class="command">qemu-system-ARCH</code>, enable multiqueue for the tap
      device:
     </p><div class="verbatim-wrap"><pre class="screen">-netdev tap,vhost=on,queues=<em class="replaceable ">2*N</em></pre></div><p>
      where <code class="literal">N</code> stands for the number of queue pairs.
     </p></li><li class="step "><p>
      In <code class="command">qemu-system-ARCH</code>, enable multiqueue and specify
      MSI-X (Message Signaled Interrupt) vectors for the virtio-net-pci
      device:
     </p><div class="verbatim-wrap"><pre class="screen">-device virtio-net-pci,mq=on,vectors=<em class="replaceable ">2*N+2</em></pre></div><p>
      where the formula for the number of MSI-X vectors results from: N
      vectors for TX (transmit) queues, N for RX (receive) queues, one for
      configuration purposes, and one for possible VQ (vector quantization)
      control.
     </p></li><li class="step "><p>
      In VM Guest, enable multiqueue on the relevant network interface
      (<code class="literal">eth0</code> in this example):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> ethtool -L eth0 combined 2*N</pre></div></li></ol></div></div><p>
    The resulting <code class="command">qemu-system-ARCH</code> command line will look
    similar to the following example:
   </p><div class="verbatim-wrap"><pre class="screen">qemu-system-x86_64 [...] -netdev tap,id=guest0,queues=8,vhost=on \
-device virtio-net-pci,netdev=guest0,mq=on,vectors=10</pre></div><p>
    Note that the <code class="literal">id</code> of the network device
    (<code class="literal">guest0</code> ) needs to be identical for both options.
   </p><p>
    Inside the running VM Guest, specify the following command with
    <code class="systemitem">root</code> privileges:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> ethtool -L eth0 combined 8</pre></div><p>
    Now the guest system networking uses the multiqueue support from the
    <code class="command">qemu-system-ARCH</code> hypervisor.
   </p></div><div class="sect2 " id="kvm.vfio"><div class="titlepage"><div><div><h3 class="title" id="kvm.vfio"><span class="number">26.3.4 </span><span class="name">VFIO: Secure Direct Access to Devices</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.vfio">#</a></h3></div></div></div><p>
    Directly assigning a PCI device to a VM Guest (PCI pass-through) avoids
    performance issues caused by avoiding any emulation in performance-critical
    paths. VFIO replaces the traditional KVM PCI Pass-Through device
    assignment. A prerequisite for this feature is a VM Host Server configuration
    as described in <a class="xref" href="chap.virtualization.introduction.html#ann.vt.io.require" title="Important: Requirements for VFIO and SR-IOV">Important: Requirements for VFIO and SR-IOV</a>.
   </p><p>
    To be able to assign a PCI device via VFIO to a VM Guest, you need to
    find out which IOMMU Group it belongs to. The
    <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.iommu" title="IOMMU">IOMMU</a> (input/output memory
    management unit that connects a direct memory access-capable I/O bus to
    the main memory) API supports the notion of groups. A group is a set of
    devices that can be isolated from all other devices in the system.
    Groups are therefore the unit of ownership used by
    <a class="xref" href="chap.virtualization.introduction.html#vt.io.vfio">VFIO</a>.
   </p><div class="procedure " id="idm139724851879232"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 26.2: </span><span class="name">Assigning a PCI Device to a VM Guest via VFIO </span><a title="Permalink" class="permalink" href="cha.qemu.host.html#idm139724851879232">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Identify the host PCI device to assign to the guest.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> lspci -nn
[...]
00:10.0 Ethernet controller [0200]: Intel Corporation 82576 \
Virtual Function [8086:10ca] (rev 01)
[...]</pre></div><p>
      Note down the device ID (<code class="literal">00:10.0</code> in this case) and
      the vendor ID (<code class="literal">8086:10ca</code>).
     </p></li><li class="step "><p>
      Find the IOMMU group of this device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> readlink /sys/bus/pci/devices/0000\:00\:10.0/iommu_group
../../../kernel/iommu_groups/20</pre></div><p>
      The IOMMU group for this device is <code class="literal">20</code>. Now you can
      check the devices belonging to the same IOMMU group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> ls -l /sys/bus/pci/devices/0000:01:10.0/iommu_group/devices/0000:01:10.0
[...] 0000:00:1e.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0
[...] 0000:01:10.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.0
[...] 0000:01:10.1 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.1</pre></div></li><li class="step "><p>
      Unbind the device from the device driver:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo "0000:01:10.0" &gt; /sys/bus/pci/devices/0000\:01\:10.0/driver/unbind</pre></div></li><li class="step "><p>
      Bind the device to the vfio-pci driver using the vendor ID from step
      1:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> echo "8086 153a" &gt; /sys/bus/pci/drivers/vfio-pci/new_id</pre></div><p>
      A new device
      <code class="filename">/dev/vfio/<em class="replaceable ">IOMMU_GROUP</em></code>
      will be created as a result, <code class="filename">/dev/vfio/20</code> in this
      case.
     </p></li><li class="step "><p>
      Change the ownership of the newly created device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> chown qemu.qemu /dev/vfio/<em class="replaceable ">DEVICE</em></pre></div></li><li class="step "><p>
      Now run the VM Guest with the PCI device assigned.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> qemu-system-<em class="replaceable ">ARCH</em> [...] -device
     vfio-pci,host=00:10.0,id=<em class="replaceable ">ID</em></pre></div></li></ol></div></div><div id="idm139724851859744" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: No Hotplugging</h6><p>
     As of <span class="productname"><span class="phrase">openSUSE Leap</span></span> <span class="productnumber"><span class="phrase">42.3</span></span> hotplugging of PCI devices passed
     to a VM Guest via VFIO is not supported.
    </p></div><p>
    You can find more detailed information on the
    <a class="xref" href="chap.virtualization.introduction.html#vt.io.vfio">VFIO</a> driver in the
    <code class="filename">/usr/src/linux/Documentation/vfio.txt</code> file (package
    <code class="systemitem">kernel-source</code> needs to be installed).
   </p></div><div class="sect2 " id="kvm.qemu.virtfs"><div class="titlepage"><div><div><h3 class="title" id="kvm.qemu.virtfs"><span class="number">26.3.5 </span><span class="name">VirtFS: Sharing Directories between Host and Guests</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.virtfs">#</a></h3></div></div></div><p>
    VM Guests usually run in a separate computing space—they are
    provided their own memory range, dedicated CPUs, and file system space.
    The ability to share parts of the VM Host Server's file system makes the
    virtualization environment more flexible by simplifying mutual data
    exchange. Network file systems, such as CIFS and NFS, have been the
    traditional way of sharing directories. But as they are not specifically
    designed for virtualization purposes, they suffer from major performance
    and feature issues.
   </p><p>
    KVM introduces a new optimized method called
    <span class="emphasis"><em>VirtFS</em></span> (sometimes called <span class="quote">“<span class="quote">file system
    pass-through</span>”</span>). VirtFS uses a paravirtual file system driver,
    which avoids converting the guest application file system operations
    into block device operations, and then again into host file system
    operations.
   </p><p>
    You typically use VirtFS for the following situations:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      To access a shared directory from several guests, or to provide
      guest-to-guest file system access.
     </p></li><li class="listitem "><p>
      To replace the virtual disk as the root file system to which the
      guest's RAM disk connects during the guest boot process.
     </p></li><li class="listitem "><p>
      To provide storage services to different customers from a single host
      file system in a cloud environment.
     </p></li></ul></div><div class="sect3 " id="kvm.qemu.virtfs.implement"><div class="titlepage"><div><div><h4 class="title" id="kvm.qemu.virtfs.implement"><span class="number">26.3.5.1 </span><span class="name">Implementation</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.virtfs.implement">#</a></h4></div></div></div><p>
     In QEMU, the implementation of VirtFS is simplified by defining two
     types of devices:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="literal">virtio-9p-pci</code> device which transports protocol
       messages and data between the host and the guest.
      </p></li><li class="listitem "><p>
       <code class="literal">fsdev</code> device which defines the export file system
       properties, such as file system type and security model.
      </p></li></ul></div><div class="complex-example"><div class="example" id="ex.qemu.virtfs.host"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 26.1: </span><span class="name">Exporting Host's File System with VirtFS </span><a title="Permalink" class="permalink" href="cha.qemu.host.html#ex.qemu.virtfs.host">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> qemu-system-x86_64 [...] \
-fsdev local,id=exp1<span id="co.virtfs.host.id"></span><span class="callout">1</span>,path=/tmp/<span id="co.virtfs.host.path"></span><span class="callout">2</span>,security_model=mapped<span id="co.virtfs.host.sec_model"></span><span class="callout">3</span> \
-device virtio-9p-pci,fsdev=exp1<span id="co.virtfs.host.fsdev"></span><span class="callout">4</span>,mount_tag=v_tmp<span id="co.virtfs.host.mnt_tag"></span><span class="callout">5</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.virtfs.host.id"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
        Identification of the file system to be exported.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.virtfs.host.path"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
        File system path on the host to be exported.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.virtfs.host.sec_model"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
        Security model to be used—<code class="literal">mapped</code> keeps the
        guest file system modes and permissions isolated from the host,
        while <code class="literal">none</code> invokes a <span class="quote">“<span class="quote">pass-through</span>”</span>
        security model in which permission changes on the guest's files are
        reflected on the host as well.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.virtfs.host.fsdev"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
        The exported file system ID defined before with <code class="literal">-fsdev
        id=</code> .
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.virtfs.host.mnt_tag"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
        Mount tag used later on the guest to mount the exported file system.
       </p></td></tr></table></div><p>
      Such an exported file system can be mounted on the guest as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">sudo</code> mount -t 9p -o trans=virtio v_tmp /mnt</pre></div><p>
      where <code class="literal">v_tmp</code> is the mount tag defined earlier with
      <code class="literal">-device mount_tag=</code> and <code class="literal">/mnt</code> is
      the mount point where you want to mount the exported file system.
     </p></div></div></div></div></div><div class="sect2 " id="kvm.qemu.ksm"><div class="titlepage"><div><div><h3 class="title" id="kvm.qemu.ksm"><span class="number">26.3.6 </span><span class="name">KSM: Sharing Memory Pages between Guests</span> <a title="Permalink" class="permalink" href="cha.qemu.host.html#kvm.qemu.ksm">#</a></h3></div></div></div><p>
    Kernel Same Page Merging (<a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a>) is a
    Linux kernel feature that merges identical memory pages from multiple
    running processes into one memory region. Because KVM guests run as
    processes under Linux, <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a> provides
    the memory overcommit feature to hypervisors for more efficient use of
    memory. Therefore, if you need to run multiple virtual machines on a
    host with limited memory, <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a> may be
    helpful to you.
   </p><p>
    <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a> stores its status information in
    the files under the <code class="filename">/sys/kernel/mm/ksm</code> directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls -1 /sys/kernel/mm/ksm
full_scans
merge_across_nodes
pages_shared
pages_sharing
pages_to_scan
pages_unshared
pages_volatile
run
sleep_millisecs</pre></div><p>
    For more information on the meaning of the
    <code class="filename">/sys/kernel/mm/ksm/*</code> files, see
    <code class="filename">/usr/src/linux/Documentation/vm/ksm.txt</code> (package
    <code class="systemitem">kernel-source</code>).
   </p><p>
    To use <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a>, do the following.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Although <span class="productname"><span class="phrase">openSUSE Leap</span></span> includes
      <a class="xref" href="gloss.vt.glossary.html#gloss.vt.acronym.ksm" title="KSM">KSM</a> support in the kernel, it is
      disabled by default. To enable it, run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root">root # </code>echo 1 &gt; /sys/kernel/mm/ksm/run</pre></div></li><li class="step "><p>
      Now run several VM Guests under KVM and inspect the content of
      files <code class="filename">pages_sharing</code> and
      <code class="filename">pages_shared</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>while [ 1 ]; do cat /sys/kernel/mm/ksm/pages_shared; sleep 1; done
13522
13523
13519
13518
13520
13520
13528</pre></div></li></ol></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha.qemu.guest_inst.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 27 </span>Guest Installation</span></a><a class="nav-link" href="cha.qemu.overview.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 25 </span>QEMU Overview</span></a></div><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>