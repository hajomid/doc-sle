<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Tuning the Memory Management Subsystem | System Analysis and Tuning Guide | SUSE Linux Enterprise Server 12 SP1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><meta name="product-name" content="SUSE Linux Enterprise Server" /><meta name="product-number" content="12 SP1" /><meta name="book-title" content="System Analysis and Tuning Guide" /><meta name="chapter-title" content="Chapter 14. Tuning the Memory Management Subsystem" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="fs@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE Linux Enterprise Server 12 SP1" /><link rel="home" href="index.html" title="SUSE Linux Enterprise Server Documentation" /><link rel="up" href="part.tuning.kernel.html" title="Part V. Kernel Tuning" /><link rel="prev" href="cha.tuning.taskscheduler.html" title="Chapter 13. Tuning the Task Scheduler" /><link rel="next" href="cha.tuning.network.html" title="Chapter 15. Tuning the Network" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="book.sle.tuning.html">System Analysis and Tuning Guide</a><span> › </span><a class="crumb" href="part.tuning.kernel.html">Kernel Tuning</a><span> › </span><a class="crumb" href="cha.tuning.memory.html">Tuning the Memory Management Subsystem</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>System Analysis and Tuning Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="preface.tuning.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="part.tuning.basics.html"><span class="number">I </span><span class="name">Basics</span></a><ol><li class="inactive"><a href="cha.tuning.basics.html"><span class="number">1 </span><span class="name">General Notes on System Tuning</span></a></li></ol></li><li class="inactive"><a href="part.tuning.monitoring.html"><span class="number">II </span><span class="name">System Monitoring</span></a><ol><li class="inactive"><a href="cha.util.html"><span class="number">2 </span><span class="name">System Monitoring Utilities</span></a></li><li class="inactive"><a href="cha.tuning.logfiles.html"><span class="number">3 </span><span class="name">Analyzing and Managing System Log Files</span></a></li></ol></li><li class="inactive"><a href="part.tuning.kerneltrace.html"><span class="number">III </span><span class="name">Kernel Monitoring</span></a><ol><li class="inactive"><a href="cha.tuning.systemtap.html"><span class="number">4 </span><span class="name">SystemTap—Filtering and Analyzing System Data</span></a></li><li class="inactive"><a href="cha.tuning.kprobes.html"><span class="number">5 </span><span class="name">Kernel Probes</span></a></li><li class="inactive"><a href="cha.perf.html"><span class="number">6 </span><span class="name">Hardware-Based Performance Monitoring with Perf</span></a></li><li class="inactive"><a href="cha.tuning.oprofile.html"><span class="number">7 </span><span class="name">OProfile—System-Wide Profiler</span></a></li></ol></li><li class="inactive"><a href="part.tuning.resources.html"><span class="number">IV </span><span class="name">Resource Management</span></a><ol><li class="inactive"><a href="cha.tuning.resources.html"><span class="number">8 </span><span class="name">General System Resource Management</span></a></li><li class="inactive"><a href="cha.tuning.cgroups.html"><span class="number">9 </span><span class="name">Kernel Control Groups</span></a></li><li class="inactive"><a href="cha.tuning.numactl.html"><span class="number">10 </span><span class="name">Automatic Non-Uniform Memory Access (NUMA) Balancing</span></a></li><li class="inactive"><a href="cha.tuning.power.html"><span class="number">11 </span><span class="name">Power Management</span></a></li></ol></li><li class="inactive"><a href="part.tuning.kernel.html"><span class="number">V </span><span class="name">Kernel Tuning</span></a><ol><li class="inactive"><a href="cha.tuning.io.html"><span class="number">12 </span><span class="name">Tuning I/O Performance</span></a></li><li class="inactive"><a href="cha.tuning.taskscheduler.html"><span class="number">13 </span><span class="name">Tuning the Task Scheduler</span></a></li><li class="inactive"><a href="cha.tuning.memory.html"><span class="number">14 </span><span class="name">Tuning the Memory Management Subsystem</span></a></li><li class="inactive"><a href="cha.tuning.network.html"><span class="number">15 </span><span class="name">Tuning the Network</span></a></li></ol></li><li class="inactive"><a href="part.tuning.dumps.html"><span class="number">VI </span><span class="name">Handling System Dumps</span></a><ol><li class="inactive"><a href="cha.tuning.tracing.html"><span class="number">16 </span><span class="name">Tracing Tools</span></a></li><li class="inactive"><a href="cha.tuning.kexec.html"><span class="number">17 </span><span class="name">Kexec and Kdump</span></a></li></ol></li><li class="inactive"><a href="part.tuning.ptp.html"><span class="number">VII </span><span class="name">Synchronized Clocks with Precision Time Protocol</span></a><ol><li class="inactive"><a href="cha.tuning.ptp.html"><span class="number">18 </span><span class="name">Precision Time Protocol</span></a></li></ol></li><li class="inactive"><a href="app.tuning.docupdates.html"><span class="number">A </span><span class="name">Documentation Updates</span></a></li><li class="inactive"><a href="bk06apb.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 13. Tuning the Task Scheduler" href="cha.tuning.taskscheduler.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 15. Tuning the Network" href="cha.tuning.network.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="book.sle.tuning.html">System Analysis and Tuning Guide</a><span> › </span><a class="crumb" href="part.tuning.kernel.html">Kernel Tuning</a><span> › </span><a class="crumb" href="cha.tuning.memory.html">Tuning the Memory Management Subsystem</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 13. Tuning the Task Scheduler" href="cha.tuning.taskscheduler.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 15. Tuning the Network" href="cha.tuning.network.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="cha.tuning.memory"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">12 SP1</span></span></span></div><div><h2 class="title"><span class="number">14 </span><span class="name">Tuning the Memory Management Subsystem</span> </h2><div class="doc-status"><ul><li><span class="ds-label">Filename: </span>tuning_memory.xml</li><li><span class="ds-label">ID: </span>cha.tuning.memory</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha.tuning.memory.html#cha.tuning.memory.usage"><span class="number">14.1 </span><span class="name">Memory Usage</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.memory.html#cha.tuning.memory.optimize"><span class="number">14.2 </span><span class="name">Reducing Memory Usage</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.memory.html#cha.tuning.memory.vm"><span class="number">14.3 </span><span class="name">Virtual Memory Manager (VM) Tunable Parameters</span></a></span></dt><dt><span class="sect1"><a href="cha.tuning.memory.html#cha.tuning.memory.monitoring"><span class="number">14.4 </span><span class="name">Monitoring VM Behavior</span></a></span></dt></dl></div></div><p>
  To understand and tune the memory management behavior of the
  kernel, it is important to first have an overview of how it works and
  cooperates with other subsystems.
 </p><p>
  
  The memory management subsystem, also called the virtual memory manager,
  will subsequently be called <span class="quote">“<span class="quote">VM</span>”</span>. The role of the VM
  is to manage the allocation of physical memory (RAM) for the entire kernel
  and user programs. It is also responsible for providing a virtual memory
  environment for user processes (managed via POSIX APIs with Linux
  extensions). Finally, the VM is responsible for freeing up RAM when there
  is a shortage, either by trimming caches or swapping out
  <span class="quote">“<span class="quote">anonymous</span>”</span> memory.
 </p><p>
  The most important thing to understand when examining and tuning VM is how
  its caches are managed. The basic goal of the VM's caches is to minimize
  the cost of I/O as generated by swapping and file system operations
  (including network file systems). This is achieved by avoiding I/O
  completely, or by submitting I/O in better patterns.
 </p><p>
  Free memory will be used and filled up by these caches as required. The
  more memory is available for caches and anonymous memory, the more
  effectively caches and swapping will operate. However, if a memory
  shortage is encountered, caches will be trimmed or memory will be swapped
  out.
 </p><p>
  For a particular workload, the first thing that can be done to improve
  performance is to increase memory and reduce the frequency that memory
  must be trimmed or swapped. The second thing is to change the way caches
  are managed by changing kernel parameters.
 </p><p>
  Finally, the workload itself should be examined and tuned as well. If an
  application is allowed to run more processes or threads, effectiveness of
  VM caches can be reduced, if each process is operating in its own area of
  the file system. Memory overheads are also increased. If applications
  allocate their own buffers or caches, larger caches will mean that less
  memory is available for VM caches. However, more processes and threads can
  mean more opportunity to overlap and pipeline I/O, and may take better
  advantage of multiple cores. Experimentation will be required for the best
  results.
 </p><div class="sect1 " id="cha.tuning.memory.usage"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.memory.usage"><span class="number">14.1 </span><span class="name">Memory Usage</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage">#</a></h2></div></div></div><p>
   Memory allocations in general can be characterized as
   <span class="quote">“<span class="quote">pinned</span>”</span> (also known as <span class="quote">“<span class="quote">unreclaimable</span>”</span>),
   <span class="quote">“<span class="quote">reclaimable</span>”</span> or <span class="quote">“<span class="quote">swappable</span>”</span>.
  </p><div class="sect2 " id="cha.tuning.memory.usage.anonymous"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.anonymous"><span class="number">14.1.1 </span><span class="name">Anonymous Memory</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.anonymous">#</a></h3></div></div></div><p>
    Anonymous memory tends to be program heap and stack memory (for example,
    <code class="literal">&gt;malloc()</code>). It is reclaimable, except in special
    cases such as <code class="literal">mlock</code> or if there is no available swap
    space. Anonymous memory must be written to swap before it can be
    reclaimed. Swap I/O (both swapping in and swapping out pages) tends to
    be less efficient than pagecache I/O, because of allocation and access
    patterns.
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.pagecache"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.pagecache"><span class="number">14.1.2 </span><span class="name">Pagecache</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.pagecache">#</a></h3></div></div></div><p>
    A cache of file data. When a file is read from disk or network, the
    contents are stored in pagecache. No disk or network access is required,
    if the contents are up-to-date in pagecache. tmpfs and shared memory
    segments count toward pagecache.
   </p><p>
    When a file is written to, the new data is stored in pagecache before
    being written back to a disk or the network (making it a write-back
    cache). When a page has new data not written back yet, it is called
    <span class="quote">“<span class="quote">dirty</span>”</span>. Pages not classified as dirty are
    <span class="quote">“<span class="quote">clean</span>”</span>. Clean pagecache pages can be reclaimed if there is
    a memory shortage by simply freeing them. Dirty pages must first be made
    clean before being reclaimed.
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.buffercache"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.buffercache"><span class="number">14.1.3 </span><span class="name">Buffercache</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.buffercache">#</a></h3></div></div></div><p>
    This is a type of pagecache for block devices (for example, /dev/sda). A
    file system typically uses the buffercache when accessing its on-disk
    metadata structures such as inode tables, allocation bitmaps, and so
    forth. Buffercache can be reclaimed similarly to pagecache.
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.bufferheads"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.bufferheads"><span class="number">14.1.4 </span><span class="name">Buffer Heads</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.bufferheads">#</a></h3></div></div></div><p>
    Buffer heads are small auxiliary structures that tend to be allocated
    upon pagecache access. They can generally be reclaimed easily when the
    pagecache or buffercache pages are clean.
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.writeback"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.writeback"><span class="number">14.1.5 </span><span class="name">Writeback</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.writeback">#</a></h3></div></div></div><p>
    As applications write to files, the pagecache (and buffercache) becomes
    dirty. When pages have been dirty for a given amount of time, or when
    the amount of dirty memory reaches a specified number of pages in bytes
    (<span class="emphasis"><em>vm.dirty_background_bytes</em></span>), the kernel begins
    writeback. Flusher threads perform writeback in the background and allow
    applications to continue running. If the I/O cannot keep up with
    applications dirtying pagecache, and dirty data reaches a critical
    setting (<span class="emphasis"><em>vm.dirty_bytes</em></span>), then applications begin
    to be throttled to prevent dirty data exceeding this threshold.
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.readahead"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.readahead"><span class="number">14.1.6 </span><span class="name">Readahead</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.readahead">#</a></h3></div></div></div><p>
    The VM monitors file access patterns and may attempt to perform
    readahead. Readahead reads pages into the pagecache from the file system
    that have not been requested yet. It is done to allow fewer,
    larger I/O requests to be submitted (more efficient). And for I/O to be
    pipelined (I/O performed at the same time as the application is
    running).
   </p></div><div class="sect2 " id="cha.tuning.memory.usage.vfs"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.usage.vfs"><span class="number">14.1.7 </span><span class="name">VFS caches</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.vfs">#</a></h3></div></div></div><div class="sect3 " id="cha.tuning.memory.usage.vfs.inode"><div class="titlepage"><div><div><h4 class="title" id="cha.tuning.memory.usage.vfs.inode"><span class="number">14.1.7.1 </span><span class="name">Inode Cache</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.vfs.inode">#</a></h4></div></div></div><p>
     This is an in-memory cache of the inode structures for each file
     system. These contain attributes such as the file size, permissions and
     ownership, and pointers to the file data.
    </p></div><div class="sect3 " id="cha.tuning.memory.usage.vfs.dir_entry"><div class="titlepage"><div><div><h4 class="title" id="cha.tuning.memory.usage.vfs.dir_entry"><span class="number">14.1.7.2 </span><span class="name">Directory Entry Cache</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.usage.vfs.dir_entry">#</a></h4></div></div></div><p>
     This is an in-memory cache of the directory entries in the system.
     These contain a name (the name of a file), the inode which it refers
     to, and children entries. This cache is used when traversing the
     directory structure and accessing a file by name.
    </p></div></div></div><div class="sect1 " id="cha.tuning.memory.optimize"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.memory.optimize"><span class="number">14.2 </span><span class="name">Reducing Memory Usage</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.optimize">#</a></h2></div></div></div><div class="sect2 " id="cha.tuning.memory.optimize.malloc"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.optimize.malloc"><span class="number">14.2.1 </span><span class="name">Reducing malloc (Anonymous) Usage</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.optimize.malloc">#</a></h3></div></div></div><p>
    Applications running on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP1</span></span> can allocate
    more memory compared to <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 10. This is because of
    <code class="systemitem">glibc</code> changing its default
    behavior while allocating userspace memory. See
    <a class="link" href="http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html" target="_blank">http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html</a>
    for explanation of these parameters.
   </p><p>
    To restore a <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 10-like behavior, M_MMAP_THRESHOLD should
    be set to 128*1024. This can be done with mallopt() call from the
    application, or via setting MALLOC_MMAP_THRESHOLD environment variable
    before running the application.
   </p></div><div class="sect2 " id="cha.tuning.memory.optimize.overhead"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.optimize.overhead"><span class="number">14.2.2 </span><span class="name">Reducing Kernel Memory Overheads</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.optimize.overhead">#</a></h3></div></div></div><p>
    Kernel memory that is reclaimable (caches, described above) will be
    trimmed automatically during memory shortages. Most other kernel memory
    cannot be easily reduced but is a property of the workload given to the
    kernel.
   </p><p>
    Reducing the requirements of the userspace workload will reduce the
    kernel memory usage (fewer processes, fewer open files and sockets,
    etc.)
   </p></div><div class="sect2 " id="cha.tuning.memory.optimize.cgoups"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.optimize.cgoups"><span class="number">14.2.3 </span><span class="name">Memory Controller (Memory Cgroups)</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.optimize.cgoups">#</a></h3></div></div></div><p>
    If the memory cgroups feature is not needed, it can be switched off by
    passing cgroup_disable=memory on the kernel command line, reducing
    memory consumption of the kernel a bit.
   </p></div></div><div class="sect1 " id="cha.tuning.memory.vm"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.memory.vm"><span class="number">14.3 </span><span class="name">Virtual Memory Manager (VM) Tunable Parameters</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm">#</a></h2></div></div></div><p>
   When tuning the VM it should be understood that some  changes will
   take time to affect the workload and take full effect. If the workload
   changes throughout the day, it may behave very differently at different
   times. A change that increases throughput under some conditions may
   decrease it under other conditions.
  </p><div class="sect2 " id="cha.tuning.memory.vm.reclaim"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.vm.reclaim"><span class="number">14.3.1 </span><span class="name">Reclaim Ratios</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm.reclaim">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="idm140016693773920"><span class="term "><code class="filename">/proc/sys/vm/swappiness</code>
     </span></dt><dd><p>
       This control is used to define how aggressively the kernel swaps out
       anonymous memory relative to pagecache and other caches. Increasing
       the value increases the amount of swapping. The default value is
       <code class="literal">60</code>.
      </p><p>
       Swap I/O tends to be much less efficient than other I/O. However,
       some pagecache pages will be accessed much more frequently than less
       used anonymous memory. The right balance should be found here.
      </p><p>
       If swap activity is observed during slowdowns, it may be worth
       reducing this parameter. If there is a lot of I/O activity and the
       amount of pagecache in the system is rather small, or if there are
       large dormant applications running, increasing this value might
       improve performance.
      </p><p>
       Note that the more data is swapped out, the longer the system will
       take to swap data back in when it is needed.
      </p></dd><dt id="idm140016693769216"><span class="term "><code class="filename">/proc/sys/vm/vfs_cache_pressure</code>
     </span></dt><dd><p>
       This variable controls the tendency of the kernel to reclaim the
       memory which is used for caching of VFS caches, versus pagecache and
       swap. Increasing this value increases the rate at which VFS caches
       are reclaimed.
      </p><p>
       It is difficult to know when this should be changed, other than by
       experimentation. The <code class="command">slabtop</code> command (part of the
       package <code class="systemitem">procps</code>) shows top
       memory objects used by the kernel. The vfs caches are the "dentry"
       and the "*_inode_cache" objects. If these are consuming a large
       amount of memory in relation to pagecache, it may be worth trying to
       increase pressure. Could also help to reduce swapping. The default
       value is <code class="literal">100</code>.
      </p></dd><dt id="idm140016693764464"><span class="term "><code class="filename">/proc/sys/vm/min_free_kbytes</code>
     </span></dt><dd><p>
       This controls the amount of memory that is kept free for use by
       special reserves including <span class="quote">“<span class="quote">atomic</span>”</span> allocations (those
       which cannot wait for reclaim). This should not normally be lowered
       unless the system is being very carefully tuned for memory usage
       (normally useful for embedded rather than server applications). If
       <span class="quote">“<span class="quote">page allocation failure</span>”</span> messages and stack traces are
       frequently seen in logs, min_free_kbytes could be increased until the
       errors disappear. There is no need for concern, if these messages are
       very infrequent. The default value depends on the amount of RAM.
      </p></dd></dl></div></div><div class="sect2 " id="cha.tuning.memory.vm.writeback"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.vm.writeback"><span class="number">14.3.2 </span><span class="name">Writeback Parameters</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm.writeback">#</a></h3></div></div></div><p>
    One important change in writeback behavior since <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 10 is
    that modification to file-backed mmap() memory is accounted immediately
    as dirty memory (and subject to writeback). Whereas previously it would
    only be subject to writeback after it was unmapped, upon an msync()
    system call, or under heavy memory pressure.
   </p><p>
    Some applications do not expect mmap modifications to be subject to such
    writeback behavior, and performance can be reduced. Berkeley DB (and
    applications using it) is one known example that can cause problems.
    Increasing writeback ratios and times can improve this type of slowdown.
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm140016693756464"><span class="term "><code class="filename">/proc/sys/vm/dirty_background_ratio</code>
     </span></dt><dd><p>
       This is the percentage of the total amount of free and reclaimable
       memory. When the amount of dirty pagecache exceeds this percentage,
       writeback threads start writing back dirty memory. The default value
       is <code class="literal">10</code> (%).
      </p></dd><dt id="idm140016693753696"><span class="term "><code class="filename">/proc/sys/vm/dirty_background_bytes</code>
     </span></dt><dd><p>
       This is the percentage of the total amount of dirty memory at which
       the background kernel flusher threads will start writeback.
       <code class="filename">dirty_background_bytes</code> is the counterpart of
       <code class="filename">dirty_background_ratio</code>. If one of them is set,
       the other one will automatically be read as <code class="literal">0</code>.
      </p></dd><dt id="idm140016693750048"><span class="term "><code class="filename">/proc/sys/vm/dirty_ratio</code>
     </span></dt><dd><p>
       Similar percentage value as for
       <code class="filename">dirty_background_ratio</code>. When this is exceeded,
       applications that want to write to the pagecache are blocked and
       start performing writeback as well. The default value is
       <code class="literal">20</code> (%).
      </p></dd><dt id="idm140016693746864"><span class="term "><code class="filename">/proc/sys/vm/dirty_bytes</code>
     </span></dt><dd><p>
       Contains the amount of dirty memory (in percent) at which a process
       generating disk writes will itself start writeback. The minimum value
       allowed for <code class="filename">dirty_bytes</code> is two pages (in bytes);
       any value lower than this limit will be ignored and the old
       configuration will be retained.
      </p><p>
       <code class="filename">dirty_bytes</code> is the counterpart of
       <code class="filename">dirty_ratio</code>.If one of them is set, the other one
       will automatically be read as <code class="literal">0</code>.
      </p></dd><dt id="idm140016693742160"><span class="term "><code class="filename">/proc/sys/vm/dirty_expires</code>
     </span></dt><dd><p>
       Data which has been dirty in-memory for longer than this interval
       will be written out next time a flusher thread wakes up. Expiration
       is measured based on the modification time of a file's inode.
       Therefore, multiple dirtied pages from the same file will all be
       written when the interval is exceeded.
      </p></dd></dl></div><p>
    <code class="filename">dirty_background_ratio</code> and
    <code class="filename">dirty_ratio</code> together determine the pagecache
    writeback behavior. If these values are increased, more dirty memory is
    kept in the system for a longer time. With more dirty memory allowed in
    the system, the chance to improve throughput by avoiding writeback I/O
    and to submitting more optimal I/O patterns increases. However, more
    dirty memory can either harm latency when memory needs to be reclaimed
    or at points of data integrity (<span class="quote">“<span class="quote">sync points</span>”</span>) when it
    needs to be written back to disk.
   </p></div><div class="sect2 " id="cha.tuning.memory.vm.writetiming"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.vm.writetiming"><span class="number">14.3.3 </span><span class="name">Timing Differences of I/O Writes between SUSE Linux Enterprise 12 and SUSE Linux Enterprise 11</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm.writetiming">#</a></h3></div></div></div><p>
    The system is required to limit what percentage of the system's memory
    contains file-backed data that needs writing to disk. This guarantees
    that the system can always allocate the necessary data structures to
    complete I/O. The maximum amount of memory that may be dirty and
    requires writing at any given time is controlled by
    <code class="literal">vm.dirty_ratio</code>
    (<code class="filename">/proc/sys/vm/dirty_ratio</code>). The defaults are:
   </p><div class="verbatim-wrap"><pre class="screen">SLE-11-SP3:     vm.dirty_ratio = 40
SLE-12:         vm.dirty_ratio = 20</pre></div><p>
    The primary advantage of using the lower ratio in SUSE Linux Enterprise 12 is that
    page reclamation and allocation in low memory situations completes
    faster as there is a higher probability that old clean pages will be
    quickly found and discarded. The secondary advantage is that if all
    data on the system must be synchronized, then the time to complete the
    operation on SUSE Linux Enterprise 12 will be lower than SUSE Linux Enterprise 11 SP3 by default.
    Most workloads will not notice this change as data is synchronized with
    <code class="literal">fsync()</code> by the application or data is not dirtied
    quickly enough to hit the limits.
   </p><p>
    There are exceptions and if your application is affected by this, it
    will manifest as an unexpected stall during writes. To prove it is
    affected by dirty data rate limiting then monitor
    <code class="literal">/proc/<em class="replaceable ">PID_OF_APPLICATION</em>/stack</code>
    and it will be observed that the application spends significant time in
    <code class="literal">balance_dirty_pages_ratelimited</code>. If this is observed
    and it is a problem, then increase the value of
    <code class="literal">vm.dirty_ratio</code> to 40 to restore the SUSE Linux Enterprise 11 SP3
    behavior.
   </p><p>
    It is important to note that the overall I/O throughput is the same
    regardless of the setting. The only difference is the timing of when
    the I/O is queued.
   </p><p>
    This is an example of using <code class="command">dd</code> to asynchronously
    write 30% of memory to disk which would happen to be affected by the
    change in <code class="literal">vm.dirty_ratio</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`
<code class="prompt user">root # </code>sysctl vm.dirty_ratio=40
<code class="prompt user">root # </code>dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s
<code class="prompt user">root # </code>sysctl vm.dirty_ratio=20
dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s</pre></div><p>
    Note that the parameter affects the time it takes for the command to
    complete and the apparent write speed of the device. With
    <code class="literal">dirty_ratio=40</code>, more of the data is cached and
    written to disk in the background by the kernel. It is very important
    to note that the speed of I/O is identical in both cases. To
    demonstrate, this is the result when <code class="command">dd</code> synchronizes
    the data before exiting:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>sysctl vm.dirty_ratio=40
<code class="prompt user">root # </code>dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s
<code class="prompt user">root # </code>sysctl vm.dirty_ratio=20
<code class="prompt user">root # </code>dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s</pre></div><p>
    Note that <code class="literal">dirty_ratio</code> had almost no impact here and
    is within the natural variability of a command. Hence,
    <code class="literal">dirty_ratio</code> does not directly impact I/O performance
    but it may affect the apparent performance of a workload that writes
    data asynchronously without synchronizing.
   </p></div><div class="sect2 " id="cha.tuning.memory.vm.readahead"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.vm.readahead"><span class="number">14.3.4 </span><span class="name">Readahead parameters</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm.readahead">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="idm140016693717760"><span class="term "><code class="filename">/sys/block/<em class="replaceable ">&lt;bdev&gt;</em>/queue/read_ahead_kb</code>
     </span></dt><dd><p>
       If one or more processes are sequentially reading a file, the kernel
       reads some data in advance (ahead) to reduce the amount of
       time that processes need to wait for data to be available. The actual
       amount of data being read in advance is computed dynamically, based
       on how much "sequential" the I/O seems to be. This parameter sets the
       maximum amount of data that the kernel reads ahead for a single file.
       If you observe that large sequential reads from a file are not fast
       enough, you can try increasing this value. Increasing it too far may
       result in readahead thrashing where pagecache used for readahead is
       reclaimed before it can be used, or slowdowns because of a large
       amount of useless I/O. The default value is <code class="literal">512</code>
       (KB).
      </p></dd></dl></div></div><div class="sect2 " id="cha.tuning.memory.vm.more"><div class="titlepage"><div><div><h3 class="title" id="cha.tuning.memory.vm.more"><span class="number">14.3.5 </span><span class="name">Further VM Parameters</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.vm.more">#</a></h3></div></div></div><p>
    For the complete list of the VM tunable parameters, see
    <code class="filename">/usr/src/linux/Documentation/sysctl/vm.txt</code>
    (available after having installed the
    <code class="systemitem">kernel-source</code> package).
   </p></div></div><div class="sect1 " id="cha.tuning.memory.monitoring"><div class="titlepage"><div><div><h2 class="title" id="cha.tuning.memory.monitoring"><span class="number">14.4 </span><span class="name">Monitoring VM Behavior</span> <a title="Permalink" class="permalink" href="cha.tuning.memory.html#cha.tuning.memory.monitoring">#</a></h2></div></div></div><p>
   Some simple tools that can help monitor VM behavior:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     vmstat: This tool gives a good overview of what the VM is doing. See
     <a class="xref" href="cha.util.html#sec.util.multi.vmstat" title="2.1.1. vmstat">Section 2.1.1, “<code class="command">vmstat</code>”</a> for details.
    </p></li><li class="listitem "><p>
     <code class="filename">/proc/meminfo</code>: This file gives a detailed
     breakdown of where memory is being used. See
     <a class="xref" href="cha.util.html#sec.util.memory.meminfo" title="2.4.2. Detailed Memory Usage: /proc/meminfo">Section 2.4.2, “Detailed Memory Usage: <code class="filename">/proc/meminfo</code>”</a> for details.
    </p></li><li class="listitem "><p>
     <code class="command">slabtop</code>: This tool provides detailed information
     about kernel slab memory usage. buffer_head, dentry, inode_cache,
     ext3_inode_cache, etc. are the major caches. This command is available
     with the package <code class="systemitem">procps</code>.
    </p></li></ol></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha.tuning.network.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 15 </span>Tuning the Network</span></a><a class="nav-link" href="cha.tuning.taskscheduler.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 13 </span>Tuning the Task Scheduler</span></a></div><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>