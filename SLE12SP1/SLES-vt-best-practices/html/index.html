<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Virtualization Best Practices | SUSE Linux Enterprise SUSE Linux Enterprise Server 12 SP1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><meta name="product-name" content="SUSE Linux Enterprise SUSE Linux Enterprise Server" /><meta name="product-number" content="12 SP1" /><meta name="book-title" content="Virtualization Best Practices" /><link rel="home" href="index.html" title="Virtualization Best Practices" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="single-crumb" href="index.html"><span class="single-contents-icon"></span>Virtualization Best Practices</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="single-crumb" href="index.html"><span class="single-contents-icon"></span>Show Contents: Virtualization Best Practices</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="index.html#vt.best.scenario"><span class="number">1 </span><span class="name">Virtualization Scenarios</span></a></li><li class="inactive"><a href="index.html#vt.best.intro"><span class="number">2 </span><span class="name">Before Any Modification</span></a></li><li class="inactive"><a href="index.html#vt.best.reco"><span class="number">3 </span><span class="name">Recommended Usage</span></a></li><li class="inactive"><a href="index.html#idm140164021258608"><span class="number">4 </span><span class="name">I/O in Virtualization</span></a></li><li class="inactive"><a href="index.html#vt.best.hostlevel"><span class="number">5 </span><span class="name">Host-Level Configuration and Settings</span></a></li><li class="inactive"><a href="index.html#vt.best.multihost"><span class="number">6 </span><span class="name">Multi-Host Configuration and Settings</span></a></li><li class="inactive"><a href="index.html#vt.best.stor"><span class="number">7 </span><span class="name">VM Guest Images</span></a></li><li class="inactive"><a href="index.html#vt.best.vm.perf"><span class="number">8 </span><span class="name">Configurations and Settings Performed within the VM Guest</span></a></li><li class="inactive"><a href="index.html#vt.best.vm.setup.config"><span class="number">9 </span><span class="name">VM Guest-Specific Configurations and Settings</span></a></li><li class="inactive"><a href="index.html#idm140164020532128"><span class="number">10 </span><span class="name">Hypervisors Vs. Containers</span></a></li><li class="inactive"><a href="index.html#idm140164020447104"><span class="number">11 </span><span class="name">External References</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="article " id="article.vt.best.practices" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname">SUSE Linux Enterprise <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">12 SP1</span></span></span></h6><div><h1 class="title">Virtualization Best Practices </h1></div><div class="date"><span class="imprint-label">Publication Date: </span>02/12/2018</div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="index.html#vt.best.scenario"><span class="number">1 </span><span class="name">Virtualization Scenarios</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.intro"><span class="number">2 </span><span class="name">Before Any Modification</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.reco"><span class="number">3 </span><span class="name">Recommended Usage</span></a></span></dt><dt><span class="sect1"><a href="index.html#idm140164021258608"><span class="number">4 </span><span class="name">I/O in Virtualization</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.hostlevel"><span class="number">5 </span><span class="name">Host-Level Configuration and Settings</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.multihost"><span class="number">6 </span><span class="name">Multi-Host Configuration and Settings</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.stor"><span class="number">7 </span><span class="name">VM Guest Images</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.vm.perf"><span class="number">8 </span><span class="name">Configurations and Settings Performed within the VM Guest</span></a></span></dt><dt><span class="sect1"><a href="index.html#vt.best.vm.setup.config"><span class="number">9 </span><span class="name">VM Guest-Specific Configurations and Settings</span></a></span></dt><dt><span class="sect1"><a href="index.html#idm140164020532128"><span class="number">10 </span><span class="name">Hypervisors Vs. Containers</span></a></span></dt><dt><span class="sect1"><a href="index.html#idm140164020447104"><span class="number">11 </span><span class="name">External References</span></a></span></dt></dl></div></div><div class="sect1 " id="vt.best.scenario"><div class="titlepage"><div><div><h2 class="title" id="vt.best.scenario"><span class="number">1 </span><span class="name">Virtualization Scenarios</span> <a title="Permalink" class="permalink" href="index.html#vt.best.scenario">#</a></h2></div></div></div><p>
   Virtualization offers your environment a lot of capabilities. It can be
   used in multiple scenarios. To get more details about
   <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html" target="_blank">Virtualization
   Capabilities</a> and
   <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html" target="_blank">Virtualization
   Benefits</a> refer to the
   <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html" target="_blank">Virtualization
   Guide</a>.
  </p><p>
   This best practice will provide advice to help you make the best choices
   for your environment; it will recommend or discourage options based on
   your virtualization use case. Optimal configuration adjustments and
   performance tuning can increase VM Guest performance close to bare
   metal.
  </p></div><div class="sect1 " id="vt.best.intro"><div class="titlepage"><div><div><h2 class="title" id="vt.best.intro"><span class="number">2 </span><span class="name">Before Any Modification</span> <a title="Permalink" class="permalink" href="index.html#vt.best.intro">#</a></h2></div></div></div><div class="sect2 " id="vt.best.intro.backup"><div class="titlepage"><div><div><h3 class="title" id="vt.best.intro.backup"><span class="number">2.1 </span><span class="name">Back Up First</span> <a title="Permalink" class="permalink" href="index.html#vt.best.intro.backup">#</a></h3></div></div></div><p>
    Adjusting VM Guest and VM Host configuration can lead to data loss or
    an unstable state. It is very important to create backup configuration
    files, data, images, etc. before making any changes. Backups ensure the
    original configuration can be restored in the event of data loss or a
    miss-configuration.
   </p><div id="idm140164021272688" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
     Backup is mandatory before doing any tests, ensuring the ability to
     roll back to a usable/stable system or configuration. Do not perform
     tests or experiments on online, production systems.
    </p></div></div><div class="sect2 " id="vt.best.intro.testing"><div class="titlepage"><div><div><h3 class="title" id="vt.best.intro.testing"><span class="number">2.2 </span><span class="name">Test Your Configurations</span> <a title="Permalink" class="permalink" href="index.html#vt.best.intro.testing">#</a></h3></div></div></div><p>
    The efficiency of a virtualization environment depends on many factors.
    This guide is provided as a reference to help make good choices when
    configuring virtualization in a production environment. Nothing is
    <span class="emphasis"><em>carved in stone</em></span>. Hardware, workloads, resource
    capacity, etc. should all be considered when planning, testing, and
    deploying your virtualization infra-structure. Testing your virtualized
    workloads is vital to a successful virtualization implementation.
   </p></div></div><div class="sect1 " id="vt.best.reco"><div class="titlepage"><div><div><h2 class="title" id="vt.best.reco"><span class="number">3 </span><span class="name">Recommended Usage</span> <a title="Permalink" class="permalink" href="index.html#vt.best.reco">#</a></h2></div></div></div><div class="sect2 " id="vt.best.intro.libvirt"><div class="titlepage"><div><div><h3 class="title" id="vt.best.intro.libvirt"><span class="number">3.1 </span><span class="name">Prefer <code class="systemitem">libvirt</code> Framework</span> <a title="Permalink" class="permalink" href="index.html#vt.best.intro.libvirt">#</a></h3></div></div></div><p>
    In SUSE Linux Enterprise it is recommended to use the <code class="systemitem">libvirt</code>
    framework to use management operations on hosts, containers and
    VM Guest. Using management tools out-of-band prevents libvirt from
    recognizing the changes. For example, creating a system image manually
    with <code class="command">qemu-img create data.raw 10G</code> will not be
    displayed in the <code class="command">virt-manager</code> pool section. A
    VM Guest started directly with the <code class="command">qemu-system-arch</code>
    command will not be visible under <code class="systemitem">libvirt</code>. So you should be careful if
    using out-of-band management tools and keep in mind their usage probably
    will not be reflected in other virtualization tools.
   </p><div id="idm140164021264240" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Read
     <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html#" target="_blank"><code class="systemitem">libvirt</code>
     overview </a> for more information.
    </p></div></div><div class="sect2 " id="vt.best.intro.qemu"><div class="titlepage"><div><div><h3 class="title" id="vt.best.intro.qemu"><span class="number">3.2 </span><span class="name">qemu-system-i386 Vs. qemu-system-x86_64</span> <a title="Permalink" class="permalink" href="index.html#vt.best.intro.qemu">#</a></h3></div></div></div><p>
    Just as a modern 64-bit x86 PC supports running a 32-bit OS and a 64-bit
    OS, <code class="command">qemu-system-x86_64</code> runs 32-bit OS's perfectly
    fine, and in fact usually provides better performance to 32-bit guests
    than <code class="command">qemu-system-i386</code>, which provides a 32- bit guest
    environment only. Hence we recommend using
    <code class="command">qemu-system-x86_64</code> over
    <code class="command">qemu-system-i386</code> for all guest types. Where
    <code class="command">qemu-system-i386</code> is known to perform better is in
    configurations that SUSE does not support.
   </p></div></div><div class="sect1 " id="idm140164021258608"><div class="titlepage"><div><div><h2 class="title" id="idm140164021258608"><span class="number">4 </span><span class="name">I/O in Virtualization</span> <a title="Permalink" class="permalink" href="index.html#idm140164021258608">#</a></h2></div></div></div><p>
   SUSE products support various I/O Virtualization technologies. The
   following table lists advantages and drawbacks of each. For more
   information about I/O in virtualization refer to
   <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html" target="_blank">Virtualization
   Guide, I/O in Virtualization</a>.
  </p><div class="table" id="idm140164021256864"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 1: </span><span class="name">I/O Virtualization Solutions </span><a title="Permalink" class="permalink" href="index.html#idm140164021256864">#</a></h6></div><div class="table-contents"><table summary="I/O Virtualization Solutions" border="1"><colgroup><col width="20%" class="1" /><col width="" class="2" /><col /></colgroup><thead><tr><th>
       <p>
        Technology
       </p>
      </th><th>
       <p>
        Advantages
       </p>
      </th><th>
       <p>
        Drawbacks
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        Device Assignment
       </p>
       <p>
        (pass-through)
       </p>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Device accessed directly by the guest
         </p></li><li class="listitem "><p>
          High performance
         </p></li></ul></div>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          No sharing among multiple guests
         </p></li><li class="listitem "><p>
          Live migration is complex
         </p></li><li class="listitem "><p>
          PCI device limit is 8 per guest
         </p></li><li class="listitem "><p>
          Limited number of slots on a server
         </p></li></ul></div>
      </td></tr><tr><td>
       <p>
        Full virtualization
       </p>
       <p>
        (IDE, SATA, SCSI, e1000)
       </p>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          VM Guest compatibility
         </p></li><li class="listitem "><p>
          Easy for live migration
         </p></li></ul></div>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Bad performance
         </p></li><li class="listitem "><p>
          Emulated operation
         </p></li></ul></div>
      </td></tr><tr><td>
       <p>
        Para-virtualization
       </p>
       <p>
        (virtio-blk, virtio-net, virtio-scsi)
       </p>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Good performance
         </p></li><li class="listitem "><p>
          Easy for live migration
         </p></li><li class="listitem "><p>
          Efficient host communication with VM Guest
         </p></li></ul></div>
      </td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Modified guest (PV drivers)
         </p></li></ul></div>
      </td></tr></tbody></table></div></div></div><div class="sect1 " id="vt.best.hostlevel"><div class="titlepage"><div><div><h2 class="title" id="vt.best.hostlevel"><span class="number">5 </span><span class="name">Host-Level Configuration and Settings</span> <a title="Permalink" class="permalink" href="index.html#vt.best.hostlevel">#</a></h2></div></div></div><div class="sect2 " id="vt.best.mem"><div class="titlepage"><div><div><h3 class="title" id="vt.best.mem"><span class="number">5.1 </span><span class="name">Memory and Pages</span> <a title="Permalink" class="permalink" href="index.html#vt.best.mem">#</a></h3></div></div></div><p>
    Linux manages memory in units called pages. On most systems the default
    page size is 4 KB. Linux and the CPU need to know which pages belongs to
    which process. That information is stored in a page table. If you have a
    lot of processes running, it takes more time to find where the memory is
    mapped, due to the time required to search the page table. To speed up
    the search of the table, the TLB (Translation Lookaside Buffer) was
    invented. But on a system with a lot of memory the TLB is not enough. To
    avoid any fallback to normal page table (resulting in a cache miss,
    which is time consuming), huge pages can be used. Using huge pages will
    reduce TLB overhead and TLB misses (pagewalk). A host with 32 GB
    (32*1014*1024 = 33554432KB) of memory and a 4 KB page size has a TLB
    with: <span class="emphasis"><em>3355443/4 = 8388608</em></span> entries. Using a 2 MB
    (2048 KB) page size, the TLB only has <span class="emphasis"><em>3355443/ 2048 =
    16384</em></span> entries, greatly reducing TLB misses.
   </p><div class="sect3 " id="idm140164021203312"><div class="titlepage"><div><div><h4 class="title" id="idm140164021203312"><span class="number">5.1.1 </span><span class="name">Huge Pages</span> <a title="Permalink" class="permalink" href="index.html#idm140164021203312">#</a></h4></div></div></div><p>
     Current CPU architectures support larger pages than 4 KB: huge pages.
     To determine the size of huge pages available on your system (could be
     2 MB or 1 GB), check the flag in <code class="filename">/proc/cpuinfo</code>.
    </p><div class="table" id="idm140164021201696"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2: </span><span class="name">DETERMINE HUGE PAGES SIZE </span><a title="Permalink" class="permalink" href="index.html#idm140164021201696">#</a></h6></div><div class="table-contents"><table summary="DETERMINE HUGE PAGES SIZE" border="1"><colgroup><col width="10%" class="1" /><col width="" class="2" /></colgroup><thead><tr><th>
         <p>
          CPU flag
         </p>
        </th><th>
         <p>
          Huge pages size available
         </p>
        </th></tr></thead><tbody><tr><td>
         <p>
          Empty string
         </p>
        </td><td>
         <p>
          No huge pages available
         </p>
        </td></tr><tr><td>
         <p>
          pse
         </p>
        </td><td>
         <p>
          2 MB
         </p>
        </td></tr><tr><td>
         <p>
          pdpe1gb
         </p>
        </td><td>
         <p>
          1 GB
         </p>
        </td></tr></tbody></table></div></div><p>
     Using huge pages should improve performance of VM Guest and reduce
     host memory consumption.
    </p><p>
     By default the system uses THP, to make huge pages available on your
     system you need to activate it at boot time with
     <code class="option">hugepages=1</code>, and optionally add the huge pages size
     with <code class="option">hugepagesz=2MB</code>.
    </p><div id="idm140164021188032" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: 1 GB huge pages</h6><p>
      1 GB pages can only be allocated at boot time and cannot be freed
      afterwards.
     </p></div><p>
     To allocate and use huge page table (HugeTlbPage) you need to mount
     <code class="filename">hugetlbfs</code> with correct permissions.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Mount <code class="option">hugetlbfs</code> to
       <code class="filename">/dev/hugepages</code>:
      </p><div class="verbatim-wrap"><pre class="screen"># mount -t hugetlbfs hugetlbfs /dev/hugepages</pre></div></li><li class="step "><p>
       To reserve memory for huge pages use <code class="command">sysctl</code> or
       <code class="command">echo <em class="replaceable ">NUMBER</em></code>. If your
       system has a <span class="emphasis"><em>Huge pagesize</em></span> of 2 MB (2048 KB),
       and you want to reserve 1 GB (1048576KB) for your VM Guest, you need
       <span class="emphasis"><em>1048576/2048=512</em></span> pages in the pool.
      </p><div class="verbatim-wrap"><pre class="screen"># sysctl vm.nr_hugepages=512</pre></div><p>
       Or:
      </p><div class="verbatim-wrap"><pre class="screen"># echo 512 &gt; /proc/sys/vm/nr_hugepages<span id="co.hp.nrhp"></span><span class="callout">1</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.hp.nrhp"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
         Current number of <span class="emphasis"><em>persistent</em></span> huge pages in the
         kernel's huge page pool. <span class="emphasis"><em>Persistent</em></span> huge pages
         will be returned to the huge page pool when freed by a task
        </p></td></tr></table></div></li><li class="step "><p>
       Add the <span class="emphasis"><em>memoryBacking</em></span> element in the VM Guest
       config:
      </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</pre></div></li><li class="step "><p>
       Start your VM Guest and check on the host whether it uses hugepages:
      </p><div class="verbatim-wrap"><pre class="screen"># cat /proc/meminfo | grep HugePages_
HugePages_Total:<span id="co.hp.total"></span><span class="callout">1</span>    512
HugePages_Free:<span id="co.hp.free"></span><span class="callout">2</span>       92
HugePages_Rsvd:<span id="co.hp.rsvd"></span><span class="callout">3</span>        0
HugePages_Surp:<span id="co.hp.surp"></span><span class="callout">4</span>        0</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.hp.total"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
         Size of the pool of huge pages
        </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.hp.free"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
         Number of huge pages in the pool that are not yet allocated
        </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.hp.rsvd"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
         Number of huge pages for which a commitment to allocate from the
         pool has been made, but no allocation has yet been made
        </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.hp.surp"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
         Number of huge pages in the pool above the value in
         <code class="filename">/proc/sys/vm/nr_hugepages</code>. The maximum number
         of surplus huge pages is controlled by
         <code class="filename">/proc/sys/vm/nr_overcommit_hugepages</code>
        </p></td></tr></table></div></li></ol></div></div><div id="idm140164021165312" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Even if huge pages provide the best performance, they do come with
      some drawbacks. You lose features such as Memory ballooning (see
      <a class="xref" href="index.html#vt.best.virtio.balloon" title="8.1.3. virtio balloon">Section 8.1.3, “virtio balloon”</a>), KSM (see
      <a class="xref" href="index.html#vt.best.perf.ksm" title="5.2. KSM and Page Sharing">Section 5.2, “KSM and Page Sharing”</a>, and huge pages cannot be swapped.
     </p></div></div><div class="sect3 " id="vt.best.mem.thp"><div class="titlepage"><div><div><h4 class="title" id="vt.best.mem.thp"><span class="number">5.1.2 </span><span class="name">Transparent Huge Pages</span> <a title="Permalink" class="permalink" href="index.html#vt.best.mem.thp">#</a></h4></div></div></div><p>
     Transparent huge pages (THP) provide a way to dynamically allocate huge
     pages with the <code class="command">khugepaged</code> kernel thread, instead of
     manually managing their allocation and use. Workloads with contiguous
     memory access patterns can benefit greatly from THP. A 1000 fold
     decrease in page faults can be observed when running synthetic
     workloads with contiguous memory access patterns. Conversely, workloads
     with sparse memory access patterns (like databases) may perform poorly
     with THP. In such cases it may be preferable to disable THP by adding
     the kernel parameter <code class="option">transparent_hugepage=never</code>,
     rebuild your grub2 configuration, and reboot. Verify THP is disabled
     with:
    </p><div class="verbatim-wrap"><pre class="screen"># cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</pre></div><div id="idm140164021159744" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      THP is not available under Xen
     </p></div></div><div class="sect3 " id="vt.best.mem.hot"><div class="titlepage"><div><div><h4 class="title" id="vt.best.mem.hot"><span class="number">5.1.3 </span><span class="name">Memory Hotplug</span> <a title="Permalink" class="permalink" href="index.html#vt.best.mem.hot">#</a></h4></div></div></div><p>
     To optimize the usage of your host memory, it may be useful to hot plug
     more memory for a running VM Guest when required. To support memory
     hot-plugging, you must first configure the
     <span class="emphasis"><em>&lt;maxMemory&gt;</em></span> tag in the XML configuration:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;maxMemory<span id="co.mem.hot.max"></span><span class="callout">1</span> slots='16'<span id="co.mem.hot.slots"></span><span class="callout">2</span> unit='KiB'&gt;20971520<span id="co.mem.hot.size"></span><span class="callout">3</span>&lt;/maxMemory&gt;
  &lt;memory<span id="co.mem.hot.mem"></span><span class="callout">4</span> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<span id="co.mem.hot.curr"></span><span class="callout">5</span> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hot.max"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       Run time maximum memory allocation of the guest.
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hot.slots"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       Number of slots available for adding memory to the guest
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hot.size"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       Valid units are:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         "KB" for kilobytes (1,000 bytes)
        </p></li><li class="listitem "><p>
         "k" or "KiB" for kibibytes (1,024 bytes)
        </p></li><li class="listitem "><p>
         "MB" for megabytes (1,000,000 bytes)
        </p></li><li class="listitem "><p>
         "M" or "MiB" for mebibytes (1,048,576 bytes)
        </p></li><li class="listitem "><p>
         "GB" for gigabytes (1,000,000,000 bytes)
        </p></li><li class="listitem "><p>
         "G" or "GiB" for gibibytes (1,073,741,824 bytes)
        </p></li><li class="listitem "><p>
         "TB" for terabytes (1,000,000,000,000 bytes)
        </p></li><li class="listitem "><p>
         "T" or "TiB" for tebibytes (1,099,511,627,776 bytes)
        </p></li></ul></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hot.mem"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       Maximum allocation of memory for the guest at boot time
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hot.curr"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
       Actual allocation of memory for the guest
      </p></td></tr></table></div><p>
     To hot plug a memory devices into the slots, use <code class="command">virsh
     attach-device vm-name mem-dev.xml</code>.
    </p><div class="verbatim-wrap"><pre class="screen"># cat mem-dev.xml
&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'&gt;524287&lt;/size&gt;
  &lt;node&gt;0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</pre></div><p>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <a class="xref" href="index.html#vt.best.numa.topo" title="5.8.3. Guest NUMA Topology">Section 5.8.3, “Guest NUMA Topology”</a>).
    </p></div><div class="sect3 " id="idm140164021133808"><div class="titlepage"><div><div><h4 class="title" id="idm140164021133808"><span class="number">5.1.4 </span><span class="name">Xen-specific Memory Notes</span> <a title="Permalink" class="permalink" href="index.html#idm140164021133808">#</a></h4></div></div></div><div class="sect4 " id="idm140164021133120"><div class="titlepage"><div><div><h5 class="title" id="idm140164021133120"><span class="number">5.1.4.1 </span><span class="name">Managing Domain-0 Memory</span> <a title="Permalink" class="permalink" href="index.html#idm140164021133120">#</a></h5></div></div></div><p>
      When using the Xen hypervisor, by default a small percentage of
      system memory is reserved for the hypervisor, with all remaining
      memory automatically allocated to Domain-0. When virtual machines are
      created, memory is ballooned out of Domain-0 to provide memory for the
      virtual machine. This process is called "autoballooning".
     </p><p>
      Autoballooning has several drawbacks:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        Reduced performance while dom0 is ballooning down to free memory for
        the new domain.
       </p></li><li class="listitem "><p>
        Memory freed by ballooning is not confined to a specific NUMA node.
        This can result in performance problems in the new domain due to
        using a non-optimal NUMA configuration.
       </p></li><li class="listitem "><p>
        Failure to start large domains due to delays while ballooning large
        amounts of memory from dom0.
       </p></li></ul></div><p>
      For these reasons, it is strongly recommended to disable
      autoballooning and give Domain-0 a predefined amount of memory.
     </p><p>
      Autoballooning is controlled by the toolstack used to manage your
      Xen installation. For the xl/libxl toolstack, autoballooning is
      controlled by the <code class="option">autoballoon</code> setting in
      <code class="filename">/etc/xen/xl.conf</code>. For the libvirt+libxl
      toolstack, autoballooning is controlled by the
      <code class="option">autoballoon</code> setting in
      <code class="filename">/etc/libvirt/libxl.conf</code>.
     </p><p>
      The amount of memory initially allocated to Domain-0 is controlled by
      the Xen hypervisor dom0_mem parameter. For example, to set the
      memory of Domain-0 to 8GB, add <code class="option">dom0_mem=8G</code> to the
      Xen hypervisor parameters.
     </p><p>
      To set dom0_mem on SLE11 products, modify
      <code class="filename">/boot/grub/menu.lst</code>, adding
      <code class="option">dom0_mem=XX</code> to the Xen hypervisor (xen.gz)
      parameters. The change will be applied at next reboot.
     </p><p>
      To set dom0_mem on SLE12 products, modify
      <code class="filename">/etc/default/grub</code>, adding
      <code class="option">dom0_mem=XX</code> to
      <code class="option">GRUB_CMDLINE_XEN_DEFAULT</code>. See
      <a class="xref" href="index.html#vt.best.kernel.parameter" title="9.5. Change Kernel Parameters at boot time">Section 9.5, “Change Kernel Parameters at boot time”</a> for more information.
     </p><p>
      Autoballooning is enabled by default since it is extremely difficult
      to determine a predefined amount of memory required by Domain-0.
      Memory needed by Domain-0 is heavily dependent on the number of hosted
      virtual machines and their configuration. Users must ensure Domain-0
      has sufficient memory resources to accommodate virtual machine
      workloads.
     </p></div><div class="sect4 " id="idm140164021119424"><div class="titlepage"><div><div><h5 class="title" id="idm140164021119424"><span class="number">5.1.4.2 </span><span class="name">xenstore in tmpfs</span> <a title="Permalink" class="permalink" href="index.html#idm140164021119424">#</a></h5></div></div></div><p>
      When using Xen, it is recommended to place the xenstore database on
      tmpfs. xenstore is used as a control plane by the xm/xend and xl/libxl
      toolstacks and the frontend and backend drivers servicing domain I/O
      devices. The load on xenstore increases linearly as the number of
      running domains increase. If you anticipate hosting many VM Guest on
      a Xen host, move the xenstore database onto tmpfs to improve overall
      performance of the control plane. Mount
      <code class="filename">/var/lib/xenstored</code> directory on tmpfs:
     </p><div class="verbatim-wrap"><pre class="screen"># mount -t tmpfs tmpfs /var/lib/xenstored/</pre></div></div></div></div><div class="sect2 " id="vt.best.perf.ksm"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.ksm"><span class="number">5.2 </span><span class="name">KSM and Page Sharing</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.ksm">#</a></h3></div></div></div><p>
    Kernel Samepage Merging is a kernel feature that allows packing more
    virtual machines into physical memory, by sharing the data common
    between them. The KSM daemon ksmd periodically scans user memory looking
    for pages of identical content which can be replaced by a single
    write-protected page. The duplicate pages are then available to run more
    VM Guest on the host. You can enable KSM with <code class="command">echo 1 &gt;
    /sys/kernel/mm/ksm/run</code>. One advantage of using KSM from a
    VM Guest perspective is that all guest memory is backed by host
    anonymous memory, so you can share <span class="emphasis"><em>pagecache</em></span>,
    <span class="emphasis"><em>tmpfs</em></span> or any kind of memory allocated in the guest.
   </p><p>
    KSM is controlled by <span class="emphasis"><em>sysfs</em></span>. You can check KSM's
    values in <code class="filename">/sys/kernel/mm/ksm/</code>:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="emphasis"><em>pages_shared</em></span>: How many shared pages are being
      used (read only).
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>pages_sharing</em></span>: How many sites are sharing the
      pages (read only).
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>pages_unshared</em></span>: How many pages are unique but
      repeatedly checked for merging (read only).
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>pages_volatile</em></span>: How many pages are changing too
      fast to be considered for merging (read only).
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>full_scans</em></span>: How many times all mergeable areas
      have been scanned (read only).
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>sleep_millisecs</em></span>: How many milliseconds
      <code class="command">ksmd</code> should sleep before the next scan. A low value
      will overuse the CPU, consuming CPU time that could be used for other
      tasks. A value greater than <code class="option">1000</code> is recommended.
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>pages_to_scan</em></span>: How many present pages to scan
      before ksmd goes to sleep. A high value will overuse the CPU. It is
      recommended to start with a value of <code class="option">1000</code>, and then
      adjust as necessary based on the KSM results observed while testing
      your deployment.
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>merge_across_nodes</em></span>: by default the system merges
      pages across NUMA nodes. Set this option to <code class="option">0</code> to
      disable this behavior.
     </p></li></ul></div><div id="idm140164021099872" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     KSM is a good technique to over-commit host memory when running
     multiple instances of the same application or VM Guest. When
     applications and VM Guest are heterogeneous and do not share any
     common data, it is preferable to disable KSM. In a mixed heterogeneous
     and homogeneous environment, KSM can be enabled on the host but
     disabled on a per VM Guest basis. Use <code class="command"> virsh edit</code>
     to disable page sharing of a VM Guest by adding the following to the
     guest's XML configuration:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</pre></div></div><div id="idm140164021097696" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
     KSM can free up some memory on the host system, but the administrator
     should also reserve enough swap to avoid out-of-memory conditions in
     the event sharable memory decreases. A decrease in the amount of
     sharable memory results in an increase in the use of physical memory.
    </p></div><div id="idm140164021096608" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
     By default, KSM will merge common pages across NUMA nodes. This may
     degrade VM Guest performance if the merged, common page is now located
     on a distant NUMA node, relative to the node running the VM Guest
     VCPUs. If increased memory access latencies are noticed in the
     VM Guest, disable cross-node merging with the <span class="emphasis"><em>
     merge_across_nodes</em></span> sysfs control: <code class="command">echo 0 &gt;
     /sys/kernel/mm/ksm/merge_across_nodes</code>.
    </p></div></div><div class="sect2 " id="vt.best.perf.swap"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.swap"><span class="number">5.3 </span><span class="name">Swapping</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.swap">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Swap</em></span> is mostly used by the system to store
    underused physical memory (low usage, or not accessed for a long time).
    To prevent the system running out of memory, setting up a minimum swap
    is highly recommended.
   </p><div class="sect3 " id="idm140164021092448"><div class="titlepage"><div><div><h4 class="title" id="idm140164021092448"><span class="number">5.3.1 </span><span class="name">swappiness</span> <a title="Permalink" class="permalink" href="index.html#idm140164021092448">#</a></h4></div></div></div><p>
     The <span class="emphasis"><em>swappiness</em></span> setting controls your systems swap
     behavior. It defines how memory pages are swapped to disk. A high value
     of <span class="emphasis"><em>swappiness</em></span> results in a system that swaps more.
     Values available are from <code class="option">0</code> to <code class="option">100</code>. A
     value of <code class="option">100</code> tells the system to find inactive pages
     and put them in swap. A value of <code class="option">0</code> reduces the systems
     tendency to swap userspace processes but does not disable swap
     completely (this is now the case with kernel =&gt; 3.5).
    </p><p>
     To change the value and do some testing on a live system, you need to
     make an echo of the value, and check your system memory usage (i.e.,
     with the <code class="command">free</code> command):
    </p><div class="verbatim-wrap"><pre class="screen"># echo 35 &gt; /proc/sys/vm/swappiness</pre></div><div class="verbatim-wrap"><pre class="screen"># free
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</pre></div><p>
     To get this permanently add a line in
     <code class="filename">/etc/systcl.conf</code>:
    </p><div class="verbatim-wrap"><pre class="screen">vm.swappiness = 35</pre></div><p>
     You can also control the swap by using the
     <span class="emphasis"><em>swap_hard_limit</em></span> element in the XML configuration
     of your VM Guest. It is highly recommended to do some testing before
     setting this parameter and using it in a production environment as the
     host can kill the domain if the value is too low.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memtune&gt;<span id="co.mem.1"></span><span class="callout">1</span>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<span id="co.mem.hard"></span><span class="callout">2</span>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<span id="co.mem.soft"></span><span class="callout">3</span>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<span id="co.mem.swap"></span><span class="callout">4</span>
&lt;/memtune&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       This element provides memory tunable parameters for the domain. If
       this is omitted, it defaults to the OS provided defaults
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.hard"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       Maximum memory the guest can use. To avoid any problems on the
       VM Guest it is strongly recommended to do not use this parameter
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.soft"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       The memory limit to enforce during memory contention
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.mem.swap"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       The maximum memory plus swap the VM Guest can use
      </p></td></tr></table></div></div></div><div class="sect2 " id="vt.best.perf.io"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.io"><span class="number">5.4 </span><span class="name">I/O Scheduler</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.io">#</a></h3></div></div></div><p>
    The default I/O scheduler is Completely Fair Queuing (CFQ). The main aim
    of the CFQ scheduler is to provide a fair allocation of the disk I/O
    bandwidth for all processes that request an I/O operation. You can have
    different I/O schedulers for different devices.
   </p><p>
    To get better performance in host and VM Guest it is recommended to use
    <span class="emphasis"><em>noop</em></span> in the VM Guest (disable the I/O scheduler)
    and the <span class="emphasis"><em>deadline</em></span> scheduler for a virtualization
    host.
   </p><div class="sect3 " id="idm140164021072992"><div class="titlepage"><div><div><h4 class="title" id="idm140164021072992"><span class="number">5.4.1 </span><span class="name">Checking the Current I/O Scheduler</span> <a title="Permalink" class="permalink" href="index.html#idm140164021072992">#</a></h4></div></div></div><p>
     To check your current I/O scheduler for your disk (replace
     <em class="replaceable ">sdX</em> by the disk you want to check):
    </p><div class="verbatim-wrap"><pre class="screen"># cat /sys/block/<em class="replaceable ">sdX</em>/queue/scheduler
noop [deadline] cfq</pre></div><p>
     In our example the <span class="emphasis"><em>deadline</em></span> scheduler is selected.
    </p></div><div class="sect3 " id="idm140164021069552"><div class="titlepage"><div><div><h4 class="title" id="idm140164021069552"><span class="number">5.4.2 </span><span class="name">Changing I/O Scheduler at Runtime</span> <a title="Permalink" class="permalink" href="index.html#idm140164021069552">#</a></h4></div></div></div><p>
     You can change it at runtime with <code class="command">echo</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># echo noop &gt; /sys/block/<em class="replaceable ">sdX</em>/queue/scheduler</pre></div></div><div class="sect3 " id="idm140164021067088"><div class="titlepage"><div><div><h4 class="title" id="idm140164021067088"><span class="number">5.4.3 </span><span class="name">Kernel Parameters</span> <a title="Permalink" class="permalink" href="index.html#idm140164021067088">#</a></h4></div></div></div><p>
     The kernel parameters are <code class="option">elevator=deadline</code> for host
     and <code class="option">elevator=noop</code> for VM Guest (change will be taken
     into account at next reboot). This will be applied to all devices on
     your system.
    </p><p>
     Changing the <code class="option">elevator=</code> for the boot command line will
     apply the <code class="option">elevator</code> to all devices on your system.
    </p><p>
     If you want to have a different parameter for each disk, create a
     <code class="filename">/usr/lib/tmpfiles.d/IO_ioscheduler.conf</code> file with:
    </p><p>
     See <a class="xref" href="index.html#vt.best.kernel.parameter" title="9.5. Change Kernel Parameters at boot time">Section 9.5, “Change Kernel Parameters at boot time”</a> to get more information
     about how to change this kernel parameter on SLE11 or SLE12.
    </p><div id="idm140164021061888" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: SLE12 and systemd</h6><p>
      If you want to have a different parameter for each disk create a
      <code class="filename">/usr/lib/tmpfiles.d/IO_ioscheduler.conf</code> file
      with:
     </p><div class="verbatim-wrap"><pre class="screen">w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</pre></div></div></div></div><div class="sect2 " id="vt.best.perf.cpu"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.cpu"><span class="number">5.5 </span><span class="name">Understanding CPU</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.cpu">#</a></h3></div></div></div><p>
    Allocation of resources for VM Guest is a crucial point in VM
    administration. Each VM Guest should be <span class="emphasis"><em>sized</em></span> to
    be able to run a certain amount of services, but over-allocating
    resources for VM Guest may impact the host and all other VM Guests. If
    all VM Guests suddenly requested all their resources, the host would
    not be able to provide all of them, and this would impact the host's
    performance and degrade all other services running on the host.
   </p><p>
    CPU's Host <span class="emphasis"><em>components</em></span> will be
    <span class="emphasis"><em>translated</em></span> as vCPU in the VM Guest, but even if
    you have a multi-core CPU with Hyper-Threading, you should understand
    that a main CPU unit and a multi-core and Hyper-Threading do not provide
    the same computation capabilities:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="emphasis"><em>CPU processor</em></span>: this describes the main CPU unit,
      it can be multi-core and Hyper-Threaded, and most dedicated servers
      have multiple CPU processors on their motherboard.
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>CPU core</em></span>: a main CPU unit can provide more than
      one core, and the proximity of cores speeds up the computation process
      and reduces energy costs.
     </p></li><li class="listitem "><p>
      <span class="emphasis"><em>CPU Hyper-Threading</em></span>: this implementation is used
      to improve parallelization of computations, but this is not efficient
      as a dedicated core.
     </p></li></ul></div></div><div class="sect2 " id="vt.best.perf.cpuparam"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.cpuparam"><span class="number">5.6 </span><span class="name">CPU Parameter</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.cpuparam">#</a></h3></div></div></div><p>
    You should avoid CPU over-commit. Unless you know exactly how many vCPU
    you require for your VM Guest you should start with 1 vCPU per
    VM Guest. Each vCPU should match one hardware processor or core. You
    should target a workload of 70% inside your VM (this could be checked
    with a lot of tools like the <code class="command">top</code>). If you allocate
    more processor than needed in the VM, this will add overhead, and will
    degrade cycle efficiency, the unused vCPU will consume timer interrupts
    and will idle loop, then this will impact the VM Guest, but also the
    host. To optimize the performance usage it is recommended to know
    whether your applications are single threaded or not, so as to avoid any
    over-allocation of vCPU.
   </p><div class="sect3 " id="idm140164021336768"><div class="titlepage"><div><div><h4 class="title" id="idm140164021336768"><span class="number">5.6.1 </span><span class="name">vCPU Model and Features</span> <a title="Permalink" class="permalink" href="index.html#idm140164021336768">#</a></h4></div></div></div><p>
     CPU model and topology can be specified for each VM Guest. The vCPU
     definition could be very specific excluding some CPU features, listing
     exact ones, etc. You can use predefined models available in the
     <code class="filename">/usr/share/libvirt/cpu_map.xml</code> file to exactly
     match your needs. Even if it seems beneficial to state a very specific
     vCPU for a VM Guest, you should keep in mind that the normal vCPU
     model and features simplify migration among heterogeneous hosts (see
     <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html" target="_blank">libvirt
     migration guide</a>). This is because changing the vCPU type
     requires the VM Guest to be off, which is a constraint in a production
     environment. You should also keep in mind that multiple sockets with a
     single core and thread generally give the best performance.
    </p><p>
     To get all capabilities and topology available on your system, use the
     <code class="command">virsh capabilities</code> command.
    </p><div id="idm140164021332896" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: host-passthrough</h6><p>
      The CPU visible to the guest should be exactly the same as the host
      CPU even in the aspects that <code class="systemitem">libvirt</code> does not understand. However,
      the downside of this mode is that the guest environment cannot be
      reproduced on different hardware. Thus, if you hit any bugs, you are
      on your own.
     </p></div><p>
     To specify a particular CPU model for a VM Guest, such as a Broadwell
     CPU, use:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
&lt;/cpu&gt;</pre></div><p>
     If you want to add the <span class="emphasis"><em>invtsc</em></span> (Invariant TSC) CPU
     feature, add a <span class="emphasis"><em>feature</em></span> element between the
     <span class="emphasis"><em>&lt;cpu&gt;</em></span> element:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu&gt;
  ....
  &lt;feature name='invtsc'/&gt;
&lt;/cpu&gt;</pre></div></div><div class="sect3 " id="idm140164021030704"><div class="titlepage"><div><div><h4 class="title" id="idm140164021030704"><span class="number">5.6.2 </span><span class="name">vCPU Pinning</span> <a title="Permalink" class="permalink" href="index.html#idm140164021030704">#</a></h4></div></div></div><p>
     This is a method to constrain vCPU threads to a NUMA node. The
     <span class="emphasis"><em>vcpupin</em></span> element specifies which of the host's
     physical CPUs the domain vCPU will be pinned to. If this is omitted,
     and the attribute <span class="emphasis"><em>cpuset</em></span> of the element
     <span class="emphasis"><em>vcpu</em></span> is not specified, the vCPU is pinned to all
     the physical CPUs by default.
    </p><p>
     You can pin a vCPU to a specific physical CPU, in which case the vCPU
     will increase the CPU cache hit ratio. To pin a vCPU to a specific
     core:
    </p><div class="verbatim-wrap"><pre class="screen">virsh# vcpupin <em class="replaceable ">DOMAIN_ID</em> --vcpu <em class="replaceable ">vCPU_NUMBER</em>
VCPU: CPU Affinity
----------------------------------
0: 0-7
virsh # vcpupin SLE12 --vcpu 0 0 --config</pre></div><p>
     In the XML configuration of your domain you should now have:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</pre></div><div id="idm140164021025392" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      vCPU pinning can also be used for a non-numa node
     </p></div><div class="figure" id="fig.qemu-img.vmnuma"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/vm_numa.png"><img src="images/vm_numa.png" width="" alt="NUMA vCPU placement" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 1: </span><span class="name">NUMA vCPU placement </span><a title="Permalink" class="permalink" href="index.html#fig.qemu-img.vmnuma">#</a></h6></div></div><div id="idm140164021019840" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Live Migration</h6><p>
      Even if <span class="emphasis"><em>vcpupin</em></span> can improve performance, you
      should take into consideration that live migration of a pinned
      VM Guest is difficult, because the resource may not be available on
      the host, or the NUMA topology could be different on the destination
      host. For more recommendations about Live Migration see
      <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html#libvirt_admin_live_migration_requirements" target="_blank">Virtualization
      Live Migration Requirements</a>.
     </p></div></div><div class="sect3 " id="idm140164021017008"><div class="titlepage"><div><div><h4 class="title" id="idm140164021017008"><span class="number">5.6.3 </span><span class="name">libvirt and CPU Configuration</span> <a title="Permalink" class="permalink" href="index.html#idm140164021017008">#</a></h4></div></div></div><p>
     For more information about vCPU configuration and tuning parameters
     refer to the
     <a class="link" href="https://libvirt.org/formatdomain.html#elementsCPU" target="_blank">libvirt</a>
     documentation.
    </p></div></div><div class="sect2 " id="idm140164021014832"><div class="titlepage"><div><div><h3 class="title" id="idm140164021014832"><span class="number">5.7 </span><span class="name">Xen: Moving from PV to FV</span> <a title="Permalink" class="permalink" href="index.html#idm140164021014832">#</a></h3></div></div></div><p>
    This chapter explains how to convert a Xen para-virtual machine into a
    Xen fully virtualized machine.
   </p><p>
    First you need to change to the <span class="emphasis"><em>-default</em></span> kernel, if
    not already installed, you must install it while in PV mode.
   </p><p>
    In case you are using <span class="emphasis"><em>vda*</em></span> disk naming, you need to
    change this to <span class="emphasis"><em>hd*</em></span> in
    <code class="filename">/etc/fstab</code>,
    <code class="filename">/boot/grub/menu.lst</code> and
    <code class="filename">/boot/grub/device.map</code>.
   </p><div id="idm140164021010144" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Prefer UUIDs</h6><p>
     You should use UUIDs or logical volumes within your
     <code class="filename">/etc/fstab</code>. Using UUID simplifies attached network
     storage, multipathing, and virtualization.
    </p></div><p>
    Moving from PV to FV will lead to disk driver modules being missing from
    the initrd. The modules expected are <span class="emphasis"><em>xen-vbd</em></span> (and
    <span class="emphasis"><em>xen-vnif</em></span> for network). These are the PV drivers for
    a fully virtualized VM Guest. All other modules like
    <span class="emphasis"><em>ata_piix</em></span>, <span class="emphasis"><em>ata_generic</em></span> and
    <span class="emphasis"><em>libata</em></span> should be added automatically.
   </p><div id="idm140164021005520" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Adding Modules to the initrd</h6><p>
     On SLES 11, you can add modules to the
     <span class="emphasis"><em>INITRD_MODULES</em></span> line in the
     <code class="filename">/etc/sysconfig/kernel</code> file, for example:
    </p><div class="verbatim-wrap"><pre class="screen">INITRD_MODULES="xen-vbd xen-vnif"</pre></div><p>
     Run <code class="command">mkinitrd</code> to build a new initrd containing the
     modules.
    </p><p>
     On SLES 12, open <code class="filename">/etc/dracut.conf.d/01-dist.conf</code>
     and add the modules with <code class="literal">force-drivers</code> by adding a
     line as in the example below:
    </p><div class="verbatim-wrap"><pre class="screen">force-drivers+="xen-vbd xen-vnif"</pre></div><p>
     Run <code class="command">dracut -f</code> to build a new initrd containing the
     modules.
    </p></div><p>
    You need to change a few parameters in the XML configuration file to
    describe your VM Guest:
   </p><p>
    Set the OS section to something like:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;os&gt;
  &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
  &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;boot dev='hd'/&gt;
&lt;/os&gt;</pre></div><p>
    In the devices section, you need to add:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</pre></div><p>
    Replace the <span class="emphasis"><em>xen</em></span> disk bus with
    <span class="emphasis"><em>ide</em></span>, and the <span class="emphasis"><em>xvda</em></span> target
    device with <span class="emphasis"><em>hda</em></span>.
   </p><div id="idm140164020994576" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: guestfs-tools</h6><p>
     If you want to script this process, or work on disk images directly,
     you can use the
     <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html" target="_blank">guestfs-tools</a>
     suite, as numerous tools exist there to help to modify disk images.
    </p></div></div><div class="sect2 " id="vt.best.perf.numa"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.numa"><span class="number">5.8 </span><span class="name">NUMA Affinity</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.numa">#</a></h3></div></div></div><p>
    Using <span class="emphasis"><em>NUMA</em></span> can potentially have a huge impact on
    performance. You should consider your host topology when sizing guests.
    First check that your host has NUMA capabilities:
   </p><div class="verbatim-wrap"><pre class="screen"># numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</pre></div><div class="sect3 " id="idm140164020988928"><div class="titlepage"><div><div><h4 class="title" id="idm140164020988928"><span class="number">5.8.1 </span><span class="name">Numa Node Tuning (host)</span> <a title="Permalink" class="permalink" href="index.html#idm140164020988928">#</a></h4></div></div></div><p>
     NUMA is the acronym of Non Uniform Memory Access. A NUMA system has
     multiple physical CPUs with local memory attached. Moreover each CPU
     can access other CPUs' Memory this is what we call remote memory, and
     access is slower than accessing local memory.
    </p><p>
     You can control the NUMA policy performance
     
     for domain processes by using the <span class="emphasis"><em>numatune</em></span>
     element:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
    &lt;memory mode="strict"<span id="co.numat.mode"></span><span class="callout">1</span> nodeset="1-4,^3"<span id="co.numat.nodeset"></span><span class="callout">2</span>/&gt;
    &lt;memnode<span id="co.numat.memnode"></span><span class="callout">3</span> cellid="0"<span id="co.numat.cellid"></span><span class="callout">4</span> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<span id="co.numat.placement"></span><span class="callout">5</span> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.numat.mode"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       Policies available are: <span class="emphasis"><em>interleave</em></span> (round-robin
       like), <span class="emphasis"><em>strict</em></span> (default) or
       <span class="emphasis"><em>preferred</em></span>
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numat.nodeset"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       Specify the NUMA nodes
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numat.memnode"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       Specify memory allocation policies for each guest NUMA node (if this
       element is not defined then this will fall back and use the
       <span class="emphasis"><em>memory</em></span> element)
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numat.cellid"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       Addresses the guest NUMA node for which the settings are applied
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numat.placement"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
       The placement attribute can be used to indicate the memory placement
       mode
       
       for domain processes, the value can be <span class="emphasis"><em>auto</em></span> or
       <span class="emphasis"><em>strict</em></span>
      </p></td></tr></table></div><div id="idm140164020972880" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Memory and CPU across NUMA nodes</h6><p>
      You should avoid allocating VM Guest memory across NUMA nodes, and
      prevent vCPUs from floating across NUMA nodes.
     </p></div></div><div class="sect3 " id="idm140164020971408"><div class="titlepage"><div><div><h4 class="title" id="idm140164020971408"><span class="number">5.8.2 </span><span class="name">NUMA Balancing</span> <a title="Permalink" class="permalink" href="index.html#idm140164020971408">#</a></h4></div></div></div><p>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU. Automatic NUMA balancing scans a task's address
     space and unmaps pages to detect whether pages are properly placed or
     whether the data should be migrated to a memory node local to where the
     task is running. Every scan delay
     (<span class="emphasis"><em>numa_balancing_scan_delay_ms</em></span>), the task scans the
     next scan size (<span class="emphasis"><em>numa_balancing_scan_size_mb</em></span>)
     number of pages in its address space. When the end of the address space
     is reached the scanner restarts from the beginning.
    </p><p>
     Higher scan rates incur higher system overhead as page faults must be
     trapped and potentially data may need to be migrated. However, the
     higher the scan rate, the more quickly a task's memory is migrated to a
     local node if the workload pattern changes, thereby minimizing
     performance impact due to remote memory accesses. These
     <code class="command">sysctls</code> control the thresholds for scan delays and
     the number of pages scanned.
    </p><div class="verbatim-wrap"><pre class="screen"># sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<span id="co.numa.balancing"></span><span class="callout">1</span>
kernel.numa_balancing_scan_delay_ms = 1000<span id="co.numa.delay"></span><span class="callout">2</span>
kernel.numa_balancing_scan_period_max_ms = 60000<span id="co.numa.pmax"></span><span class="callout">3</span>
kernel.numa_balancing_scan_period_min_ms = 1000<span id="co.numa.pmin"></span><span class="callout">4</span>
kernel.numa_balancing_scan_size_mb = 256<span id="co.numa.size"></span><span class="callout">5</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.balancing"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       Enables/disables automatic page fault-based NUMA balancing
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.delay"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       Starting scan delay used for a task when it initially forks
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.pmax"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       Maximum time in milliseconds to scan a task's virtual memory
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.pmin"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       Minimum time in milliseconds to scan a task's virtual memory
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.size"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
       How many megabytes worth of pages are scanned for a given scan
      </p></td></tr></table></div><p>
     The main goal of automatic NUMA balancing is to reschedule tasks on the
     same node's memory, so the CPU follows the memory, or to copy the
     memory's pages to the same node, so the memory follows the CPU.
    </p><div id="idm140164020957168" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Task Placement</h6><p>
      There are no rules to define the best place to run a task, because
      tasks could share memory with another task, so you should group such
      tasks on the same node to obtain the best performance. Check NUMA
      statistics with <code class="command"># cat /proc/vmstat | grep numa_</code>.
     </p></div></div><div class="sect3 " id="vt.best.numa.topo"><div class="titlepage"><div><div><h4 class="title" id="vt.best.numa.topo"><span class="number">5.8.3 </span><span class="name">Guest NUMA Topology</span> <a title="Permalink" class="permalink" href="index.html#vt.best.numa.topo">#</a></h4></div></div></div><p>
     VM Guest NUMA topology can be specified using the
     <span class="emphasis"><em>numa</em></span> element in the XML configuration:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<span id="co.numa.cell"></span><span class="callout">1</span> id='0'<span id="co.numa.id"></span><span class="callout">2</span> cpus='0-1'<span id="co.numa.cpus"></span><span class="callout">3</span> memory='512000' unit='KiB'/&gt;
    &lt;cell id='1' cpus='2-3' memory='256000'<span id="co.numa.mem"></span><span class="callout">4</span>
    unit='KiB'<span id="co.numa.unit"></span><span class="callout">5</span> memAccess='shared'<span id="co.numa.memaccess"></span><span class="callout">6</span>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.cell"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       Each <span class="emphasis"><em>cell</em></span> element specifies a NUMA cell or a
       NUMA node
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.id"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       All cells should have an <span class="emphasis"><em>id</em></span> attribute in case it
       is necessary to refer to any cell in the code. Otherwise the cells
       are assigned ids in ascending order starting from 0
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.cpus"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       cpus specifies the CPU or range of CPUs that are part of the node
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.mem"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       The node memory in kilobytes (i.e. blocks of 1024 bytes)
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.unit"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
       Attribute to define units in which memory is specified
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.numa.memaccess"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>
       Optional attribute which can control whether the memory is to be
       mapped as <code class="option">shared</code> or <code class="option">private</code>. This
       is valid only for hugepages-backed memory.
      </p></td></tr></table></div><p>
     To find where the VM Guest has allocated its pages. use: <code class="command">cat
     /proc/<em class="replaceable ">PID</em>/numa_maps</code> and
     <code class="command">cat
     /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<em class="replaceable ">KVM_NAME</em>/memory.numa_stat</code>.
    </p><div id="idm140164020937216" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: NUMA specification</h6><p>
      The <code class="systemitem">libvirt</code> guest NUMA specification is currently only available for
      QEMU/KVM.
     </p></div></div><div class="sect3 " id="idm140164020935088"><div class="titlepage"><div><div><h4 class="title" id="idm140164020935088"><span class="number">5.8.4 </span><span class="name">Cpuset Memory Policy</span> <a title="Permalink" class="permalink" href="index.html#idm140164020935088">#</a></h4></div></div></div><p>
     There are three memory cpuset policy modes available: interleave, bind
     and preferred.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="emphasis"><em>interleave</em></span>: This is a memory placement policy
       which is also known as round-robin. This policy can provide
       substantial improvements for jobs that need to place thread local
       data on the corresponding node. When the interleave destination is
       not available, it will be moved to another nodes.
      </p></li><li class="listitem "><p>
       <span class="emphasis"><em>bind</em></span>: This will place memory only on one node,
       which means in case of insufficient memory, the allocation will fail.
      </p></li><li class="listitem "><p>
       <span class="emphasis"><em>preferred</em></span>: This policy will apply a preference
       to allocate memory to a node, but if there is not enough space for
       memory on this node, it will fall back to another node.
      </p></li></ul></div><p>
     You can change the memory policy mode with the <code class="command">cgset</code>
     tool:
    </p><div class="verbatim-wrap"><pre class="screen"># cgset -r cpuset.mems=<em class="replaceable ">NODE</em> sysdefault/libvirt/qemu/<em class="replaceable ">KVM_NAME</em>/emulator</pre></div><p>
     To migrate pages to a node, use the <code class="command">migratepages</code>
     tool:
    </p><div class="verbatim-wrap"><pre class="screen"># migratepages <em class="replaceable ">PID</em> <em class="replaceable ">FROM-NODE</em> <em class="replaceable ">TO-NODE</em></pre></div><p>
     To check everything is fine. use: <code class="command">cat
     /proc/<em class="replaceable ">PID</em>/status | grep Cpus</code>.
    </p><div id="idm140164020923376" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Kernel NUMA/cpuset memory policy</h6><p>
      For more information see
      <a class="link" href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt" target="_blank">Kernel
      NUMA memory policy</a> and
      <a class="link" href="https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt" target="_blank">cpusets
      memory policy</a>. Check also the
      <a class="link" href="https://libvirt.org/formatdomain.html#elementsNUMATuning" target="_blank">Libvirt
      NUMA Tuning documentation</a>.
     </p></div></div><div class="sect3 " id="idm140164020919648"><div class="titlepage"><div><div><h4 class="title" id="idm140164020919648"><span class="number">5.8.5 </span><span class="name">Memory Pinning</span> <a title="Permalink" class="permalink" href="index.html#idm140164020919648">#</a></h4></div></div></div><div class="sect4 " id="idm140164020918976"><div class="titlepage"><div><div><h5 class="title" id="idm140164020918976"><span class="number">5.8.5.1 </span><span class="name">non-vNUMA Guest</span> <a title="Permalink" class="permalink" href="index.html#idm140164020918976">#</a></h5></div></div></div><p>
      On a non-vNUMA guest, pinning memory to host NUMA nodes would be done
      with:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</pre></div><p>
      This says memory should be allocated from host <span class="emphasis"><em>nodes
      0</em></span> and <span class="emphasis"><em>1</em></span>. Starting the guest will fail
      if its memory requirements cannot be fulfilled by <span class="emphasis"><em>nodes
      0</em></span> and <span class="emphasis"><em>1</em></span>.
      <code class="command">virt-install</code> also supports this configuration with
      the <code class="option">--numatune</code> option.
     </p></div><div class="sect4 " id="idm140164020914000"><div class="titlepage"><div><div><h5 class="title" id="idm140164020914000"><span class="number">5.8.5.2 </span><span class="name">vNUMA Guest</span> <a title="Permalink" class="permalink" href="index.html#idm140164020914000">#</a></h5></div></div></div><p>
      If the guest has a vNUMA topology, the method is as follows:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;numatune&gt;
   &lt;memnode cellid="0" mode="strict" nodeset="0"/&gt;
   &lt;memnode cellid="1" mode="strict" nodeset="1"/&gt;
&lt;/numatune&gt;</pre></div><p>
      This says vNUMA <span class="emphasis"><em>cell 0</em></span> gets its memory from host
      <span class="emphasis"><em>node 0</em></span>, and <span class="emphasis"><em>cell 1</em></span> from host
      <span class="emphasis"><em>node 1</em></span>. The amount of memory in each vNUMA cell
      is defined as part of the topology description (see
      <a class="xref" href="index.html#vt.best.numa.topo" title="5.8.3. Guest NUMA Topology">Section 5.8.3, “Guest NUMA Topology”</a>).
     </p></div><div class="sect4 " id="idm140164020909312"><div class="titlepage"><div><div><h5 class="title" id="idm140164020909312"><span class="number">5.8.5.3 </span><span class="name">virt-install / virt-manager</span> <a title="Permalink" class="permalink" href="index.html#idm140164020909312">#</a></h5></div></div></div><p>
      Configuring vNUMA and pinning guest memory to host nodes is not
      supported by <code class="command">virt-install</code> or
      <code class="command">virt-manager</code>.
     </p></div></div></div></div><div class="sect1 " id="vt.best.multihost"><div class="titlepage"><div><div><h2 class="title" id="vt.best.multihost"><span class="number">6 </span><span class="name">Multi-Host Configuration and Settings</span> <a title="Permalink" class="permalink" href="index.html#vt.best.multihost">#</a></h2></div></div></div><div class="sect2 " id="vt.best.stor.fs"><div class="titlepage"><div><div><h3 class="title" id="vt.best.stor.fs"><span class="number">6.1 </span><span class="name">Storage and File System</span> <a title="Permalink" class="permalink" href="index.html#vt.best.stor.fs">#</a></h3></div></div></div><div class="sect3 " id="vt.best.stor.blxvsimage"><div class="titlepage"><div><div><h4 class="title" id="vt.best.stor.blxvsimage"><span class="number">6.1.1 </span><span class="name">Block Devices Vs. Image Files</span> <a title="Permalink" class="permalink" href="index.html#vt.best.stor.blxvsimage">#</a></h4></div></div></div><p>
     A block device is a storage device (for example,
     <code class="filename">/dev/xvd<em class="replaceable ">X</em></code>,
     <code class="filename">/dev/sd<em class="replaceable ">X</em></code>,
     <code class="filename">/dev/hd<em class="replaceable ">X</em></code>). A disk image
     file is stored on the host or accessible over a network.
    </p><div class="table" id="idm140164020900368"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 3: </span><span class="name">Block Devices Vs. Disk Images </span><a title="Permalink" class="permalink" href="index.html#idm140164020900368">#</a></h6></div><div class="table-contents"><table summary="Block Devices Vs. Disk Images" border="1"><colgroup><col width="10%" class="1" /><col width="" class="2" /><col /></colgroup><thead><tr><th>
         <p>
          Technology
         </p>
        </th><th>
         <p>
          Advantages
         </p>
        </th><th>
         <p>
          Drawbacks
         </p>
        </th></tr></thead><tbody><tr><td>
         <p>
          Block devices
         </p>
        </td><td>
         <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Better performance
           </p></li><li class="listitem "><p>
            Use standard tools for administration/disk modification
           </p></li><li class="listitem "><p>
            Accessible from host (pro and con)
           </p></li></ul></div>
        </td><td>
         <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Live migration is complex
           </p></li><li class="listitem "><p>
            Impossible to increase capacity
           </p></li></ul></div>
        </td></tr><tr><td>
         <p>
          Image files
         </p>
        </td><td>
         <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Easier system management
           </p></li><li class="listitem "><p>
            Easily move, clone, expand, back up domains
           </p></li><li class="listitem "><p>
            Comprehensive toolkit (guestfs) for image manipulation
           </p></li><li class="listitem "><p>
            Reduce overhead through sparse files
           </p></li><li class="listitem "><p>
            Fully allocate for best performance
           </p></li></ul></div>
        </td><td>
         <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Less performance than block devices
           </p></li></ul></div>
        </td></tr></tbody></table></div></div></div><div class="sect3 " id="idm140164020877328"><div class="titlepage"><div><div><h4 class="title" id="idm140164020877328"><span class="number">6.1.2 </span><span class="name">NFS Storage for Images</span> <a title="Permalink" class="permalink" href="index.html#idm140164020877328">#</a></h4></div></div></div><p>
     If your image is stored on an NFS share, you should check some server
     and client parameters to improve access to the VM Guest image.
    </p><div class="sect4 " id="idm140164020876112"><div class="titlepage"><div><div><h5 class="title" id="idm140164020876112"><span class="number">6.1.2.1 </span><span class="name">NFS Read/Write (Client)</span> <a title="Permalink" class="permalink" href="index.html#idm140164020876112">#</a></h5></div></div></div><p>
      Options <code class="option">rsize</code> and <code class="option">wsize</code> specify the
      size of the chunks of data that the client and server pass back and
      forth to each other. You should ensure NFS read/write sizes are
      sufficiently large, especially for large I/O. Change the
      <code class="option">rsize</code> and <code class="option">wsize</code> parameter in your
      <code class="filename">/etc/fstab</code> by increasing the value to 16 KB,
      which will ensure that all operations can be frozen if there is any
      instance of hanging.
     </p><div class="verbatim-wrap"><pre class="screen">nfs_server:/exported/vm_images<span id="co.nfs.server"></span><span class="callout">1</span> /mnt/images<span id="co.nfs.mnt"></span><span class="callout">2</span> nfs<span id="co.nfs.nfs"></span><span class="callout">3</span> rw<span id="co.nfs.rw"></span><span class="callout">4</span>,hard<span id="co.nfs.hard"></span><span class="callout">5</span>,sync<span id="co.nfs.sync"></span><span class="callout">6</span>, rsize=8192<span id="co.nfs.rsize"></span><span class="callout">7</span>,wsize=8192<span id="co.nfs.wsize"></span><span class="callout">8</span> 0 0</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.server"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
        NFS server's host name and export path name
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.mnt"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
        Where to mount the NFS exported share
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.nfs"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
        This is an <code class="option">nfs</code> mount point
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.rw"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
        This mount point will be accessible in read/write
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.hard"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
        Determines the recovery behavior of the NFS client after an NFS
        request times out. <code class="option">hard</code> is the best option to avoid
        data corruption
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.sync"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>
        Any system call that writes data to files on that mount point causes
        that data to be flushed to the server before the system call returns
        control to user space
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.rsize"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>
        Maximum number of bytes in each network READ request that the NFS
        client can receive when reading data from a file on an NFS server
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.nfs.wsize"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>
        Maximum number of bytes per network WRITE request that the NFS
        client can send when writing data to a file on an NFS server.
       </p></td></tr></table></div></div><div class="sect4 " id="idm140164020856016"><div class="titlepage"><div><div><h5 class="title" id="idm140164020856016"><span class="number">6.1.2.2 </span><span class="name">NFS Threads (Server)</span> <a title="Permalink" class="permalink" href="index.html#idm140164020856016">#</a></h5></div></div></div><p>
      Your NFS server should have enough NFS threads to handle
      multi-threaded workloads. Use the <code class="command">nfsstat</code> tool to
      get some RPC statistics on your server:
     </p><div class="verbatim-wrap"><pre class="screen"># nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</pre></div><p>
      If the <span class="emphasis"><em>retrans</em></span> is equal to 0 everything is fine.
      Otherwise, the client needs to retransmit, so increase the
      <span class="emphasis"><em>USE_KERNEL_NFSD_NUMBER</em></span> variable in
      <code class="filename">/etc/sysconfig/nfs</code>, and adjust accordingly until
      <span class="emphasis"><em>retrans</em></span> is equal to <span class="emphasis"><em>0</em></span>.
     </p></div></div></div></div><div class="sect1 " id="vt.best.stor"><div class="titlepage"><div><div><h2 class="title" id="vt.best.stor"><span class="number">7 </span><span class="name">VM Guest Images</span> <a title="Permalink" class="permalink" href="index.html#vt.best.stor">#</a></h2></div></div></div><div class="sect2 " id="vt.best.stor.imageformat"><div class="titlepage"><div><div><h3 class="title" id="vt.best.stor.imageformat"><span class="number">7.1 </span><span class="name">VM Guest Image Format</span> <a title="Permalink" class="permalink" href="index.html#vt.best.stor.imageformat">#</a></h3></div></div></div><p>
    Certain storage formats which QEMU recognizes have their origins in
    other virtualization technologies. By recognizing these formats, QEMU
    can leverage either data stores or entire guests that were originally
    targeted to run under these other virtualization technologies. Some
    formats are supported only in read-only mode, enabling either direct use
    of that read-only data store in a QEMU guest or conversion to a fully
    supported QEMU storage format (using <code class="command">qemu-img</code>)
    which could then be used in read/write mode. See SUSE Linux Enterprise
    <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891" target="_blank">Release
    Notes</a> to get the list of supported formats.
   </p><div id="idm140164020845488" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     It is recommended to convert the disk images to either raw or qcow2 to
     achieve good performance.
    </p></div><div id="idm140164020844592" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Encryption</h6><p>
     When you create an image, you cannot use compression
     (<code class="option">-c</code>) in the output file with the encryption option
     (<code class="option">-e</code>).
    </p></div><div class="sect3 " id="idm140164020842464"><div class="titlepage"><div><div><h4 class="title" id="idm140164020842464"><span class="number">7.1.1 </span><span class="name">Raw Format</span> <a title="Permalink" class="permalink" href="index.html#idm140164020842464">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       This format is simple and easily exportable to all other
       emulators/hypervisors
      </p></li><li class="listitem "><p>
       It provides best performance (least I/O overhead)
      </p></li><li class="listitem "><p>
       If your file system supports holes (for example in ext2 or ext3 on
       Linux or NTFS on Windows*), then only the written sectors will
       reserve space
      </p></li><li class="listitem "><p>
       The raw format allows to copy a VM Guest image to a physical device
       (<code class="command">dd if=<em class="replaceable ">vmguest.raw</em>
       of=<em class="replaceable ">/dev/sda</em></code>)
      </p></li><li class="listitem "><p>
       It is byte-for-byte the same as what the VM Guest sees, so this
       wastes a lot of space
      </p></li></ul></div></div><div class="sect3 " id="idm140164020834848"><div class="titlepage"><div><div><h4 class="title" id="idm140164020834848"><span class="number">7.1.2 </span><span class="name">qcow2 Format</span> <a title="Permalink" class="permalink" href="index.html#idm140164020834848">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Use this to have smaller images (useful if your file system does not
       supports holes, for example on Windows*)
      </p></li><li class="listitem "><p>
       It has optional AES encryption
      </p></li><li class="listitem "><p>
       Zlib-based compression option
      </p></li><li class="listitem "><p>
       Support of multiple VM snapshots (internal, external)
      </p></li><li class="listitem "><p>
       Improved performance and stability
      </p></li><li class="listitem "><p>
       Supports changing the backing file
      </p></li><li class="listitem "><p>
       Supports consistency checks
      </p></li><li class="listitem "><p>
       Less performance than raw format
      </p></li></ul></div><div class="variablelist "><dl class="variablelist"><dt id="idm140164020825984"><span class="term ">l2-cache-size</span></dt><dd><p>
        qcow2 can provide the same performance for random read/write access as
        raw format, but it needs a well-sized cache size. By default cache size
        is set to 1 MB. This will give good performance up to a disk size of 8
        GB. If you need a bigger disk size, you need to adjust the cache
        size. For a disk size of 64 GB (64*1024 = 65536), you need 65536 /
        8192B = 8 MB of cache (<code class="option">-drive
        format=qcow2,l2-cache-size=8M</code>).
       </p></dd><dt id="idm140164020823504"><span class="term ">Cluster Size</span></dt><dd><p>
        The qcow2 format offers the capability to change the cluster size. The
        value must be between 512 KB and 2 MB. Smaller cluster sizes
        can improve the image file size whereas larger cluster sizes generally
        provide better performance.
       </p></dd><dt id="idm140164020821376"><span class="term ">Preallocation</span></dt><dd><p>
        An image with preallocated metadata is initially larger but can
        improve performance when the image needs to grow.
       </p></dd><dt id="idm140164020819648"><span class="term ">Lazy Refcounts</span></dt><dd><p>
        Reference count updates are postponed with the goal of avoiding
        metadata I/O and improving performance. This is particularly beneficial
        with <code class="option">cache=writethrough</code>, which does not batch metadata
        updates, but in case of host crash, the reference count tables must be
        rebuilt, this is done automatically at the next open with
        <code class="command">qemu-img check -r all</code>, but this takes some time.
       </p></dd></dl></div></div><div class="sect3 " id="idm140164020816560"><div class="titlepage"><div><div><h4 class="title" id="idm140164020816560"><span class="number">7.1.3 </span><span class="name">qed format</span> <a title="Permalink" class="permalink" href="index.html#idm140164020816560">#</a></h4></div></div></div><p>
     qed is the next-generation qcow (Qemu Copy On Write). Its
     characteristics include:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Strong data integrity because of simple design
      </p></li><li class="listitem "><p>
       Retains sparseness over non-sparse channels (e.g. HTTP)
      </p></li><li class="listitem "><p>
       Supports changing the backing file
      </p></li><li class="listitem "><p>
       Supports consistency checks
      </p></li><li class="listitem "><p>
       Fully asynchronous I/O path
      </p></li><li class="listitem "><p>
       Does not support internal snapshots
      </p></li><li class="listitem "><p>
       Relies on the host file system and cannot be stored on a logical
       volume directly
      </p></li></ul></div></div><div class="sect3 " id="idm140164020808976"><div class="titlepage"><div><div><h4 class="title" id="idm140164020808976"><span class="number">7.1.4 </span><span class="name">VMDK format</span> <a title="Permalink" class="permalink" href="index.html#idm140164020808976">#</a></h4></div></div></div><p>
     VMware 3, 4, or 6 image format, for exchanging images with that
     product.
    </p></div><div class="sect3 " id="idm140164020807680"><div class="titlepage"><div><div><h4 class="title" id="idm140164020807680"><span class="number">7.1.5 </span><span class="name">Image Information</span> <a title="Permalink" class="permalink" href="index.html#idm140164020807680">#</a></h4></div></div></div><p>
     Use <code class="command">qemu-img info
     <em class="replaceable ">vmguest.img</em></code> to get an image's
     information,such as: the format, the virtual size, the physical size,
     snapshots if available.
    </p></div><div class="sect3 " id="idm140164020805616"><div class="titlepage"><div><div><h4 class="title" id="idm140164020805616"><span class="number">7.1.6 </span><span class="name">qemu-img Reference</span> <a title="Permalink" class="permalink" href="index.html#idm140164020805616">#</a></h4></div></div></div><p>
     Refer to
     <a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create" target="_blank">SLE12
     qemu-img documentation</a> for more information on the
     <code class="command">qemu-img</code> tool and examples.
    </p></div><div class="sect3 " id="vt.best.overlay"><div class="titlepage"><div><div><h4 class="title" id="vt.best.overlay"><span class="number">7.1.7 </span><span class="name">Overlay Storage Image</span> <a title="Permalink" class="permalink" href="index.html#vt.best.overlay">#</a></h4></div></div></div><p>
     The qcow2 and qed formats provide a way to create a base image, but
     also a way to create available overlay disk images on top of the base
     image. A backing file is useful to be able to revert to a known state
     and discard the overlay.
    </p><p>
     To create an overlay image:
    </p><div class="verbatim-wrap"><pre class="screen"># qemu-img create -o<span id="co.1.minoro"></span><span class="callout">1</span>backing_file=vmguest.raw<span id="co.1.backingfile"></span><span class="callout">2</span>,backing_fmt=raw<span id="co.1.backingfmt"></span><span class="callout">3</span>\
     -f<span id="co.1.minorf"></span><span class="callout">4</span> qcow2 vmguest.cow<span id="co.1.imagename"></span><span class="callout">5</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.1.minoro"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       use <code class="option">-o ?</code> for an overview of available options
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.1.backingfile"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       the backing file name
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.1.backingfmt"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
       specify the file format for the backing file
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.1.minorf"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
       specify the image format for the VM Guest
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.1.imagename"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
       image name of the VM Guest, it will only record the differences from
       the backing file
      </p></td></tr></table></div><p>
     Now you can start your VM Guest, use it make some changes etc., and
     the backing image will be untouched and all changes to the storage will
     be recorded in the overlay image file. The backing file will never be
     modified unless you use the <code class="option">commit</code> monitor command (or
     <code class="command">qemu-img commit</code>).
    </p><div id="idm140164020789312" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Backing Image Path</h6><p>
      You should not change the path to the backing image, otherwise you
      will need to adjust it. The path is stored in the overlay image file.
      If you want to update the path, you should make a symbolic link from
      the original path to the new path and then use the
      <code class="command">qemu-img</code> <code class="option">rebase</code> option.
     </p><div class="verbatim-wrap"><pre class="screen"># ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw</pre></div><div class="verbatim-wrap"><pre class="screen"># qemu-img rebase<span id="co.2.rebase"></span><span class="callout">1</span>-u<span id="co.2.unsafe"></span><span class="callout">2</span> -b<span id="co.2.minorb"></span><span class="callout">3</span> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<span id="co.2.image"></span><span class="callout">4</span></pre></div><p>
      The <code class="command">rebase</code> subcommand tells
      <code class="command">qemu-img</code> to change the backing file image. The
      <code class="option">-u</code> option activates the unsafe mode (see note below).
      The backing image to be used is specified with <code class="option">-b</code> and
      the image path is the last argument of the command.
     </p><p>
      There are two different modes in which <code class="option">rebase</code> can
      operate:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <span class="emphasis"><em>Safe</em></span>: This is the default mode and performs a
        real rebase operation. The safe mode is a time-consuming operation.
       </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Unsafe</em></span>: The unsafe mode (<code class="option">-u</code>)
        only changes the backing files name and the format of the file name
        without making any checks on the files contents. You should use this
        mode to rename or moving a backing file.
       </p></li></ul></div></div><p>
     A common use is to initiate a new guest with the backing file. Let's
     assume we have a <code class="filename">sle12_base.img</code> VM Guest ready to
     use (fresh installation without any modification). This will be our
     backing file. Now you need to test a new package, on an updated system
     and on a system with a different kernel. We can use
     <code class="filename">sle12_base.img</code> to instantiate the new SUSE Linux Enterprise
     VM Guest by creating a qcow2 overlay file pointing to this backing
     file (<code class="filename">sle12_base.img</code>).
    </p><p>
     In our example we will use <code class="filename">sle12_updated.qcow2</code> for
     the updated system, and <code class="filename">sle12_kernel.qcow2</code> for the
     system with a different kernel.
    </p><p>
     To create the two thin provisioned systems use the
     <code class="command">qemu-img</code> command line with the <code class="option">-b</code>
     option:
    </p><div class="verbatim-wrap"><pre class="screen"># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_updated.qcow2
     Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
     backing_file='sle12_base.img' encryption=off cluster_size=65536
     lazy_refcounts=off nocow=off</pre></div><div class="verbatim-wrap"><pre class="screen"># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_kernel.qcow2
     Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
     backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536
     lazy_refcounts=off nocow=off</pre></div><p>
     The images are now usable, and you can do your test without touching
     the initial <code class="filename">sle12_base.img</code> backing file, all
     changes will be stored in the new images. Moreover, you can also use
     these new images as a backing file, and create a new overlay.
    </p><div class="verbatim-wrap"><pre class="screen"># qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</pre></div><p>
     <code class="command">qemu-img info</code> with the option
     <code class="option">--backing-chain</code> will return all information about the
     entire backing chain recursively:
    </p><div class="verbatim-wrap"><pre class="screen"># qemu-img info --backing-chain<span id="co.3.backingchain"></span><span class="callout">1</span> \
/var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.3.backingchain"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       will list information about backing files in a disk image chain
      </p></td></tr></table></div><div class="figure" id="fig.qemu-img.overlay"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/qemu-img-overlay.png"><img src="images/qemu-img-overlay.png" width="" alt="Understanding Image Overlay" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 2: </span><span class="name">Understanding Image Overlay </span><a title="Permalink" class="permalink" href="index.html#fig.qemu-img.overlay">#</a></h6></div></div></div></div><div class="sect2 " id="idm140164020759280"><div class="titlepage"><div><div><h3 class="title" id="idm140164020759280"><span class="number">7.2 </span><span class="name">Open a VM Guest Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020759280">#</a></h3></div></div></div><p>
    You should use <span class="emphasis"><em>guestfs-tools</em></span> to access the file
    system image of your VM Guest. If you do not have this tool installed
    you can mount it with other Linux tools, but you should avoid accessing
    an untrusted or unknown VM Guest's image system because this can lead
    to security issues (read
    <a class="link" href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/" target="_blank">D.
    Berrangé's post</a> for more information).
   </p><div class="sect3 " id="idm140164020756624"><div class="titlepage"><div><div><h4 class="title" id="idm140164020756624"><span class="number">7.2.1 </span><span class="name">Raw Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020756624">#</a></h4></div></div></div><div class="sect4 " id="idm140164020755952"><div class="titlepage"><div><div><h5 class="title" id="idm140164020755952"><span class="number">7.2.1.1 </span><span class="name">Mounting a Raw Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020755952">#</a></h5></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        To be able to mount the image, first you need to find a free loop
        device:
       </p><div class="verbatim-wrap"><pre class="screen"># losetup -f<span id="co.losetup.find"></span><span class="callout">1</span>
/dev/loop1</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.losetup.find"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Find first unused device
         </p></td></tr></table></div></li><li class="step "><p>
        Associate the image with the loop device:
       </p><div class="verbatim-wrap"><pre class="screen"># losetup /dev/loop1 SLE12.raw</pre></div></li><li class="step "><p>
        Check everything is fine:
       </p><div class="verbatim-wrap"><pre class="screen"># losetup -l<span id="co.losetup.list"></span><span class="callout">1</span>
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.losetup.list"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          List information about all loop devices
         </p></td></tr></table></div></li><li class="step "><p>
        Check the image's partitions with <code class="command">kpartx</code>:
       </p><div class="verbatim-wrap"><pre class="screen"># kpartx -a<span id="co.kpartx.a"></span><span class="callout">1</span> -v<span id="co.kpartx.v"></span><span class="callout">2</span> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.kpartx.a"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Add partition devmappings
         </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.kpartx.v"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
          Verbose mode
         </p></td></tr></table></div></li><li class="step "><p>
        You can now mount the image partition(s):
       </p><div class="verbatim-wrap"><pre class="screen"># mkdir /mnt/sle12mount
# mount /dev/mapper/loop1p1 /mnt/sle12mount</pre></div></li></ol></div></div><div id="idm140164020739648" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Raw image with LVM</h6><p>
       If your raw image contains an LVM volume group you should use LVM
       tools to mount the partition. Refer to
       <a class="xref" href="index.html#sect4.lvm.found" title="7.2.3. Image with LVM">Section 7.2.3, “Image with LVM”</a>
      </p></div></div><div class="sect4 " id="idm140164020737664"><div class="titlepage"><div><div><h5 class="title" id="idm140164020737664"><span class="number">7.2.1.2 </span><span class="name">Unmount a Raw Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020737664">#</a></h5></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Unmount all mounted partitions
       </p><div class="verbatim-wrap"><pre class="screen"># umount /mnt/sle12mount</pre></div></li><li class="step " id="step.umount.raw"><p>
        Delete partition devmappings with the <code class="command">kpartx</code> and
        <code class="option">-d</code> options
       </p><div class="verbatim-wrap"><pre class="screen"># kpartx -d /dev/loop1</pre></div></li><li class="step "><p>
        Detach the devices with <code class="command">losetup</code>
       </p><div class="verbatim-wrap"><pre class="screen"># losetup -d /dev/loop1</pre></div></li></ol></div></div></div></div><div class="sect3 " id="idm140164020730912"><div class="titlepage"><div><div><h4 class="title" id="idm140164020730912"><span class="number">7.2.2 </span><span class="name">Qcow2 Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020730912">#</a></h4></div></div></div><div class="sect4 " id="idm140164020730240"><div class="titlepage"><div><div><h5 class="title" id="idm140164020730240"><span class="number">7.2.2.1 </span><span class="name">Mount a Qcow2 Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020730240">#</a></h5></div></div></div><p>
      This procedure describes the step-by-step process you should follow to
      mount a qcow2 image.
     </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        First you need to probe the <span class="emphasis"><em>nbd</em></span> (network block
        devices) modules.
       </p><div class="verbatim-wrap"><pre class="screen"># modprobe nbd max_part=16<span id="co.nbd.maxpart"></span><span class="callout">1</span>
# dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.nbd.maxpart"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Number of partitions per device
         </p></td></tr></table></div></li><li class="step "><p>
        Check that <code class="filename">/dev/nbd0</code> is not used, and connect
        the VM Guest image (i.e. SLE12.qcow2) to the NBD device witht the
        <code class="command">qemu-nbd</code> command:
       </p><div class="verbatim-wrap"><pre class="screen"># qemu-nbd -c<span id="co.qemunbd.minusc"></span><span class="callout">1</span> /dev/nbd0<span id="co.qemunbd.device"></span><span class="callout">2</span> SLE12.qcow2<span id="co.qemunbd.image"></span><span class="callout">3</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.qemunbd.minusc"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Connect <code class="filename">SLE12.qcow2</code> to the local NBD device
          <code class="filename">/dev/nbd0</code>
         </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.qemunbd.device"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
          NBD device to use
         </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.qemunbd.image"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
          VM Guest image to use
         </p></td></tr></table></div></li><li class="step "><p>
        Inform the operating system about partition table changes with
        <code class="command">partprobe</code>
       </p><div class="verbatim-wrap"><pre class="screen"># partprobe /dev/nbd0 -s<span id="co.partprobe.sum"></span><span class="callout">1</span>
/dev/nbd0: msdos partitions 1 2
# dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.partprobe.sum"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Print a summary of contents
         </p></td></tr></table></div></li><li class="step "><p>
        Two partitions are available in the SLE12.qcow2 image:
        <code class="filename">/dev/nbd0p1</code> and
        <code class="filename">/dev/nbd0p2</code>. Before mounting this partition you
        need to check that there is no LVM volume with
        <code class="command">vgscan</code>.
       </p><div class="verbatim-wrap"><pre class="screen"># vgscan
No volume groups found</pre></div></li><li class="step "><p>
        No LVM volume has been found, so you can mount the partition with
        <code class="command">mount</code>. Refer to <a class="xref" href="index.html#sect4.lvm.found" title="7.2.3. Image with LVM">Section 7.2.3, “Image with LVM”</a>
        to see how to handle LVM volumes.
       </p><div class="verbatim-wrap"><pre class="screen"># mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</pre></div><p>
        You can now access your VM Guest file system.
       </p></li></ol></div></div></div><div class="sect4 " id="idm140164020706448"><div class="titlepage"><div><div><h5 class="title" id="idm140164020706448"><span class="number">7.2.2.2 </span><span class="name">Unmount a Qcow2 Image</span> <a title="Permalink" class="permalink" href="index.html#idm140164020706448">#</a></h5></div></div></div><div class="procedure " id="idm140164020705776"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 1: </span><span class="name">Cleanup </span><a title="Permalink" class="permalink" href="index.html#idm140164020705776">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        To clean up everything, use <code class="command">umount</code> to unmount the
        partition:
       </p><div class="verbatim-wrap"><pre class="screen"># umount /mnt/nbd0p2</pre></div></li><li class="step " id="step.umount.qcow2"><p>
        Disconnect the image from the <code class="filename">/dev/nbd0</code> device
       </p><div class="verbatim-wrap"><pre class="screen"># qemu-nbd -d<span id="co.qemunbd.minusd"></span><span class="callout">1</span> /dev/nbd0</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.qemunbd.minusd"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
          Disconnect the specified device
         </p></td></tr></table></div></li></ol></div></div><div id="idm140164020699008" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Free NBD devices</h6><p>
       It is pretty easy to detect whether an NBD device is free. Run the
       following command:
      </p><div class="verbatim-wrap"><pre class="screen"># lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</pre></div><p>
       If it produces a line, the NBD device is busy. This can also be
       confirmed by the presence of the
       <code class="filename">/sys/devices/virtual/block/nbd0/pid</code> file.
      </p></div></div></div><div class="sect3 " id="sect4.lvm.found"><div class="titlepage"><div><div><h4 class="title" id="sect4.lvm.found"><span class="number">7.2.3 </span><span class="name">Image with LVM</span> <a title="Permalink" class="permalink" href="index.html#sect4.lvm.found">#</a></h4></div></div></div><p>
     In case an LVM volume group has been found, <code class="command">vgscan</code>
     will return nothing, until you have passed the <code class="option">-v</code>
     (verbose) parameter.
    </p><div class="verbatim-wrap"><pre class="screen"># vgscan -v
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</pre></div><p>
     The <span class="emphasis"><em>system</em></span> LVM volume group has been found on the
     system. You can get more information about this volume with
     <code class="command">vgdisplay <em class="replaceable ">VOLUMEGROUPNAME</em></code>
     (in our case <em class="replaceable ">VOLUMEGROUPNAME</em> is
     <span class="emphasis"><em>system</em></span>). You should activate this volume group to
     expose LVM partitions as devices so the system can mount them. Use
     <code class="command">vgchange</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># vgchange -ay<span id="co.lvm.a"></span><span class="callout">1</span> -v<span id="co.lvm.v"></span><span class="callout">2</span>
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.lvm.a"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
       Activate the volume group
      </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.lvm.v"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
       Verbose mode, without this parameter the output is quiet
      </p></td></tr></table></div><p>
     All partitions in the volume group will be listed in the
     <code class="filename">/dev/mapper</code> directory. You can simply mount them
     now.
    </p><div class="verbatim-wrap"><pre class="screen"># ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

# mkdir /mnt/system-root
# mount  /dev/mapper/system-root /mnt/system-root

# ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</pre></div><p>
     To clean up:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Unmount all partitions (with <code class="command">umount</code>)
      </p><div class="verbatim-wrap"><pre class="screen"># umount /mnt/system-root</pre></div></li><li class="step "><p>
       Deactivate the LVM volume group (with <code class="command">vgchange -an
       <em class="replaceable ">VOLUMEGROUPNAME</em></code>)
      </p><div class="verbatim-wrap"><pre class="screen"># vgchange -an -v system
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</pre></div></li><li class="step "><p>
       Now you you have two choices:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         In case of Qcow2 format, end the procedure from
         <a class="xref" href="index.html#step.umount.qcow2" title="Step 2">Step 2</a> (<code class="command">qemu-nbd -d
         /dev/nbd0</code>)
        </p></li><li class="listitem "><p>
         In case of Raw format, end the procedure from
         <a class="xref" href="index.html#step.umount.raw" title="Step 2">Step 2</a> (<code class="command">kpartx -d
         /dev/loop1</code>; <code class="command">losetup -d /dev/loop1</code>)
        </p></li></ul></div></li></ol></div></div><div id="idm140164020672048" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Check cleanup is OK</h6><p>
      You should double-check that your cleanup procedure is ok by using a
      system command like <code class="command">losetup</code>,
      <code class="command">qemu-nbd</code>, <code class="command">mount</code> or
      <code class="command">vgscan</code>. If this is not the case you may have
      trouble using the VM Guest because the system image will be used in
      different places.
     </p></div></div></div><div class="sect2 " id="idm140164020668608"><div class="titlepage"><div><div><h3 class="title" id="idm140164020668608"><span class="number">7.3 </span><span class="name">File System Sharing</span> <a title="Permalink" class="permalink" href="index.html#idm140164020668608">#</a></h3></div></div></div><p>
    You can access a host directory in the VM Guest using the
    <span class="emphasis"><em>file system</em></span> element. In the following example we
    will share the <code class="filename">/data/shared</code> directory and mount it
    under the VM Guest. Note that the <span class="emphasis"><em>accessmode</em></span>
    parameter only works with <span class="emphasis"><em>type='mount'</em></span> for the
    QEMU/KVM drive.
    
    All other <span class="emphasis"><em>type</em></span> are mostly available for LXC driver.
   </p><div class="verbatim-wrap"><pre class="screen">&lt;filesystem type='mount'<span id="co.fs.mount"></span><span class="callout">1</span> accessmode='mapped'<span id="co.fs.mode"></span><span class="callout">2</span>&gt;
   &lt;source dir='/data/shared'<span id="co.fs.sourcedir"></span><span class="callout">3</span>&gt;
   &lt;target dir='shared'<span id="co.fs.targetdir"></span><span class="callout">4</span>/&gt;
&lt;/filesystem&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.fs.mount"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      A host directory to mount VM Guest
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.fs.mode"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      Access mode (the security mode) set to <span class="emphasis"><em>mapped</em></span>
      will give access with the permissions of the hypervisor. Use
      <span class="emphasis"><em>passthrough</em></span> to access this share with the
      permissions of the user inside the VM Guest
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.fs.sourcedir"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Path to share with the VM Guest
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.fs.targetdir"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
      Name or label of the path for the mount command
     </p></td></tr></table></div><p>
    Under the VM Guest now you need to mount the <span class="emphasis"><em>target
    dir='shared'</em></span>:
   </p><div class="verbatim-wrap"><pre class="screen"># mkdir /opt/mnt_shared
# mount shared -t 9p /opt/mnt_shared -o trans=virtio
# mount | grep shared shared on /opt/mnt_shared type 9p (rw,relatime,sync,dirsync,trans=virtio)</pre></div><p>
    Read
    <a class="link" href="https://libvirt.org/formatdomain.html#elementsFilesystems" target="_blank"><code class="systemitem">libvirt</code>
    File System </a> and
    <a class="link" href="http://wiki.qemu.org/Documentation/9psetup" target="_blank">QEMU
    9psetup</a> for more information.
   </p></div></div><div class="sect1 " id="vt.best.vm.perf"><div class="titlepage"><div><div><h2 class="title" id="vt.best.vm.perf"><span class="number">8 </span><span class="name">Configurations and Settings Performed within the VM Guest</span> <a title="Permalink" class="permalink" href="index.html#vt.best.vm.perf">#</a></h2></div></div></div><div class="sect2 " id="vt.best.perf.virtio"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.virtio"><span class="number">8.1 </span><span class="name">Virtio Driver</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.virtio">#</a></h3></div></div></div><p>
    To increase VM Guest performance it is recommended to use PV drivers,
    the host implementation is in user space, so no driver is needed in the
    host. Virtio is a virtualization standard, so the guest's device driver
    is aware that it is running in a virtual environment. Note that virtio
    used in KVM is different, but architecturally similar to the Xen
    paravirtualized device drivers (like
    <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
    in a Windows* guest).
   </p><div id="idm140164020647232" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: I/O in virtualization</h6><p>
     To have a better understanding of this topic refer to the
     <a class="link" href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html" target="_blank">I/O
     Virtualization</a> section in the official Virtualization guide.
    </p></div><div class="sect3 " id="idm140164020645152"><div class="titlepage"><div><div><h4 class="title" id="idm140164020645152"><span class="number">8.1.1 </span><span class="name">virtio blk</span> <a title="Permalink" class="permalink" href="index.html#idm140164020645152">#</a></h4></div></div></div><p>
     <span class="emphasis"><em>virtio_blk</em></span> is the virtio block device for disk.
    </p><div id="idm140164020643616" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: QEMU option</h6><p>
      The <code class="option">-hd[abcd]</code> for virtio disk will not work, you must
      use <code class="option">-drive</code> instead.
     </p></div><div id="idm140164020641472" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Disk name change</h6><p>
      For a Linux guest, the disk will show up as
      <code class="option">/dev/vd[a-z][1-9]</code>. If you migrate from a non-virtio
      disk you need to change <code class="option">root=</code> in GRUB config, and
      regenerate the <code class="filename">initrd</code> file, otherwise the system
      will not be able to boot.
     </p></div><p>
     Example of a virtio disk definition:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><p>
     This is preferable to removing every disk block in the XML
     configuration containing <span class="emphasis"><em>&lt;address .*&gt;</em></span>
     because it will be regenerated automatically.
    </p></div><div class="sect3 " id="idm140164020636576"><div class="titlepage"><div><div><h4 class="title" id="idm140164020636576"><span class="number">8.1.2 </span><span class="name">virtio net</span> <a title="Permalink" class="permalink" href="index.html#idm140164020636576">#</a></h4></div></div></div><p>
     <span class="emphasis"><em>virtio_net</em></span> is the virtio network device. The
     kernel modules should be insmoded automatically in the guest at boot
     time. You need to start the service to make the network available.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</pre></div></div><div class="sect3 " id="vt.best.virtio.balloon"><div class="titlepage"><div><div><h4 class="title" id="vt.best.virtio.balloon"><span class="number">8.1.3 </span><span class="name">virtio balloon</span> <a title="Permalink" class="permalink" href="index.html#vt.best.virtio.balloon">#</a></h4></div></div></div><p>
     The virtio balloon is used for host memory over-commits for guests. For
     Linux guests, the balloon driver runs in the guest kernel, whereas for
     Windows guests, the balloon driver is in the VMDP package.
     <span class="emphasis"><em>virtio_balloon</em></span> is a PV driver to give or take
     memory from a VM Guest.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="emphasis"><em>Inflate balloon</em></span>: Return memory from guest to
       host kernel (for kvm) or to hypervisor (for xen)
      </p></li><li class="listitem "><p>
       <span class="emphasis"><em>Deflate balloon</em></span>: Guest will have more available
       memory
      </p></li></ul></div><p>
     It is controlled by the <span class="emphasis"><em>currentMemory</em></span> and
     <span class="emphasis"><em>memory</em></span> options.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
    &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</pre></div><p>
     You can also use <code class="command">virsh</code> to change it:
    </p><div class="verbatim-wrap"><pre class="screen"># virsh setmem <em class="replaceable ">DOMAIN_ID</em> <em class="replaceable ">MEMORY in KB</em></pre></div></div><div class="sect3 " id="idm140164020625104"><div class="titlepage"><div><div><h4 class="title" id="idm140164020625104"><span class="number">8.1.4 </span><span class="name">Checking virtio Presence</span> <a title="Permalink" class="permalink" href="index.html#idm140164020625104">#</a></h4></div></div></div><p>
     You can check the virtio block pci with:
    </p><div class="verbatim-wrap"><pre class="screen"># find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</pre></div><p>
     To find the block device associated with <code class="filename">vdX</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</pre></div><p>
     To get more information on the virtio block:
    </p><div class="verbatim-wrap"><pre class="screen"># udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</pre></div><p>
     To check all virtio drivers being used:
    </p><div class="verbatim-wrap"><pre class="screen"># find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</pre></div></div><div class="sect3 " id="idm140164020618000"><div class="titlepage"><div><div><h4 class="title" id="idm140164020618000"><span class="number">8.1.5 </span><span class="name">Find Device Driver Options</span> <a title="Permalink" class="permalink" href="index.html#idm140164020618000">#</a></h4></div></div></div><p>
     Virtio devices and other drivers have various options, to list all of
     them use the <code class="option">help</code> parameter of
     the<code class="command">qemu-system-ARCH</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"># qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</pre></div></div></div><div class="sect2 " id="vt.best.perf.cirrus"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.cirrus"><span class="number">8.2 </span><span class="name">Cirrus Video Driver</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.cirrus">#</a></h3></div></div></div><p>
    To get 16-bit color, high compatibility and better performance it is
    recommended to use the <span class="emphasis"><em>cirrus</em></span> video driver.
   </p><div id="idm140164020613072" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: <code class="systemitem">libvirt</code></h6><p>
     <code class="systemitem">libvirt</code> ignores the vram value because video size has been hardcoded
     in QEMU.
    </p></div><div class="verbatim-wrap"><pre class="screen">&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</pre></div></div><div class="sect2 " id="vt.best.entropy"><div class="titlepage"><div><div><h3 class="title" id="vt.best.entropy"><span class="number">8.3 </span><span class="name">Better Entropy</span> <a title="Permalink" class="permalink" href="index.html#vt.best.entropy">#</a></h3></div></div></div><p>
    Virtio RNG is a paravirtualized device that is exposed as a hardware RNG
    device to the guest. On the host side, it can be wired up to one of
    several sources of entropy, including a real hardware RNG device as well
    as the host's <code class="filename">/dev/random</code> if hardware support does
    not exist. The Linux kernel contains the guest driver for the device
    from version 2.6.26 and higher.
   </p><p>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications. The virtual
    random number generator device (paravirtualized device) allows the host
    to pass through entropy to VM Guest operating systems. This results in
    a better entropy in the VM Guest.
   </p><p>
    To use the <code class="filename">hwrng</code>, add the device in the XML
    configuration:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
   &lt;rng model='virtio'&gt;
   &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</pre></div><p>
    The host now should used <code class="filename">/dev/random</code>:
   </p><div class="verbatim-wrap"><pre class="screen"># lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</pre></div><p>
    On the VM Guest the source of entropy can be checked with <code class="command">cat
    /sys/devices/virtual/misc/hw_random/rng_available</code> and the
    current device used for entropy can be checked with:
   </p><div class="verbatim-wrap"><pre class="screen"># cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</pre></div><p>
    You should install the <span class="emphasis"><em>rn-tools</em></span> package on the
    VM Guest, enable the service, and start it. Under SLE12 do the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"># zypper in rng-tools
# systemctl enable rng-tools
# systemctl start rng-tools</pre></div></div><div class="sect2 " id="vt.best.perf.disable"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.disable"><span class="number">8.4 </span><span class="name">Disable Unused Tools and Devices</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.disable">#</a></h3></div></div></div><p>
    Per host, use one virtualization technology only. For example, do not
    use KVM and Containers on the same computer. Otherwise, you may find
    yourself with a reduced amount of available resources, increased
    security risk and a longer software update queue. Even when the amount
    of resources allocated to each of the technologies is configured
    carefully, the host may suffer from reduced overall availability and
    degraded performance.
   </p><p>
    Minimize the amount of software and services available on hosts. Most
    default installations of operating systems are not optimized for VM
    usage. Install what you really need and remove all other components in
    the VM Guest.
   </p><p>
    Windows* Guest:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Disable the screen saver
     </p></li><li class="listitem "><p>
      Remove all graphical effects
     </p></li><li class="listitem "><p>
      Disable indexing of hard disks if not necessary
     </p></li><li class="listitem "><p>
      Check the list of started services and disable the ones you do not
      need
     </p></li><li class="listitem "><p>
      Check and remove all unneeded devices
     </p></li><li class="listitem "><p>
      Disable system update if not needed, or configure it to avoid any
      delay while rebooting or shutting down the host
     </p></li><li class="listitem "><p>
      Check the Firewall rules
     </p></li><li class="listitem "><p>
      Schedule backups and anti-virus updates appropriately
     </p></li><li class="listitem "><p>
      Install the
      <a class="link" href="https://www.suse.com/products/vmdriverpack/" target="_blank">VMDP</a>
      para-virtualized driver for best performance
     </p></li><li class="listitem "><p>
      Check the operating system recommendations, such as on the
      <a class="link" href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7" target="_blank">Microsoft
      Windows* 7 better performance</a> Web page.
     </p></li></ul></div><p>
    Linux Guest:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Remove Xorg start if not needed
     </p></li><li class="listitem "><p>
      Check the list of started services and disable the ones you do not
      need
     </p></li><li class="listitem "><p>
      Check the OS recommendations for kernel parameters that enable better
      performance
     </p></li><li class="listitem "><p>
      Only install software that you really need
     </p></li><li class="listitem "><p>
      Optimize the scheduling of predictable tasks (system updates, hard
      disk checks, etc.)
     </p></li></ul></div></div><div class="sect2 " id="vt.best.perf.mtype"><div class="titlepage"><div><div><h3 class="title" id="vt.best.perf.mtype"><span class="number">8.5 </span><span class="name">Updating the Guest Machine Type</span> <a title="Permalink" class="permalink" href="index.html#vt.best.perf.mtype">#</a></h3></div></div></div><p>
    QEMU machine types define some details of the architecture that are
    particularly relevant in the context of migration and save/restore,
    where all the details of the virtual machine ABI need to be carefully
    accounted for. As changes or improvements to QEMU are made or certain
    types of fixes are done, new machine types are created which include
    those changes. Though the older machine types used for supported
    versions of QEMU are still valid to use, the user would do well to try
    to move to the latest machine type supported by the current release, so
    as to be able to take advantage of all the changes represented in that
    machine type.
   </p><p>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent, whereas for Windows* guests, it is probably a good idea to
    take a snapshot or backup of the guest in case Windows* has issues with
    the changes it detects and subsequently the user decides to revert to
    the original machine type the guest was created with.
   </p><div id="idm140164020578320" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Changing the machine type</h6><p>
     Refer to the Virtualization guide section
     <a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh" target="_blank">Change
     Machine Type</a> for documentation.
    </p></div></div></div><div class="sect1 " id="vt.best.vm.setup.config"><div class="titlepage"><div><div><h2 class="title" id="vt.best.vm.setup.config"><span class="number">9 </span><span class="name">VM Guest-Specific Configurations and Settings</span> <a title="Permalink" class="permalink" href="index.html#vt.best.vm.setup.config">#</a></h2></div></div></div><div class="sect2 " id="vt.best.acpi"><div class="titlepage"><div><div><h3 class="title" id="vt.best.acpi"><span class="number">9.1 </span><span class="name">ACPI Testing</span> <a title="Permalink" class="permalink" href="index.html#vt.best.acpi">#</a></h3></div></div></div><p>
    The ability to change a VM Guest's state heavily depends on the
    operating system. It is very important to test this feature before any
    use of your VM Guest in production. For example most Linux operating
    system disable this capability by default, so this requires you to
    enable this operation (mostly through Policy Kit).
   </p><p>
    ACPI must be enabled in the guest for a graceful shutdown to work. To
    check if ACPI is enabled, run:
   </p><div class="verbatim-wrap"><pre class="screen"># virsh dumpxml <em class="replaceable ">VMNAME</em> | grep acpi</pre></div><p>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <code class="command">virsh edit</code> to add the following XML under
    &lt;domain&gt;:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</pre></div><p>
    If ACPI was enabled during a Windows* Server 20XX guest installation,
    turning it on in the VM Guest configuration alone is not sufficient.
    See the following articles for more information:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><a class="link" href="http://support.microsoft.com/kb/314088/EN-US/" target="_blank">http://support.microsoft.com/kb/314088/EN-US/</a>
    </td></tr><tr><td><a class="link" href="http://support.microsoft.com/?kbid=309283" target="_blank">http://support.microsoft.com/?kbid=309283</a>
    </td></tr></table><p>
    A graceful shutdown is of course always possible from within the guest
    operating system, regardless of the VM Guest's configuration.
   </p></div><div class="sect2 " id="vt.best.guest.kbd"><div class="titlepage"><div><div><h3 class="title" id="vt.best.guest.kbd"><span class="number">9.2 </span><span class="name">Keyboard Layout</span> <a title="Permalink" class="permalink" href="index.html#vt.best.guest.kbd">#</a></h3></div></div></div><p>
    Though it is possible to specify the keyboard layout from a
    <code class="command">qemu-system-ARCH</code> command, it is recommended to do
    this configuration in the <code class="systemitem">libvirt</code> XML file. To change the keyboard
    layout while connecting to a remote VM Guest using vnc, you should edit
    the VM Guest XML configuration file. The XML is located at
    <code class="filename">/etc/libvirt/<em class="replaceable ">HYPERVISOR</em></code>.
    For example, to add an "en-us" keymap, add in the
    <span class="emphasis"><em>&lt;devices&gt;</em></span> section:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</pre></div><p>
    Check the vncdisplay configuration and connect to your VM Guest:
   </p><div class="verbatim-wrap"><pre class="screen"># virsh vncdisplay sles12 127.0.0.1:0</pre></div></div><div class="sect2 " id="vt.best.guest.spice_default_url"><div class="titlepage"><div><div><h3 class="title" id="vt.best.guest.spice_default_url"><span class="number">9.3 </span><span class="name">Spice default listen URL</span> <a title="Permalink" class="permalink" href="index.html#vt.best.guest.spice_default_url">#</a></h3></div></div></div><p>
    If no network interface other than <span class="emphasis"><em>lo</em></span> is assigned
    an IPv4 address on the host, the default address the spice server will
    listen on will not work. An error like the following one will occur:
   </p><div class="verbatim-wrap"><pre class="screen"># virsh start sles12
error: Failed to start domain sles12
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</pre></div><p>
    To change this you can change the default
    <span class="emphasis"><em>spice_listen</em></span> value in
    <code class="filename">/etc/libvirt/qemu.conf</code> using the local ipv6 address
    <span class="emphasis"><em>::1</em></span>. The spice server listening address can also be
    changed on a per VM Guest basis, use <code class="command">virsh edit</code> to
    add the listen XML attribute to the <span class="emphasis"><em>graphics
    type='spice'</em></span> element:
   </p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;&gt;</pre></div></div><div class="sect2 " id="idm140164020554960"><div class="titlepage"><div><div><h3 class="title" id="idm140164020554960"><span class="number">9.4 </span><span class="name">XML to QEMU command line</span> <a title="Permalink" class="permalink" href="index.html#idm140164020554960">#</a></h3></div></div></div><p>
    Sometimes it could be useful to get the QEMU command line to launch the
    VM Guest from the XML file. Use <code class="command">virsh</code> with
    <code class="option">domxml</code>.
   </p><div class="verbatim-wrap"><pre class="screen"># virsh domxml-to-native<span id="co.domxml.native"></span><span class="callout">1</span> qemu-argv<span id="co.domxml.argv"></span><span class="callout">2</span> SLE12.xml<span id="co.domxml.file"></span><span class="callout">3</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.domxml.native"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      Convert the file xml in domain XML format to the native guest
      configuration
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.domxml.argv"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      For QEMU/KVM hypervisor, the format argument must be qemu-argv
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.domxml.file"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Domain XML file to use
     </p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen"># virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE12.xml
   LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE12 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE12.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE12.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</pre></div></div><div class="sect2 " id="vt.best.kernel.parameter"><div class="titlepage"><div><div><h3 class="title" id="vt.best.kernel.parameter"><span class="number">9.5 </span><span class="name">Change Kernel Parameters at boot time</span> <a title="Permalink" class="permalink" href="index.html#vt.best.kernel.parameter">#</a></h3></div></div></div><div class="sect3 " id="idm140164020541536"><div class="titlepage"><div><div><h4 class="title" id="idm140164020541536"><span class="number">9.5.1 </span><span class="name">SLE11 SPX product</span> <a title="Permalink" class="permalink" href="index.html#idm140164020541536">#</a></h4></div></div></div><p>
     To change the value at boot time for SLE11 SPX product, you need to
     modify your <code class="filename">/boot/grub/menu.lst</code> file, adding the
     <code class="option">OPTION=parameter</code>, and then reboot your system.
    </p></div><div class="sect3 " id="idm140164020539328"><div class="titlepage"><div><div><h4 class="title" id="idm140164020539328"><span class="number">9.5.2 </span><span class="name">SLE12 SPX product</span> <a title="Permalink" class="permalink" href="index.html#idm140164020539328">#</a></h4></div></div></div><p>
     To change the value at boot time for SLE12 SPX product, you need to
     modify your <code class="filename">/etc/default/grub</code> file. Find the
     variable starting with <code class="option">GRUB_CMDLINE_LINUX_DEFAULT</code> and
     add at the end <code class="option">OPTION=parameter</code> (or change it with the
     correct value if it is already available).
    </p><p>
     Now you need to regenerate your grub2 config with:
    </p><div class="verbatim-wrap"><pre class="screen"># grub2-mkconfig -o /boot/grub2/grub.cfg</pre></div><p>
     Then reboot your system
    </p></div></div><div class="sect2 " id="idm140164020535088"><div class="titlepage"><div><div><h3 class="title" id="idm140164020535088"><span class="number">9.6 </span><span class="name">Add Device to an XML Configuration</span> <a title="Permalink" class="permalink" href="index.html#idm140164020535088">#</a></h3></div></div></div><p>
    If you want to create a new VM Guest based on an XML file, and you are
    not familiar with XML configuration syntax, you can specify the QEMU
    command line using the special tag
    <span class="emphasis"><em>qemu:commandline</em></span>. For example if you want to add a
    virtio-balloon-pci, add this block at the end of the XML configuration
    file (before the &lt;/domain&gt; tag).
   </p><div class="verbatim-wrap"><pre class="screen">&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</pre></div></div></div><div class="sect1 " id="idm140164020532128"><div class="titlepage"><div><div><h2 class="title" id="idm140164020532128"><span class="number">10 </span><span class="name">Hypervisors Vs. Containers</span> <a title="Permalink" class="permalink" href="index.html#idm140164020532128">#</a></h2></div></div></div><div class="table" id="idm140164020531184"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 4: </span><span class="name">Hypervisors Vs. Containers </span><a title="Permalink" class="permalink" href="index.html#idm140164020531184">#</a></h6></div><div class="table-contents"><table summary="Hypervisors Vs. Containers" border="1"><colgroup><col width="20%" /><col width="30%" /><col width="30%" /></colgroup><thead><tr><th>
       <p>
        Features
       </p>
      </th><th>
       <p>
        Hypervisors
       </p>
      </th><th>
       <p>
        Containers
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        Technologies
       </p>
      </td><td>
       <p>
        Emulation of a physical computing environment
       </p>
      </td><td>
       <p>
        Use kernel host
       </p>
      </td></tr><tr><td>
       <p>
        System layer level
       </p>
      </td><td>
       <p>
        Managed by a virtualization layer (Hypervisor)
       </p>
      </td><td>
       <p>
        Rely on kernel namespaces and cgroups
       </p>
      </td></tr><tr><td>
       <p>
        Level (layer)
       </p>
      </td><td>
       <p>
        Hardware level
       </p>
      </td><td>
       <p>
        Software level
       </p>
      </td></tr><tr><td>
       <p>
        Virtualization mode available
       </p>
      </td><td>
       <p>
        FV or PV
       </p>
      </td><td>
       <p>
        None, only userspace
       </p>
      </td></tr><tr><td>
       <p>
        Security
       </p>
      </td><td>
       <p>
        Strong
       </p>
      </td><td>
       <div id="idm140164020510768" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Security is very low
        </p></div>
      </td></tr><tr><td>
       <p>
        Confinement
       </p>
      </td><td>
       <p>
        Full isolation
       </p>
      </td><td>
       <div id="idm140164020507552" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Host kernel (OS must be compatible with kernel version)
        </p></div>
      </td></tr><tr><td>
       <p>
        Operating system
       </p>
      </td><td>
       <p>
        Any operating system
       </p>
      </td><td>
       <p>
        Only Linux (must be "kernel" compatible)
       </p>
      </td></tr><tr><td>
       <p>
        Type of system
       </p>
      </td><td>
       <p>
        Full OS needed
       </p>
      </td><td>
       <p>
        Scope is an instance of Linux
       </p>
      </td></tr><tr><td>
       <p>
        Boot time
       </p>
      </td><td>
       <p>
        Slow to start (OS delay)
       </p>
      </td><td>
       <p>
        Really quick start
       </p>
      </td></tr><tr><td>
       <p>
        Overhead
       </p>
      </td><td>
       <p>
        High
       </p>
      </td><td>
       <p>
        Very low
       </p>
      </td></tr><tr><td>
       <p>
        Efficiency
       </p>
      </td><td>
       <p>
        Depends on OS
       </p>
      </td><td>
       <p>
        Very efficient
       </p>
      </td></tr><tr><td>
       <p>
        Sharing with host
       </p>
      </td><td>
       <div id="idm140164020490896" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Complex because of isolation
        </p></div>
      </td><td>
       <p>
        Sharing is easy (host sees everything; container sees its own
        objects)
       </p>
      </td></tr><tr><td>
       <p>
        Migration
       </p>
      </td><td>
       <p>
        Supports migration (live mode)
       </p>
      </td><td>
       <div id="idm140164020486768" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Not possible
        </p></div>
      </td></tr></tbody></table></div></div><div class="sect2 " id="idm140164020485312"><div class="titlepage"><div><div><h3 class="title" id="idm140164020485312"><span class="number">10.1 </span><span class="name">Getting the Best of Both Worlds</span> <a title="Permalink" class="permalink" href="index.html#idm140164020485312">#</a></h3></div></div></div><p>
    Even if the above table seems to indicate that running a single
    application in a very secure way is not possible, starting with SUSE Linux Enterprise Server 12
    SP1 <code class="command">virt-sandbox</code> will allow running a single
    application in a KVM guest. <code class="command">virt-sandbox</code> bootstraps
    any command within a Linux kernel with a minimal root file system.
   </p><p>
    The guest root file system can either be the root file system mounted
    read-only or a disk image. The following steps will show how to set up a
    sandbox with qcow2 disk image as root file system.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the disk image using <code class="command">qemu-img</code>:
     </p><div class="verbatim-wrap"><pre class="screen"># qemu-img create -f qcow2 rootfs.qcow2 6G</pre></div></li><li class="step "><p>
      Format the disk image:
     </p><div class="verbatim-wrap"><pre class="screen"># modprobe nbd<span id="co.vsmkfs.modprobe"></span><span class="callout">1</span>
# /usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<span id="co.vsmkfs.qemu-nbd"></span><span class="callout">2</span>
# mkfs.ext3 /dev/nbd0<span id="co.vsmkfs.do"></span><span class="callout">3</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsmkfs.modprobe"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
        Make sure the nbd module is loaded: it is not loaded by default and
        will only be used to format the qcow image.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsmkfs.qemu-nbd"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
        Create an NBD device for the qcow2 image. This device will then
        behave like any other block device. The example uses
        <em class="replaceable ">/dev/nbd0</em> but any other free NBD device
        will work.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsmkfs.do"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
        Format the disk image directly. Note that no partition table has
        been created: <code class="command">virt-sandbox</code> considers the image to
        be a partition, not a disk.
       </p><p>
        The partition formats that can be used are limited: the Linux kernel
        bootstrapping the sandbox needs to have the corresponding features
        built in. The ext4 module is also available at the sandbox start-up
        time.
       </p></td></tr></table></div></li><li class="step "><p>
      Now populate the newly formatted image:
     </p><div class="verbatim-wrap"><pre class="screen"># guestmount -a base.qcow2 -m /dev/sda:/ /mnt<span id="co.vsfs.mount"></span><span class="callout">1</span>

# zypper --root /mnt ar cd:///?devices=/dev/dvd SLES12_DVD
# zypper --root /mnt in -t pattern Minimal<span id="co.vsfs.install"></span><span class="callout">2</span>

# guestunmount /mnt<span id="co.vsfs.unmount"></span><span class="callout">3</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsfs.mount"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
        Mount the qcow2 image using the <code class="command">guestfs</code> tools.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsfs.install"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
        Use zypper with the <span class="emphasis"><em>--root</em></span> parameter to add a
        SUSE Linux Enterprise Server repository and install the Minimal pattern in the disk image.
        Any additional package or configuration change should be done in
        this step.
       </p><div id="idm140164020465072" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Using backing chains</h6><p>
         To share the root file system between several sandboxes, create
         qcow2 images with a common disk image as backing chain as described
         in the <a class="link" href="index.html#vt.best.overlay" title="7.1.7. Overlay Storage Image">Overlay Storage Image
         section</a>.
        </p></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vsfs.unmount"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
        Unmount the qcow2 image.
       </p></td></tr></table></div></li><li class="step "><p>
      Run the sandbox, using <code class="command">virt-sandbox</code>. This command
      has many interesting options, read its man page to discover them all.
      It can either be running as root or as an unprivileged user.
     </p><div class="verbatim-wrap"><pre class="screen"># virt-sandbox -n <em class="replaceable ">name</em> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <span id="co.vs.rootfs"></span><span class="callout">1</span>
     -m host-bind:/srv/www=/guests/www \ <span id="co.vs.bind"></span><span class="callout">2</span>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <span id="co.vs.tmpfs"></span><span class="callout">3</span>
     -N source=default,address=192.168.122.12/24 \ <span id="co.vs.net"></span><span class="callout">4</span>
     -- \
     <em class="replaceable ">/bin/sh</em></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#co.vs.rootfs"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
        Mount the created disk image as the root file system. Note that
        without any image being mounted as <code class="filename">/</code>, the host
        root file system is read-only mounted as the guest one.
       </p><p>
        The host-image mount is not reserved for the root file system, it
        can be used to mount any disk image anywhere in the guest.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vs.bind"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
        The host-bind mount is pretty convenient for sharing files and
        folders between the host and the guest. In this example the host
        folder <code class="filename">/guests/www</code> is mounted as
        <code class="filename">/srv/www</code> in the sandbox.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vs.tmpfs"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
        The ram mounts are defining <span class="emphasis"><em>tmpfs</em></span> mounts in the
        sandbox.
       </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#co.vs.net"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
        The network uses a network defined in libvirt. When running as an
        unprivileged user, the source can be omitted, and the KVM user
        networking feature will be used. Using this option requires
        <span class="emphasis"><em>dhcp-client</em></span> and <span class="emphasis"><em>iproute2</em></span>,
        which is the case with the SUSE Linux Enterprise Server Minimal pattern.
       </p></td></tr></table></div></li></ol></div></div></div></div><div class="sect1 " id="idm140164020447104"><div class="titlepage"><div><div><h2 class="title" id="idm140164020447104"><span class="number">11 </span><span class="name">External References</span> <a title="Permalink" class="permalink" href="index.html#idm140164020447104">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf" target="_blank">Increasing
     memory density using KSM</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.linux-kvm.org/page/KSM" target="_blank">linux-kvm.org
     KSM</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/vm/ksm.txt" target="_blank">KSM's
     kernel documentation</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://lwn.net/Articles/329123/" target="_blank">ksm - dynamic page
     sharing driver for linux v4</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.espenbraastad.no/post/memory-ballooning/" target="_blank">Memory
     Ballooning</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://wiki.libvirt.org/page/Virtio" target="_blank">libvirt
     virtio</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt" target="_blank">CFQ's
     kernel documentation</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt" target="_blank">Documentation
     for sysctl</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://lwn.net/Articles/525459/" target="_blank">LWN Random
     Number</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://wiki.mikejung.biz/KVM_/_Xen" target="_blank">KVM Xen
     tweaks</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf" target="_blank">Dr.
     Khoa Huynh, IBM Linux Technology Center</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/kernel-parameters.txt" target="_blank">Kernel
     Parameters</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://lwn.net/Articles/374424/" target="_blank">Huge pages
     Administration (Mel Gorman)</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">kernel
     hugetlbpage</a>
    </p></li></ul></div></div></div></div><div class="page-bottom"><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>