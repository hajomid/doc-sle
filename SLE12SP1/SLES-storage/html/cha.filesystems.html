<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Overview of File Systems in Linux | Storage Administration Guide | SUSE Linux Enterprise Server 12 SP1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><meta name="product-name" content="SUSE Linux Enterprise Server" /><meta name="product-number" content="12 SP1" /><meta name="book-title" content="Storage Administration Guide" /><meta name="chapter-title" content="Chapter 1. Overview of File Systems in Linux" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="fs@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE Linux Enterprise Server 12 SP1" /><link rel="home" href="index.html" title="SUSE Linux Enterprise Server Documentation" /><link rel="up" href="part.filesystems.html" title="Part I. File Systems and Mounting" /><link rel="prev" href="part.filesystems.html" title="Part I. File Systems and Mounting" /><link rel="next" href="cha.resize_fs.html" title="Chapter 2. Resizing File Systems" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="stor_admin.html">Storage Administration Guide</a><span> › </span><a class="crumb" href="part.filesystems.html">File Systems and Mounting</a><span> › </span><a class="crumb" href="cha.filesystems.html">Overview of File Systems in Linux</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Storage Administration Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="storage.preface.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="part.filesystems.html"><span class="number">I </span><span class="name">File Systems and Mounting</span></a><ol><li class="inactive"><a href="cha.filesystems.html"><span class="number">1 </span><span class="name">Overview of File Systems in Linux</span></a></li><li class="inactive"><a href="cha.resize_fs.html"><span class="number">2 </span><span class="name">Resizing File Systems</span></a></li><li class="inactive"><a href="cha.uuid.html"><span class="number">3 </span><span class="name">Using UUIDs to Mount Devices</span></a></li></ol></li><li class="inactive"><a href="part.lvm.html"><span class="number">II </span><span class="name">Logical Volumes (LVM)</span></a><ol><li class="inactive"><a href="cha.lvm.html"><span class="number">4 </span><span class="name">LVM Configuration</span></a></li><li class="inactive"><a href="cha.lvm_snapshots.html"><span class="number">5 </span><span class="name">LVM Volume Snapshots</span></a></li></ol></li><li class="inactive"><a href="part.software_raid.html"><span class="number">III </span><span class="name">Software RAID</span></a><ol><li class="inactive"><a href="cha.raid.html"><span class="number">6 </span><span class="name">Software RAID Configuration</span></a></li><li class="inactive"><a href="cha.raidroot.html"><span class="number">7 </span><span class="name">Configuring Software RAID 1 for the Root Partition</span></a></li><li class="inactive"><a href="cha.raid10.html"><span class="number">8 </span><span class="name">Creating Software RAID 10 Devices</span></a></li><li class="inactive"><a href="cha.raid_degraded.html"><span class="number">9 </span><span class="name">Creating a Degraded RAID Array</span></a></li><li class="inactive"><a href="cha.raid_resize.html"><span class="number">10 </span><span class="name">Resizing Software RAID Arrays with mdadm</span></a></li><li class="inactive"><a href="cha.raid_leds.html"><span class="number">11 </span><span class="name">Storage Enclosure LED Utilities for MD Software RAIDs</span></a></li></ol></li><li class="inactive"><a href="part.net_storage.html"><span class="number">IV </span><span class="name">Network Storage</span></a><ol><li class="inactive"><a href="cha.isns.html"><span class="number">12 </span><span class="name">iSNS for Linux</span></a></li><li class="inactive"><a href="cha.iscsi.html"><span class="number">13 </span><span class="name">Mass Storage over IP Networks: iSCSI</span></a></li><li class="inactive"><a href="cha.fcoe.html"><span class="number">14 </span><span class="name">Fibre Channel Storage over Ethernet Networks: FCoE</span></a></li><li class="inactive"><a href="cha.multipath.html"><span class="number">15 </span><span class="name">Managing Multipath I/O for Devices</span></a></li><li class="inactive"><a href="cha.nfs4_acls.html"><span class="number">16 </span><span class="name">Managing Access Control Lists over NFSv4</span></a></li></ol></li><li class="inactive"><a href="part.storage_trouble.html"><span class="number">V </span><span class="name">Troubleshooting</span></a><ol><li class="inactive"><a href="cha.storage_trouble.html"><span class="number">17 </span><span class="name">Troubleshooting Storage Issues</span></a></li></ol></li><li class="inactive"><a href="storage.docupdates.html"><span class="number">A </span><span class="name">Documentation Updates</span></a></li><li class="inactive"><a href="bk08apb.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Part I. File Systems and Mounting" href="part.filesystems.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 2. Resizing File Systems" href="cha.resize_fs.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="stor_admin.html">Storage Administration Guide</a><span> › </span><a class="crumb" href="part.filesystems.html">File Systems and Mounting</a><span> › </span><a class="crumb" href="cha.filesystems.html">Overview of File Systems in Linux</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Part I. File Systems and Mounting" href="part.filesystems.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 2. Resizing File Systems" href="cha.resize_fs.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="chapter " id="cha.filesystems" lang="en"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">12 SP1</span></span></span></div><div><h2 class="title"><span class="number">1 </span><span class="name">Overview of File Systems in Linux</span> </h2><div class="doc-status"><ul><li><span class="ds-label">Filename: </span>storage_filesystems.xml</li><li><span class="ds-label">ID: </span>cha.filesystems</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha.filesystems.html#sec.filesystems.glossary"><span class="number">1.1 </span><span class="name">Terminology</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sec.filesystems.major"><span class="number">1.2 </span><span class="name">Major File Systems in Linux</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sec.filesystems.other"><span class="number">1.3 </span><span class="name">Other Supported File Systems</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sec.filesystems.lfs"><span class="number">1.4 </span><span class="name">Large File Support in Linux</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sect.filesystems.stor_limits"><span class="number">1.5 </span><span class="name">Linux Kernel Storage Limitations</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sect.filesystems.trouble"><span class="number">1.6 </span><span class="name">Troubleshooting File Systems</span></a></span></dt><dt><span class="sect1"><a href="cha.filesystems.html#sec_filesystems_info"><span class="number">1.7 </span><span class="name">Additional Information</span></a></span></dt></dl></div></div><p>
  <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> ships with several different file systems from which
  to choose, including Btrfs, Ext4, Ext3, Ext2, ReiserFS and XFS. Each file
  system has its own advantages and disadvantages. For a side-by-side
  feature comparison of the major operating systems in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>, see
  <a class="link" href="http://www.suse.com/products/server/technical-information/#FileSystem" target="_blank">http://www.suse.com/products/server/technical-information/#FileSystem</a>
  (File System Support and Sizes).
 </p><p>
  Professional high-performance setups might require a highly available
  storage systems. To meet the requirements of high-performance clustering
  scenarios, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> includes OCFS2 (Oracle Cluster File System 2)
  and the Distributed Replicated Block Device (DRBD) in the High Availability Extension
  add-on. These advanced storage systems are not covered in this guide. For
  information, see the <em class="citetitle ">SUSE Linux Enterprise High Availability Extension
  <em class="citetitle ">Administration Guide</em></em> at <a class="link" href="http://www.suse.com/doc" target="_blank">http://www.suse.com/doc</a>.
 </p><p>
  With SUSE Linux Enterprise 12, Btrfs is the default file system for the operating
  system and XFS is the default for all other use cases. SUSE also
  continues to support the Ext family of file systems, ReiserFS and OCFS2.
  By default, the Btrfs file system will be set up with subvolumes.
  Snapshots will be automatically enabled for the root file system using the
  snapper infrastructure. For more information about snapper, refer to
  <span class="intraxref">Book “<em class="citetitle ">Administration Guide</em>”, Chapter 3 “System Recovery and Snapshot Management with Snapper”</span>.
 </p><div class="sect1 " id="sec.filesystems.glossary"><div class="titlepage"><div><div><h2 class="title" id="sec.filesystems.glossary"><span class="number">1.1 </span><span class="name">Terminology</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.glossary">#</a></h2></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="idm139647990709024"><span class="term ">metadata</span></dt><dd><p>
      A data structure that is internal to the file system. It ensures that
      all of the on-disk data is properly organized and accessible.
      Essentially, it is <span class="quote">“<span class="quote">data about the data.</span>”</span> Almost every
      file system has its own structure of metadata, which is one reason
      the file systems show different performance characteristics. It is
      extremely important to maintain metadata intact, because otherwise all
      data on the file system could become inaccessible.
     </p></dd><dt id="idm139647990706336"><span class="term ">inode</span></dt><dd><p>
      A data structure on a file system that contains a variety of
      information about a file, including size, number of links, pointers to
      the disk blocks where the file contents are actually stored, and date
      and time of creation, modification, and access.
     </p></dd><dt id="idm139647990704272"><span class="term ">journal</span></dt><dd><p>
      In the context of a file system, a journal is an on-disk structure
      containing a type of log in which the file system stores what it is
      about to change in the file system’s metadata. Journaling greatly
      reduces the recovery time of a file system because it has no need for
      the lengthy search process that checks the entire file system at
      system start-up. Instead, only the journal is replayed.
     </p></dd></dl></div></div><div class="sect1 " id="sec.filesystems.major"><div class="titlepage"><div><div><h2 class="title" id="sec.filesystems.major"><span class="number">1.2 </span><span class="name">Major File Systems in Linux</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major">#</a></h2></div></div></div><p>
   <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers a variety of file systems from which to choose.
   This section contains an overview of how these file systems work and
   which advantages they offer.
  </p><p>
   It is very important to remember that no file system best suits all kinds
   of applications. Each file system has its particular strengths and
   weaknesses, which must be taken into account. In addition, even the most
   sophisticated file system cannot replace a reasonable backup strategy.
  </p><p>
   The terms <span class="emphasis"><em>data integrity</em></span> and <span class="emphasis"><em>data
   consistency</em></span>, when used in this section, do not refer to the
   consistency of the user space data (the data your application writes to
   its files). Whether this data is consistent must be controlled by the
   application itself.
  </p><div id="idm139647989994224" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Default File Systems on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP1</span></span></h6><p>
    <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP1</span></span> is set up using Btrfs and snapshot support
    for the root partition by default. See <span class="intraxref">Book “<em class="citetitle ">Administration Guide</em>”, Chapter 3 “System Recovery and Snapshot Management with Snapper”</span> for
    details. Data partitions (such as /home residing on a separate partition)
    are formatted with XFS by default.
   </p></div><div id="idm139647989987872" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: The YaST Partitioner</h6><p>
    Unless stated otherwise in this section, all the steps required to set
    up or change partitions and file systems can be performed by using the
    YaST Partitioner (which is also strongly recommended). For
    information, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide</em>”, Chapter 14 “Advanced Disk Setup”</span>.
   </p></div><div class="sect2 " id="sec.filesystems.major.btrfs"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.btrfs"><span class="number">1.2.1 </span><span class="name">Btrfs</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs">#</a></h3></div></div></div><p>
    Btrfs is a copy-on-write (COW) file system developed by Chris Mason. It
    is based on COW-friendly B-trees developed by Ohad Rodeh. Btrfs is a
    logging-style file system. Instead of journaling the block changes, it
    writes them in a new location, then links the change in. Until the last
    write, the new changes are not committed.
   </p><div class="sect3 " id="sec.filesystems.major.btrfs.features"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.features"><span class="number">1.2.1.1 </span><span class="name">Key Features</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.features">#</a></h4></div></div></div><p>
     Btrfs provides fault tolerance, repair, and easy management features,
     such as the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Writable snapshots that allow you to easily roll back your system if
       needed after applying updates, or to back up files.
      </p></li><li class="listitem "><p>
       Subvolume support: Btrfs creates a default subvolume in its assigned
       pool of space. It allows you to create additional subvolumes that act
       as individual file systems within the same pool of space. The number
       of subvolumes is limited only by the space allocated to the pool.
      </p></li><li class="listitem "><p>
       The online check and repair functionality <code class="command">scrub</code> is
       available as part of the Btrfs command line tools. It verifies the
       integrity of data and metadata, assuming the tree structure is fine.
       You can run scrub periodically on a mounted file system; it runs as a
       background process during normal operation.
      </p></li><li class="listitem "><p>
       Different RAID levels for metadata and user data.
      </p></li><li class="listitem "><p>
       Different checksums for metadata and user data to improve error
       detection.
      </p></li><li class="listitem "><p>
       Integration with Linux Logical Volume Manager (LVM) storage objects.
      </p></li><li class="listitem "><p>
       Integration with the YaST Partitioner and AutoYaST on
       <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>. This also includes creating a Btrfs file system on
       Multiple Devices (MD) and Device Mapper (DM) storage configurations.
      </p></li><li class="listitem "><p>
       Offline migration from existing Ext2, Ext3, and Ext4 file systems.
      </p></li><li class="listitem "><p>
       Boot loader support for <code class="filename">/boot</code>, allowing to boot
       from a Btrfs partition.
      </p></li><li class="listitem "><p>
       Multivolume Btrfs is supported in RAID0, RAID1, and RAID10 profiles
       in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP1</span></span>. Higher RAID levels are not
       supported yet, but might be enabled with a future service pack.
      </p></li><li class="listitem "><p>
       
       Use Btrfs commands to set up transparent compression. 
      </p></li></ul></div></div><div class="sect3 " id="sec.filesystems.major.btrfs.suse"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.suse"><span class="number">1.2.1.2 </span><span class="name">The Root File System Setup on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.suse">#</a></h4></div></div></div><p>
     By default, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is set up using Btrfs and snapshots for
     the root partition. Snapshots allow you to easily roll back your system
     if needed after applying updates, or to back up files. Snapshots can
     easily be managed with the SUSE Snapper infrastructure as
     explained in <span class="intraxref">Book “<em class="citetitle ">Administration Guide</em>”, Chapter 3 “System Recovery and Snapshot Management with Snapper”</span>. For general information
     about the SUSE Snapper project, see the Snapper Portal wiki at
     OpenSUSE.org (<a class="link" href="http://snapper.io" target="_blank">http://snapper.io</a>).
    </p><p>
     When using a snapshot to roll back the system, it must be ensured that
     data such as user's home directories, Web and FTP server contents or
     log files do not get lost or overwritten during a roll back. This is
     achieved by using Btrfs subvolumes on the root file system. Subvolumes
     can be excluded from snapshots. The default root file system setup on
     <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> as proposed by YaST during the installation
     contains the following subvolumes. They are excluded from snapshots for
     the reasons given below.
    </p><p>
     
    </p><div class="variablelist " id="vl.filesystems.major.btrfs.subvolumes"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Default Subvolume Setup for the Root Partition </span><a title="Permalink" class="permalink" href="cha.filesystems.html#vl.filesystems.major.btrfs.subvolumes">#</a></h6></div><dl class="variablelist"><dt id="idm139647991304080"><span class="term "><code class="filename">/boot/grub2/i386-pc,</code><code class="filename">/boot/grub2/x86_64-efi, </code><code class="filename">/boot/grub2/powerpc-ieee1275, </code><code class="filename">/boot/grub2/s390x-emu</code>
      </span></dt><dd><p>
        A rollback of the boot loader configuration is not supported. The
        directories listed above are architecture-specific. The first two
        directories are present on x86_64 machines, the latter two on IBM
        POWER and on IBM z Systems, respectively.
       </p></dd><dt id="idm139647991299792"><span class="term "><code class="filename">/home</code>
      </span></dt><dd><p>
        If <code class="filename">/home</code> does not reside on a separate
        partition, it is excluded to avoid data loss on rollbacks.
       </p></dd><dt id="idm139647991297168"><span class="term "><code class="filename">/opt</code>, <code class="filename">/var/opt</code>
      </span></dt><dd><p>
        Third-party products and add-ons usually get installed to
        <code class="filename">/opt</code>. It is excluded to avoid uninstalling
        these applications on rollbacks.
       </p></dd><dt id="idm139647991294256"><span class="term "><code class="filename">/srv</code>
      </span></dt><dd><p>
        Contains data for Web and FTP servers. It is excluded to avoid data
        loss on rollbacks.
       </p></dd><dt id="idm139647991292080"><span class="term "><code class="filename">/tmp</code>, <code class="filename">/var/tmp</code>,
       <code class="filename">/var/crash</code>
      </span></dt><dd><p>
        All directories containing temporary files are excluded from
        snapshots.
       </p></dd><dt id="idm139647991289024"><span class="term "><code class="filename">/usr/local</code>
      </span></dt><dd><p>
        This directory is used when manually installing software. It is
        excluded to avoid uninstalling these installations on rollbacks.
       </p></dd><dt id="idm139647991286800"><span class="term "><code class="filename">/var/lib/libvirt/images</code>
      </span></dt><dd><p>Default directory for all VM images created via libvirt. Excluded
        from snapshots. By default, this subvolume is created with the option
         <code class="literal">no copy on write</code>.</p></dd><dt id="idm139647991284128"><span class="term "><code class="filename">/var/lib/named</code>
      </span></dt><dd><p>
        Contains zone data for the DNS server. Excluded from snapshots to
        ensure a name server can operate after a rollback.
       </p></dd><dt id="idm139647990449328"><span class="term "><code class="filename">/var/lib/mailman</code>, <code class="filename">/var/spool</code>
      </span></dt><dd><p>
        Directories containing mail queues or mail are excluded to avoid a
        loss of mail after a rollback.
       </p></dd><dt id="idm139647990446688"><span class="term "><code class="filename">/var/lib/mariadb</code>
      </span></dt><dd><p>
        For the MariaDB data. Excluded from snapshots. By default, this
        subvolume is created with the option <code class="literal">no copy on
        write</code>.
       </p></dd><dt id="idm139647990444048"><span class="term "><code class="filename">/var/lib/pgsql</code>
      </span></dt><dd><p>
        Contains PostgreSQL data. Excluded from snapshots. By default,
        this subvolume is created with the option <code class="literal">no copy on
        write</code>.
       </p></dd><dt id="idm139647990441392"><span class="term "><code class="filename">/var/log</code>
      </span></dt><dd><p>
        Log file location. Excluded from snapshots to allow log file
        analysis after the rollback of a broken system.
       </p></dd></dl></div><div id="idm139647990439040" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Support for Rollbacks</h6><p>
      Rollbacks are only supported by the SUSE support if you do not
      remove any of the preconfigured subvolumes. You may, however, add
      additional subvolumes using the YaST Partitioner.
     </p></div><div class="sect4 " id="sec.filesystems.major.btrfs.compress"><div class="titlepage"><div><div><h5 class="title" id="sec.filesystems.major.btrfs.compress"><span class="number">1.2.1.2.1 </span><span class="name">Mounting Compressed Btrfs File Systems</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.compress">#</a></h5></div></div></div><div id="idm139647990435936" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: GRUB 2 and LZO Compressed Root</h6><p>GRUB 2 cannot read an lzo compressed root. You need a separate
        <code class="filename">/boot</code> partition if you want to use
        compression.</p></div><p>Since SLE12 SP1, compression for Btrfs file systems is
      supported.  Use the <code class="option">compress</code> or
      <code class="option">compress-force</code> option and select the compression
      algorithm, <code class="literal">lzo</code> or <code class="literal">zlib</code> (the
      default). The zlib compression has a higher compression ratio while lzo
      is faster and takes less CPU load.
      </p><p>
        For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -o compress /dev/sdx /mnt</pre></div><p>
       In case you create a file, write to it, and the compressed result is
       greater or equal to the uncompressed size, Btrfs will skip compression
       for future write operations forever for this file.  If you do not like
       this behavior, use the <code class="option">compress-force</code> option. This can
       be useful for files that have some initial uncompressible data.
      </p><p>
       Note, compression takes effect for new files only. Files that were
       written without compression are not compressed when the file system is
       mounted with the <code class="option">compress</code> or
       <code class="option">compress-force</code> option.  Furthermore, files with the
       <code class="option">nodatacow</code> attribute never get their extents
       compressed:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">chattr</code> +C <em class="replaceable ">FILE</em>
<code class="prompt user">root # </code><code class="command">mount</code> -o nodatacow  /dev/sdx /mnt</pre></div><p>
       In regard to encryption, this is independent from any compression.
       After you have written some data to this partition, print the details:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>btrfs filesystem show /mnt
btrfs filesystem show /mnt
Label: 'Test-Btrfs'  uuid: 62f0c378-e93e-4aa1-9532-93c6b780749d
        Total devices 1 FS bytes used 3.22MiB
      devid    1 size 2.00GiB used 240.62MiB path /dev/sdb1</pre></div><p>
       If you want this to be permanent, add the <code class="option">compress</code> or
       <code class="option">compress-force</code> option into the
       <code class="filename">/etc/fstab</code> configuration file. For example:
      </p><div class="verbatim-wrap"><pre class="screen">UUID=1a2b3c4d /home btrfs subvol=@/home,<span class="strong"><strong>compress</strong></span> 0 0</pre></div></div><div class="sect4 " id="sec.filesystems.major.btrfs.suse.mount"><div class="titlepage"><div><div><h5 class="title" id="sec.filesystems.major.btrfs.suse.mount"><span class="number">1.2.1.2.2 </span><span class="name">Mounting Subvolumes</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.suse.mount">#</a></h5></div></div></div><p>
      A system rollback from a snapshot on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is performed by
      booting from the snapshot first. This allows you to check the snapshot
      while running before doing the rollback. Being able to boot from
      snapshots is achieved by mounting the subvolumes (which would normally
      not be necessary).
     </p><p>
      In addition to the subvolumes listed at
      <a class="xref" href="cha.filesystems.html#vl.filesystems.major.btrfs.subvolumes" title="Default Subvolume Setup for the Root Partition">Default Subvolume Setup for the Root Partition</a> a volume named
      <code class="literal">@</code> exists. This is the default subvolume that will
      be mounted as the root partition (<code class="filename">/</code>). The other
      subvolumes will be mounted into this volume.
     </p><p>
      When booting from a snapshot, not the <code class="literal">@</code> subvolume
      will be used, but rather the snapshot. The parts of the file system
      included in the snapshot will be mounted read-only as
      <code class="filename">/</code>. The other subvolumes will be mounted writable
      into the snapshot. This state is temporary by default: the previous
      configuration will be restored with the next reboot. To make it
      permanent, execute the <code class="command">snapper rollback</code> command.
      This will make the snapshot that is currently booted the new
      <span class="emphasis"><em>default</em></span> subvolume, which will be used after a
      reboot.
     </p></div></div><div class="sect3 " id="sec.filesystems.major.btrfs.migrate"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.migrate"><span class="number">1.2.1.3 </span><span class="name">Migration from Ext and ReiserFS File Systems to Btrfs</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.migrate">#</a></h4></div></div></div><p>
     You can migrate data volumes from existing Ext (Ext2, Ext3, or Ext4) or
     ReiserFS to the Btrfs file system. The conversion process occurs
     offline and in place on the device. The file system needs at least 15%
     of available free space on the device.
    </p><p>
     To convert the file system to Btrfs, take the file system offline, then
     enter:
    </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs-convert &lt;<em class="replaceable ">device</em>&gt;</pre></div><p>
     To roll back the migration to the original file system, take the file
     system offline, then enter:
    </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs-convert -r &lt;<em class="replaceable ">device</em>&gt;</pre></div><div id="idm139647990676400" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Possible Loss of Data</h6><p>
      When rolling back to the original file system, all data will be lost
      that you added after the conversion to Btrfs. That is, only the
      original data is converted back to the previous file system.
     </p></div></div><div class="sect3 " id="sec.filesystems.major.btrfs.admin"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.admin"><span class="number">1.2.1.4 </span><span class="name">Btrfs Administration</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.admin">#</a></h4></div></div></div><p>
     Btrfs is integrated in the YaST Partitioner and AutoYaST. It is
     available during the installation to allow you to set up a solution for
     the root file system. You can use the YaST Partitioner after the
     installation to view and manage Btrfs volumes.
    </p><p>
     Btrfs administration tools are provided in the
     <code class="filename">btrfsprogs</code> package. For information about using
     Btrfs commands, see the <code class="command">man 8 btrfs</code>, <code class="command">man 8
     btrfsck</code>, and <code class="command">man 8 mkfs.btrfs</code> commands.
     For information about Btrfs features, see the <em class="citetitle ">Btrfs
     wiki</em> at <a class="link" href="http://btrfs.wiki.kernel.org" target="_blank">http://btrfs.wiki.kernel.org</a>.
    </p></div><div class="sect3 " id="sec.filesystems.major.btrfs.quota"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.quota"><span class="number">1.2.1.5 </span><span class="name">Btrfs Quota Support for Subvolumes</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.quota">#</a></h4></div></div></div><p>
     The Btrfs root file system subvolumes <code class="filename">/var/log</code>,
     <code class="filename">/var/crash</code> and <code class="filename">/var/cache</code> can
     use all of the available disk space during normal operation, and cause
     a system malfunction. To help avoid this situation, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>
     now offers Btrfs quota support for subvolumes. If you set up the root
     file system by using the respective YaST proposal, it is prepared
     accordingly: quota groups (<code class="literal">qgroup</code>) for all
     subvolumes are already set up. To set a quota for a subvolume in the
     root file system, proceed as follows:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Enable quota support:
      </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs quota enable /</pre></div></li><li class="step "><p>
       Get a list of subvolumes:
      </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs subvolume list /</pre></div><p>
       Quotas can only be set for existing subvolumes.
      </p></li><li class="step "><p>
       Set a quota for one of the subvolumes that was listed in the previous
       step. A subvolume can either be identified by path (for example
       <code class="filename">/var/tmp</code>) or by
       <code class="literal">0/<em class="replaceable ">subvolume id</em></code> (for
       example <code class="literal">0/272</code>). The following example sets a quota
       of five GB for <code class="filename">/var/tmp</code>.
      </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs qgroup limit 5G /var/tmp</pre></div><p>
       The size can either be specified in bytes (5000000000), kilobytes
       (5000000K), megabytes (5000M), or gigabytes (5G). The resulting
       values in bytes slightly differ, since 1024 Bytes = 1 KiB, 1024 KiB =
       1 MiB, etc.
      </p></li><li class="step "><p>
       To list the existing quotas, use the following command. The column
       <code class="literal">max_rfer</code> shows the quota in bytes.
      </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs qgroup show -r /</pre></div></li></ol></div></div><div id="idm139647990114768" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Nullifying a Quota</h6><p>
      In case you want to nullify an existing quota, set a quota size of
      <code class="literal">0</code>:
     </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs qgroup limit 0 /var/tmp</pre></div><p>
      To disable quota support for a partition and all its subvolumes, use
      <code class="command">btrfs quota disable</code>:
     </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs quota disable /</pre></div></div><p>
     See the <code class="command">man 8 btrfs-qgroup</code> and <code class="command">man 8
     btrfs-quota</code> for more details. The
     <em class="citetitle ">UseCases</em> page on the Btrfs wiki
     (<a class="link" href="https://btrfs.wiki.kernel.org/index.php/UseCases" target="_blank">https://btrfs.wiki.kernel.org/index.php/UseCases</a>)
     also provides more information.
    </p></div><div class="sect3 " id="sec.filesystems.major.btrfs.deduplication"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.btrfs.deduplication"><span class="number">1.2.1.6 </span><span class="name">Data Deduplication Support</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.btrfs.deduplication">#</a></h4></div></div></div><p>
     Btrfs supports data deduplication by replacing identical blocks in the
     file system with logical links to a single copy of the block in a
     common storage location. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> provides the tool
     <code class="command">duperemove</code> for scanning the file system for
     identical blocks. When used on a Btrfs file system, it can also be used
     to deduplicate these blocks. duperemove is not installed by default. To
     make it available, install the package
     <code class="systemitem">duperemove</code>.
    </p><div id="idm139647990104144" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Use Cases</h6><p>
      As of <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> <span class="productnumber"><span class="phrase">12 SP1</span></span> duperemove is not suited to
      deduplicate the entire file system. It is intended to be used to
      deduplicate a set of 10 to 50 large files that possibly have lots of
      blocks in common, such as virtual machine images.
     </p></div><p>
     <code class="command">duperemove</code> can either operate on a list of files or
     recursively scan a directory:
    </p><div class="verbatim-wrap"><pre class="screen">sudo duperemove <em class="replaceable ">[options]</em> file1 file2 file3
sudo duperemove -r <em class="replaceable ">[options]</em> directory</pre></div><p>
     It operates in two modes: read-only and de-duping. When run in
     read-only mode (that is without the <code class="option">-d</code> switch), it
     scans the given files or directories for duplicated blocks and prints
     them out. This works on any file system.
    </p><p>
     Running <code class="command">duperemove</code> in de-duping mode is only
     supported on Btrfs file systems. After having scanned the given files
     or directories, the duplicated blocks will be submitted for
     deduplication.
    </p><p>
     For more information see <code class="command">man 8 duperemove</code>.
    </p></div></div><div class="sect2 " id="sec.filesystems.major.xfs"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.xfs"><span class="number">1.2.2 </span><span class="name">XFS</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.xfs">#</a></h3></div></div></div><p>
    Originally intended as the file system for their IRIX OS, SGI started
    XFS development in the early 1990s. The idea behind XFS was to create a
    high-performance 64-bit journaling file system to meet extreme computing
    challenges. XFS is very good at manipulating large files and performs
    well on high-end hardware.

    XFS is the default file system for data partitions in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
   </p><p>
    A quick review of XFS’s key features explains why it might prove to be
    a strong competitor for other journaling file systems in high-end
    computing.
   </p><div class="sect3 " id="sec.filesystems.major.xfs.scalability"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.xfs.scalability"><span class="number">1.2.2.1 </span><span class="name">High Scalability by Using Allocation Groups</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.xfs.scalability">#</a></h4></div></div></div><p>
     At the creation time of an XFS file system, the block device underlying
     the file system is divided into eight or more linear regions of equal
     size. Those are called <span class="emphasis"><em>allocation groups</em></span>.
     Each allocation group manages its own inodes and free disk space.
     Practically, allocation groups can be seen as file systems in a file
     system. Because allocation groups are rather independent of each other,
     more than one of them can be addressed by the kernel simultaneously.
     This feature is the key to XFS’s great scalability. Naturally, the
     concept of independent allocation groups suits the needs of
     multiprocessor systems.
    </p></div><div class="sect3 " id="sec.filesystems.major.xfs.mgmt"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.xfs.mgmt"><span class="number">1.2.2.2 </span><span class="name">High Performance through Efficient Management of Disk Space</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.xfs.mgmt">#</a></h4></div></div></div><p>
     Free space and inodes are handled by B<sup>+</sup>
     trees inside the allocation groups. The use of
     B<sup>+</sup> trees greatly contributes to XFS’s
     performance and scalability. XFS uses <span class="emphasis"><em>delayed
     allocation</em></span>, which handles allocation by breaking the process
     into two pieces. A pending transaction is stored in RAM and the
     appropriate amount of space is reserved. XFS still does not decide
     where exactly (in file system blocks) the data should be stored. This
     decision is delayed until the last possible moment. Some short-lived
     temporary data might never make its way to disk, because it is obsolete
     by the time XFS decides where actually to save it. In this way, XFS
     increases write performance and reduces file system fragmentation.
     Because delayed allocation results in less frequent write events than
     in other file systems, it is likely that data loss after a crash during
     a write is more severe.
    </p></div><div class="sect3 " id="sec.filesystems.major.prealloc"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.prealloc"><span class="number">1.2.2.3 </span><span class="name">Preallocation to Avoid File System Fragmentation</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.prealloc">#</a></h4></div></div></div><p>
     Before writing the data to the file system, XFS
     <span class="emphasis"><em>reserves</em></span> (preallocates) the free space needed for
     a file. Thus, file system fragmentation is greatly reduced. Performance
     is increased because the contents of a file are not distributed all
     over the file system.
    </p><div id="idm139647990296976" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: The new XFS On-disk Format</h6><p>
      Starting with version 12, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> supports the new
      <span class="quote">“<span class="quote">on-disk format</span>”</span> (v5) of the XFS file system. XFS file
      systems created by YaST will use this new format. The main
      advantages of this format are automatic checksums of all XFS metadata,
      file type support, and support for a larger number of access control
      lists for a file.
     </p><p>
      Note that this format is <span class="emphasis"><em>not</em></span> supported by
      SUSE Linux Enterprise kernels older than version 3.12, by xfsprogs older than
      version 3.2.0, and GRUB 2 versions released before SUSE Linux Enterprise 12.
      This will be problematic if the file system should also be used from
      systems not meeting these prerequisites.
     </p><p>
      If you require interoperability of the XFS file system with older
      SUSE systems or other Linux distributions, format the file system
      manually using the <code class="command">mkfs.xfs</code> command. This will
      create an XFS file system in the old format (unless you use the
      <code class="option">-m crc=1</code> option).
     </p></div></div></div><div class="sect2 " id="sec.filesystems.major.ext2"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.ext2"><span class="number">1.2.3 </span><span class="name">Ext2</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext2">#</a></h3></div></div></div><p>
    The origins of Ext2 go back to the early days of Linux history. Its
    predecessor, the Extended File System, was implemented in April 1992 and
    integrated in Linux 0.96c. The Extended File System underwent a number
    of modifications and, as Ext2, became the most popular Linux file system
    for years. With the creation of journaling file systems and their short
    recovery times, Ext2 became less important.
   </p><p>
    A brief summary of Ext2’s strengths might help understand why it
    was—and in some areas still is—the favorite Linux file
    system of many Linux users.
   </p><div class="sect3 " id="sec.filesystems.major.ext2.speed"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext2.speed"><span class="number">1.2.3.1 </span><span class="name">Solidity and Speed</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext2.speed">#</a></h4></div></div></div><p>
     Being quite an <span class="quote">“<span class="quote">old-timer,</span>”</span> Ext2 underwent many
     improvements and was heavily tested. This might be the reason
     people often refer to it as rock-solid. After a system outage when the
     file system could not be cleanly unmounted, e2fsck starts to analyze
     the file system data. Metadata is brought into a consistent state and
     pending files or data blocks are written to a designated directory
     (called <code class="filename">lost+found</code>). In contrast to journaling
     file systems, e2fsck analyzes the entire file system and not only the
     recently modified bits of metadata. This takes significantly longer
     than checking the log data of a journaling file system. Depending on
     file system size, this procedure can take half an hour or more.
     Therefore, it is not desirable to choose Ext2 for any server that needs
     high availability. However, because Ext2 does not maintain a journal
     and uses significantly less memory, it is sometimes faster than other
     file systems.
    </p></div><div class="sect3 " id="sec.filesystems.major.ext2.upgrade"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext2.upgrade"><span class="number">1.2.3.2 </span><span class="name">Easy Upgradability</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext2.upgrade">#</a></h4></div></div></div><p>
     Because Ext3 is based on the Ext2 code and shares its on-disk format and
     its metadata format, upgrades from Ext2 to Ext3 are very easy.
    </p></div></div><div class="sect2 " id="sec.filesystems.major.ext3"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.ext3"><span class="number">1.2.4 </span><span class="name">Ext3</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext3">#</a></h3></div></div></div><p>
    Ext3 was designed by Stephen Tweedie. Unlike all other next-generation
    file systems, Ext3 does not follow a completely new design principle. It
    is based on Ext2. These two file systems are very closely related to
    each other. An Ext3 file system can be easily built on top of an Ext2
    file system. The most important difference between Ext2 and Ext3 is that
    Ext3 supports journaling. In summary, Ext3 has three major advantages to
    offer:
   </p><div class="sect3 " id="sec.filesystems.major.ext3.upgrade"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext3.upgrade"><span class="number">1.2.4.1 </span><span class="name">Easy and Highly Reliable Upgrades from Ext2</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext3.upgrade">#</a></h4></div></div></div><p>
     The code for Ext2 is the strong foundation on which Ext3 could become a
     highly acclaimed next-generation file system. Its reliability and
     solidity are elegantly combined in Ext3 with the advantages of a
     journaling file system. Unlike transitions to other journaling file
     systems, such as ReiserFS or XFS, which can be quite tedious (making
     backups of the entire file system and re-creating it from scratch), a
     transition to Ext3 is a matter of minutes. It is also very safe,
     because re-creating an entire file system from scratch might not work
     flawlessly. Considering the number of existing Ext2 systems that await
     an upgrade to a journaling file system, you can easily see why Ext3
     might be of some importance to many system administrators. Downgrading
     from Ext3 to Ext2 is as easy as the upgrade. Perform a clean unmount of
     the Ext3 file system and remount it as an Ext2 file system.
    </p></div><div class="sect3 " id="sec.filesystems.major.ext3.performance"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext3.performance"><span class="number">1.2.4.2 </span><span class="name">Reliability and Performance</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext3.performance">#</a></h4></div></div></div><p>
     Some other journaling file systems follow the
     <span class="quote">“<span class="quote">metadata-only</span>”</span> journaling approach. This means your
     metadata is always kept in a consistent state, but this cannot be
     automatically guaranteed for the file system data itself. Ext3 is
     designed to take care of both metadata and data. The degree of
     <span class="quote">“<span class="quote">care</span>”</span> can be customized. Enabling Ext3 in the
     <code class="option">data=journal</code> mode offers maximum security (data
     integrity), but can slow down the system because both metadata and data
     are journaled. A relatively new approach is to use the
     <code class="option">data=ordered</code> mode, which ensures both data and
     metadata integrity, but uses journaling only for metadata. The file
     system driver collects all data blocks that correspond to one metadata
     update. These data blocks are written to disk before the metadata is
     updated. As a result, consistency is achieved for metadata and data
     without sacrificing performance. A third option to use is
     <code class="option">data=writeback</code>, which allows data to be written to the
     main file system after its metadata has been committed to the journal.
     This option is often considered the best in performance. It can,
     however, allow old data to reappear in files after crash and recovery
     while internal file system integrity is maintained. Ext3 uses the
     <code class="option">data=ordered</code> option as the default.
    </p></div><div class="sect3 " id="sec.filesystems.major.ext3.ext22ext3a"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext3.ext22ext3a"><span class="number">1.2.4.3 </span><span class="name">Converting an Ext2 File System into Ext3</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext3.ext22ext3a">#</a></h4></div></div></div><p>
     To convert an Ext2 file system to Ext3:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Create an Ext3 journal by running <code class="command">tune2fs -j</code> as
       the <code class="systemitem">root</code> user.
      </p><p>
       This creates an Ext3 journal with the default parameters.
      </p><p>
       To specify how large the journal should be and on which device it
       should reside, run <code class="command">tune2fs</code> <code class="option">-J</code>
       instead together with the desired journal options
       <code class="option">size=</code> and <code class="option">device=</code>. More information
       about the <code class="command">tune2fs</code> program is available in the
       <code class="command">tune2fs</code> man page.
      </p></li><li class="step "><p>
       Edit the file <code class="filename">/etc/fstab</code> as the <code class="systemitem">root</code>
       user to change the file system type specified for the corresponding
       partition from <code class="literal">ext2</code> to <code class="literal">ext3</code>,
       then save the changes.
      </p><p>
       This ensures that the Ext3 file system is recognized as such. The
       change takes effect after the next reboot.
      </p></li><li class="step "><p>
       To boot a root file system that is set up as an Ext3 partition, add
       the modules <code class="literal">ext3</code> and <code class="literal">jbd</code> in the
       <code class="filename">initrd</code>. Do so by
      </p><ol type="a" class="substeps "><li class="step "><p>
         adding the following line to
         <code class="filename">/etc/dracut.conf.d/01-dist.conf</code>:
        </p><div class="verbatim-wrap"><pre class="screen">force_drivers+="ext3 jbd"</pre></div></li><li class="step "><p>
         and running the <code class="command">dracut</code> <code class="option">-f</code>
         command.
        </p></li></ol></li><li class="step "><p>
       Reboot the system.
      </p></li></ol></div></div></div><div class="sect3 " id="sec.filesystems.major.ext3.inodesize"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.ext3.inodesize"><span class="number">1.2.4.4 </span><span class="name">Ext3 File System Inode Size and Number of Inodes</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext3.inodesize">#</a></h4></div></div></div><p>
     An inode stores information about the file and its block location in
     the file system. To allow space in the inode for extended attributes
     and ACLs, the default inode size for Ext3 was increased from 128 bytes
     on SLES 10 to 256 bytes on SLES 11. As compared to SLES 10, when you
     make a new Ext3 file system on SLES 11, the default amount of space
     preallocated for the same number of inodes is doubled, and the usable
     space for files in the file system is reduced by that amount. Thus, you
     must use larger partitions to accommodate the same number of inodes and
     files than were possible for an Ext3 file system on SLES 10.
    </p><p>
     When you create a new Ext3 file system, the space in the inode table is
     preallocated for the total number of inodes that can be created. The
     bytes-per-inode ratio and the size of the file system determine how
     many inodes are possible. When the file system is made, an inode is
     created for every bytes-per-inode bytes of space:
    </p><div class="verbatim-wrap"><pre class="screen">number of inodes = total size of the file system divided by the number of bytes per inode</pre></div><p>
     The number of inodes controls the number of files you can have in the
     file system: one inode for each file. To address the increased inode
     size and reduced usable space available, the default for the
     bytes-per-inode ratio was increased from 8192 bytes on SLES 10 to 16384
     bytes on SLES 11. The doubled ratio means that the number of files that
     can be created is one-half of the number of files possible for an Ext3
     file system on SLES 10.
    </p><div id="idm139647990006096" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Changing the Inode Size of an Existing Ext3 File System</h6><p>
      After the inodes are allocated, you cannot change the settings for the
      inode size or bytes-per-inode ratio. No new inodes are possible
      without re-creating the file system with different settings, or unless
      the file system gets extended. When you exceed the maximum number of
      inodes, no new files can be created on the file system until some
      files are deleted.
     </p></div><p>
     When you make a new Ext3 file system, you can specify the inode size
     and bytes-per-inode ratio to control inode space usage and the number
     of files possible on the file system. If the blocks size, inode size,
     and bytes-per-inode ratio values are not specified, the default values
     in the <code class="filename">/etc/mked2fs.conf</code> file are applied. For
     information, see the <code class="filename">mke2fs.conf(5)</code> man page.
    </p><p>
     Use the following guidelines:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Inode size: </span>
        The default inode size is 256 bytes. Specify a value in bytes that
        is a power of 2 and equal to 128 or larger in bytes and up to the
        block size, such as 128, 256, 512, and so on. Use 128 bytes only if
        you do not use extended attributes or ACLs on your Ext3 file
        systems.
       </p></li><li class="listitem "><p><span class="formalpara-title">Bytes-per-inode ratio: </span>
        The default bytes-per-inode ratio is 16384 bytes. Valid
        bytes-per-inode ratio values must be a power of 2 equal to 1024 or
        greater in bytes, such as 1024, 2048, 4096, 8192, 16384, 32768, and
        so on. This value should not be smaller than the block size of the
        file system, because the block size is the smallest chunk of space
        used to store data. The default block size for the Ext3 file system
        is 4 KB.
       </p><p>
       In addition, you should consider the number of files and the size of
       files you need to store. For example, if your file system will have
       many small files, you can specify a smaller bytes-per-inode ratio,
       which increases the number of inodes. If your file system will have
       very large files, you can specify a larger bytes-per-inode ratio,
       which reduces the number of possible inodes.
      </p><p>
       Generally, it is better to have too many inodes than to run out of
       them. If you have too few inodes and very small files, you could
       reach the maximum number of files on a disk that is practically
       empty. If you have too many inodes and very large files, you might
       have free space reported but be unable to use it because you cannot
       create new files in space reserved for inodes.
      </p></li></ul></div><p>
     If you do not use extended attributes or ACLs on your Ext3 file
     systems, you can restore the SLES 10 behavior specifying 128 bytes as
     the inode size and 8192 bytes as the bytes-per-inode ratio when you
     make the file system. Use any of the following methods to set the inode
     size and bytes-per-inode ratio:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Modifying the default settings for all new Ext3 files: </span>
        In a text editor, modify the <code class="literal">defaults</code> section of
        the <code class="filename">/etc/mke2fs.conf</code> file to set the
        <code class="literal">inode_size</code> and <code class="literal">inode_ratio</code> to
        the desired default values. The values apply to all new Ext3 file
        systems. For example:
       </p><div class="verbatim-wrap"><pre class="screen">blocksize = 4096
inode_size = 128
inode_ratio = 8192</pre></div></li><li class="listitem "><p><span class="formalpara-title">At the command line: </span>
        Pass the inode size (<code class="literal">-I 128</code>) and the
        bytes-per-inode ratio (<code class="literal">-i 8192</code>) to the
        <code class="command">mkfs.ext3(8)</code> command or the
        <code class="command">mke2fs(8)</code> command when you create a new Ext3 file
        system. For example, use either of the following commands:
       </p><div class="verbatim-wrap"><pre class="screen">sudo mkfs.ext3 -b 4096 -i 8092 -I 128 /dev/sda2
sudo mke2fs -t ext3 -b 4096 -i 8192 -I 128 /dev/sda2</pre></div></li><li class="listitem "><p><span class="formalpara-title">During installation with YaST: </span>
        Pass the inode size and bytes-per-inode ratio values when you create
        a new Ext3 file system during the installation. In the YaST
        Partitioner on the <span class="guimenu">Edit Partition</span> page
        under<span class="guimenu"> Formatting Options</span>, select <span class="guimenu">Format
        partition</span><span class="guimenu">Ext3</span>, then click
        <span class="guimenu">Options</span>. In the <span class="guimenu">File system
        options</span> dialog, select the desired values from the
        <span class="guimenu">Block Size in Bytes</span>,
        <span class="guimenu">Bytes-per-inode</span>, and <span class="guimenu">Inode
        Size</span> drop-down box.
       </p><p>
       For example, select 4096 for the <span class="guimenu">Block Size in
       Bytes</span> drop-down box, select 8192 from the <span class="guimenu">Bytes
       per inode</span> drop-down box, select 128 from the
       <span class="guimenu">Inode Size</span> drop-down box, then click
       <span class="guimenu">OK</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/ext3_inode_yast_a.png"><img src="images/ext3_inode_yast_a.png" width="" /></a></div></div></li><li class="listitem "><p><span class="formalpara-title">During installation with AutoYaST: </span>
        In an AutoYaST profile, you can use the <code class="literal">fs_options
        </code>tag to set the <code class="literal">opt_bytes_per_inode</code>
        ratio value of 8192 for -i and the
        <code class="literal">opt_inode_density</code> value of 128 for -I:
       </p><div class="verbatim-wrap"><pre class="screen">&lt;partitioning config:type="list"&gt;
  &lt;drive&gt;
    &lt;device&gt;/dev/sda&lt;/device&gt;
    &lt;initialize config:type="boolean"&gt;true&lt;/initialize&gt;
    &lt;partitions config:type="list"&gt;
      &lt;partition&gt;
        &lt;filesystem config:type="symbol"&gt;ext3&lt;/filesystem&gt;
        &lt;format config:type="boolean"&gt;true&lt;/format&gt;
        &lt;fs_options&gt;
          &lt;opt_bytes_per_inode&gt;
            &lt;option_str&gt;-i&lt;/option_str&gt;
            &lt;option_value&gt;8192&lt;/option_value&gt;
          &lt;/opt_bytes_per_inode&gt;
          &lt;opt_inode_density&gt;
            &lt;option_str&gt;-I&lt;/option_str&gt;
            &lt;option_value&gt;128&lt;/option_value&gt;
          &lt;/opt_inode_density&gt;
        &lt;/fs_options&gt;
        &lt;mount&gt;/&lt;/mount&gt;
        &lt;partition_id config:type="integer"&gt;131&lt;/partition_id&gt;
        &lt;partition_type&gt;primary&lt;/partition_type&gt;
        &lt;size&gt;25G&lt;/size&gt;
      &lt;/partition&gt;
    &lt;/partitions&gt;
  &lt;/drive&gt;
&lt;partitioning&gt;</pre></div></li></ul></div><p>
     For information, see
     <a class="link" href="http://www.suse.com/support/kb/doc.php?id=7009075" target="_blank">http://www.suse.com/support/kb/doc.php?id=7009075</a>
     (<em class="citetitle ">SLES11 ext3 partitions can only store 50% of the files that
     can be stored on SLES10</em> [Technical Information Document
     7009075]).
    </p></div></div><div class="sect2 " id="sec.filesystems.major.ext4"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.ext4"><span class="number">1.2.5 </span><span class="name">Ext4</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.ext4">#</a></h3></div></div></div><p>
    In 2006, Ext4 started as a fork from Ext3. It eliminates some storage
    limitations of Ext3 by supporting volumes with a size of up to 1
    exbibyte, files with a size of up to 16 tebibytes and an unlimited
    number of subdirectories. It also introduces several performance
    enhancements such as delayed block allocation and a much faster file
    system checking routine. Ext4 is also more reliable by supporting
    journal checksums and by providing time stamps measured in nanoseconds.
    Ext4 is fully backwards compatible to Ext2 and Ext3—both file
    systems can be mounted as Ext4.
   </p></div><div class="sect2 " id="sec.filesystems.major.reiser"><div class="titlepage"><div><div><h3 class="title" id="sec.filesystems.major.reiser"><span class="number">1.2.6 </span><span class="name">ReiserFS</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.reiser">#</a></h3></div></div></div><p>
    Officially one of the key features of the 2.4 kernel release, ReiserFS
    has been available as a kernel patch for 2.2.x SUSE kernels since
    version 6.4. ReiserFS was designed by Hans Reiser and the Namesys
    development team. It has proven itself to be a powerful alternative to
    Ext2. Its key assets are better disk space usage, better disk
    access performance, faster crash recovery, and reliability through data
    journaling.
   </p><div id="idm139647990540096" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Support of ReiserFS in <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12</h6><p>
     Existing ReiserFS partitions are supported for the lifetime of
     <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12 specifically for migration purposes. Support for
     creating new ReiserFS file systems has been removed starting with
     <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> 12.
    </p></div><div class="sect3 " id="sec.filesystems.major.reiser.diskspace"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.reiser.diskspace"><span class="number">1.2.6.1 </span><span class="name">Better Disk Space Usage</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.reiser.diskspace">#</a></h4></div></div></div><p>
     In ReiserFS, all data is organized in a structure called a B*-balanced
     tree. The tree structure contributes to better disk space usage
     because small files can be stored directly in the B* tree leaf nodes
     instead of being stored elsewhere and maintaining a pointer to the
     actual disk location. In addition to that, storage is not allocated in
     chunks of 1 or 4 KB, but in portions of the exact size needed. Another
     benefit lies in the dynamic allocation of inodes. This keeps the file
     system more flexible than traditional file systems, like Ext2, where
     the inode density must be specified at file system creation time.
    </p></div><div class="sect3 " id="sec.filesystems.major.reiser.access"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.reiser.access"><span class="number">1.2.6.2 </span><span class="name">Better Disk Access Performance</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.reiser.access">#</a></h4></div></div></div><p>
     For small files, file data and <span class="quote">“<span class="quote">stat_data</span>”</span> (inode)
     information are often stored next to each other. They can be read with
     a single disk I/O operation, meaning that only one access to disk is
     required to retrieve all the information needed.
    </p></div><div class="sect3 " id="sec.filesystems.major.reiser.recovery"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.reiser.recovery"><span class="number">1.2.6.3 </span><span class="name">Fast Crash Recovery</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.reiser.recovery">#</a></h4></div></div></div><p>
     Using a journal to keep track of recent metadata changes makes a file
     system check a matter of seconds, even for huge file systems.
    </p></div><div class="sect3 " id="sec.filesystems.major.reiser.reliability"><div class="titlepage"><div><div><h4 class="title" id="sec.filesystems.major.reiser.reliability"><span class="number">1.2.6.4 </span><span class="name">Reliability through Data Journaling</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.major.reiser.reliability">#</a></h4></div></div></div><p>
     ReiserFS also supports data journaling and ordered data modes similar
     to the concepts outlined in
     <a class="xref" href="cha.filesystems.html#sec.filesystems.major.ext3" title="1.2.4. Ext3">Section 1.2.4, “Ext3”</a>.
     The default mode is <code class="literal">data=ordered</code>, which ensures both
     data and metadata integrity, but uses journaling only for metadata.
    </p></div></div></div><div class="sect1 " id="sec.filesystems.other"><div class="titlepage"><div><div><h2 class="title" id="sec.filesystems.other"><span class="number">1.3 </span><span class="name">Other Supported File Systems</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.other">#</a></h2></div></div></div><p class="intro">
   <a class="xref" href="cha.filesystems.html#tab.filesystems.other" title="File System Types in Linux">Table 1.1, “File System Types in Linux”</a> summarizes
   some other file systems supported by Linux. They are supported mainly to
   ensure compatibility and interchange of data with different kinds of
   media or foreign operating systems.
  </p><div class="table" id="tab.filesystems.other"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 1.1: </span><span class="name">File System Types in Linux </span><a title="Permalink" class="permalink" href="cha.filesystems.html#tab.filesystems.other">#</a></h6></div><div class="table-contents"><table summary="File System Types in Linux" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        File System Type
       </p>
      </th><th>
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        <code class="systemitem">cramfs</code>
       </p>
      </td><td>
       <p>
        Compressed ROM file system: A compressed read-only file system for
        ROMs.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">hpfs</code>
       </p>
      </td><td>
       <p>
        High Performance File System: The IBM OS/2 standard file system.
        Only supported in read-only mode.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">iso9660</code>
       </p>
      </td><td>
       <p>
        Standard file system on CD-ROMs.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">minix</code>
       </p>
      </td><td>
       <p>
        This file system originated from academic projects on operating
        systems and was the first file system used in Linux. Today, it is
        used as a file system for floppy disks.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">msdos</code>
       </p>
      </td><td>
       <p>
        <code class="filename">fat</code>, the file system originally used by DOS, is
        today used by various operating systems.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">nfs</code>
       </p>
      </td><td>
       <p>
        Network File System: Here, data can be stored on any machine in a
        network and access might be granted via a network.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">ntfs</code>
       </p>
      </td><td>
       <p>
        Windows NT file system; read-only.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">smbfs</code>
       </p>
      </td><td>
       <p>
        Server Message Block is used by products such as Windows to enable
        file access over a network.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">sysv</code>
       </p>
      </td><td>
       <p>
        Used on SCO Unix, Xenix, and Coherent (commercial Unix systems for
        PCs).
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">ufs</code>
       </p>
      </td><td>
       <p>
        Used by BSD, SunOS, and NextStep. Only supported in read-only mode.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">umsdos</code>
       </p>
      </td><td>
       <p>
        Unix on MS-DOS: Applied on top of a standard
        <code class="filename">fat</code> file system, achieves Unix functionality
        (permissions, links, long file names) by creating special files.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">vfat</code>
       </p>
      </td><td>
       <p>
        Virtual FAT: Extension of the <code class="literal">fat</code> file system
        (supports long file names).
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1 " id="sec.filesystems.lfs"><div class="titlepage"><div><div><h2 class="title" id="sec.filesystems.lfs"><span class="number">1.4 </span><span class="name">Large File Support in Linux</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec.filesystems.lfs">#</a></h2></div></div></div><p>
   Originally, Linux supported a maximum file size of 2 GiB
   (2<sup>31</sup> bytes). Unless a file system comes with
   large file support, the maximum file size on a 32-bit system is 2 GiB.
  </p><p>
   Currently, all of our standard file systems have LFS (large file
   support), which gives a maximum file size of
   2<sup>63</sup> bytes in theory.
   <a class="xref" href="cha.filesystems.html#tab.filesystems.maxsize" title="Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)">Table 1.2, “Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)”</a> offers an
   overview of the current on-disk format limitations of Linux files and
   file systems. The numbers in the table assume that the file systems are
   using 4 KiB block size, which is a common standard. When using different
   block sizes, the results are different. The maximum file sizes in
   <a class="xref" href="cha.filesystems.html#tab.filesystems.maxsize" title="Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)">Table 1.2, “Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)”</a> can be
   larger than the file system's actual size when using sparse blocks.
  </p><div id="idm139647990254080" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Binary Multiples</h6><p>
    In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1
    GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB (see also
    <a class="link" href="http://physics.nist.gov/cuu/Units/binary.html" target="_blank"><em class="citetitle ">NIST:
    Prefixes for Binary Multiples</em></a>.
   </p></div><div class="table" id="tab.filesystems.maxsize"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 1.2: </span><span class="name">Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size) </span><a title="Permalink" class="permalink" href="cha.filesystems.html#tab.filesystems.maxsize">#</a></h6></div><div class="table-contents"><table summary="Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
       <p>
        File System (4 KiB Block Size)
       </p>
      </th><th>
       <p>
        Maximum File System Size
       </p>
      </th><th>
       <p>
        Maximum File Size
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        Btrfs
       </p>
      </td><td>
       <p>
        16 EiB
       </p>
      </td><td>
       <p>
        16 EiB
       </p>
      </td></tr><tr><td>
       <p>
        Ext3
       </p>
      </td><td>
       <p>
        16 TiB
       </p>
      </td><td>
       <p>
        2 TiB
       </p>
      </td></tr><tr><td>
       <p>
        Ext4
       </p>
      </td><td>
       <p>
        1 EiB
       </p>
      </td><td>
       <p>
        16 TiB
       </p>
      </td></tr><tr><td>
       <p>
        OCFS2 (a cluster-aware file system available in the High
        Availability Extension)
       </p>
      </td><td>
       <p>
        16 TiB
       </p>
      </td><td>
       <p>
        1 EiB
       </p>
      </td></tr><tr><td>
       <p>
        ReiserFS v3.6
       </p>
      </td><td>
       <p>
        16 TiB
       </p>
      </td><td>
       <p>
        1 EiB
       </p>
      </td></tr><tr><td>
       <p>
        XFS
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td></tr><tr><td>
       <p>
        NFSv2 (client side)
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td><td>
       <p>
        2 GiB
       </p>
      </td></tr><tr><td>
       <p>
        NFSv3 (client side)
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td><td>
       <p>
        8 EiB
       </p>
      </td></tr></tbody></table></div></div><div id="idm139647990079152" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Limitations</h6><p>
    <a class="xref" href="cha.filesystems.html#tab.filesystems.maxsize" title="Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)">Table 1.2, “Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)”</a>
    describes the limitations regarding the on-disk format. The Linux kernel
    imposes its own limits on the size of files and file systems handled by
    it. These are as follows:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm139647990076672"><span class="term ">File Size</span></dt><dd><p>
       On 32-bit systems, files cannot exceed 2 TiB
       (2<sup>41</sup> bytes).
      </p></dd><dt id="idm139647990074464"><span class="term ">File System Size</span></dt><dd><p>
       File systems can be up to 2<sup>73</sup> bytes in
       size. However, this limit is still out of reach for the currently
       available hardware.
      </p></dd></dl></div></div></div><div class="sect1 " id="sect.filesystems.stor_limits"><div class="titlepage"><div><div><h2 class="title" id="sect.filesystems.stor_limits"><span class="number">1.5 </span><span class="name">Linux Kernel Storage Limitations</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.stor_limits">#</a></h2></div></div></div><p>
   <a class="xref" href="cha.filesystems.html#tab.filesystems.stor_limits" title="Storage Limitations">Table 1.3, “Storage Limitations”</a>
   summarizes the kernel limits for storage associated with
   <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span>.
  </p><div class="table" id="tab.filesystems.stor_limits"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 1.3: </span><span class="name">Storage Limitations </span><a title="Permalink" class="permalink" href="cha.filesystems.html#tab.filesystems.stor_limits">#</a></h6></div><div class="table-contents"><table summary="Storage Limitations" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        Storage Feature
       </p>
      </th><th>
       <p>
        Limitation
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        Maximum number of LUNs supported
       </p>
      </td><td>
       <p>
        16384 LUNs per target.
       </p>
      </td></tr><tr><td>
       <p>
        Maximum number of paths per single LUN
       </p>
      </td><td>
       <p>
        No limit by default. Each path is treated as a normal LUN.
       </p>
       <p>
        The actual limit is given by the number of LUNs per target and the
        number of targets per HBA (16777215 for a Fibre Channel HBA).
       </p>
      </td></tr><tr><td>
       <p>
        Maximum number of HBAs
       </p>
      </td><td>
       <p>
        Unlimited. The actual limit is determined by the amount of PCI slots
        of the system.
       </p>
      </td></tr><tr><td>
       <p>
        Maximum number of paths with device-mapper-multipath (in total) per
        operating system
       </p>
      </td><td>
       <p>
        Approximately 1024. The actual number depends on the length of the
        device number strings. It is a compile-time variable within
        multipath-tools, which can be raised if this limit poses a problem.
       </p>
      </td></tr><tr><td>
       <p>
        Maximum size per block device
       </p>
      </td><td>
       <p>
        Up to 8 EiB.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1 " id="sect.filesystems.trouble"><div class="titlepage"><div><div><h2 class="title" id="sect.filesystems.trouble"><span class="number">1.6 </span><span class="name">Troubleshooting File Systems</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.trouble">#</a></h2></div></div></div><p>
   This section describes some known issues and possible solutions for file
   systems.
  </p><div class="sect2 " id="sect.filesystems.trouble.btrfs_volfull"><div class="titlepage"><div><div><h3 class="title" id="sect.filesystems.trouble.btrfs_volfull"><span class="number">1.6.1 </span><span class="name">Btrfs Error: No space is left on device</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.trouble.btrfs_volfull">#</a></h3></div></div></div><p>
    The root (<code class="filename">/</code>) partition using the Btrfs file system
    stops accepting data. You receive the error <span class="quote">“<span class="quote"><code class="literal">No space
    left on device</code></span>”</span>.
   </p><p>
    See the following sections for information about possible causes and
    prevention of this issue.
   </p><div class="sect3 " id="sect.filesystems.trouble.btrfs_volfull.snapshots"><div class="titlepage"><div><div><h4 class="title" id="sect.filesystems.trouble.btrfs_volfull.snapshots"><span class="number">1.6.1.1 </span><span class="name">Disk Space Consumed by Snapper Snapshots</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.trouble.btrfs_volfull.snapshots">#</a></h4></div></div></div><p>
     If Snapper is running for the Btrfs file system, the <span class="quote">“<span class="quote"><code class="literal">No
     space left on device</code></span>”</span> problem is typically caused by
     having too much data stored as snapshots on your system.
    </p><p>
     You can remove some snapshots from Snapper, however, the snapshots are
     not deleted immediately and might not free up as much space as you
     need.
    </p><p>
     To delete files from Snapper:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Open a terminal console.
      </p></li><li class="step "><p>
       At the command prompt, enter <code class="command">btrfs filesystem
       show</code>, for example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo btrfs filesystem show
Label: none uuid: 40123456-cb2c-4678-8b3d-d014d1c78c78
 Total devices 1 FS bytes used 20.00GB
 devid 1 size 20.00GB used 20.00GB path /dev/sda3</pre></div></li><li class="step "><p>
       Enter
      </p><div class="verbatim-wrap"><pre class="screen">sudo btrfs fi balance start <em class="replaceable ">mountpoint</em> -dusage=5</pre></div><p>
       This command attempts to relocate data in empty or near-empty data
       chunks, allowing the space to be reclaimed and reassigned to
       metadata. This can take a while (many hours for 1 TB) although the
       system is otherwise usable during this time.
      </p></li><li class="step "><p>
       List the snapshots in Snapper. Enter
      </p><div class="verbatim-wrap"><pre class="screen">sudo snapper -c root list</pre></div></li><li class="step "><p>
       Delete one or more snapshots from Snapper. Enter
      </p><div class="verbatim-wrap"><pre class="screen">sudo snapper -c root delete <em class="replaceable ">snapshot_number(s)</em></pre></div><p>
       Ensure that you delete the oldest snapshots first. The older a
       snapshot is, the more disk space it occupies.
      </p></li></ol></div></div><p>
     To help prevent this problem, you can change the Snapper cleanup
     algorithms. See <span class="intraxref">Book “<em class="citetitle ">Administration Guide</em>”, Chapter 3 “System Recovery and Snapshot Management with Snapper”, Section 3.5.1.2 “Cleanup-algorithms”</span>
     for details. The configuration values controlling snapshot cleanup are
     <code class="envar">EMPTY_*</code>, <code class="envar">NUMBER_*</code>, and
     <code class="envar">TIMELINE_*</code>.
    </p><p>
     If you use Snapper with Btrfs on the file system disk, it is advisable
     to reserve twice the amount of disk space than the standard storage
     proposal. The YaST Partitioner automatically proposes twice the
     standard disk space in the Btrfs storage proposal for the root file
     system.
    </p></div><div class="sect3 " id="sect.filesystems.trouble.btrfs_volfull.var"><div class="titlepage"><div><div><h4 class="title" id="sect.filesystems.trouble.btrfs_volfull.var"><span class="number">1.6.1.2 </span><span class="name">Disk Space Consumed by Log, Crash, and Cache Files</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.trouble.btrfs_volfull.var">#</a></h4></div></div></div><p>
     If the system disk is filling up with data, you can try deleting files
     from <code class="filename">/var/log</code>, <code class="filename">/var/crash</code>,
     and <code class="filename">/var/cache</code>.
    </p><p>
     The Btrfs <code class="systemitem">root</code> file system subvolumes
     <code class="filename">/var/log</code>, <code class="filename">/var/crash</code> and
     <code class="filename">/var/cache</code> can use all of the available disk space
     during normal operation, and cause a system malfunction. To help avoid
     this situation, <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers Btrfs quota support for
     subvolumes. See <a class="xref" href="cha.filesystems.html#sec.filesystems.major.btrfs.quota" title="1.2.1.5. Btrfs Quota Support for Subvolumes">Section 1.2.1.5, “Btrfs Quota Support for Subvolumes”</a> for
     details.
    </p></div></div><div class="sect2 " id="sect.filesystems.trouble.trim"><div class="titlepage"><div><div><h3 class="title" id="sect.filesystems.trouble.trim"><span class="number">1.6.2 </span><span class="name">Freeing Unused Filesystem Blocks</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sect.filesystems.trouble.trim">#</a></h3></div></div></div><p>
    On solid state drives (SSDs) and thinly provisioned volumes it is useful
    to trim blocks not in use by the file system. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> fully supports
    <code class="literal">unmap</code> or <code class="literal">trim</code> operations on all file
    systems supporting these methods.
   </p><p>
    The recommended way to trim a supported file system on <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> is to
    run <code class="command">/sbin/wiper.sh</code>. Make sure to read
    <code class="filename">/usr/share/doc/packages/hdparm/README.wiper</code> before
    running this script. For most desktop and server systems the sufficient
    trimming frequency is once a week. Mounting a file system with <code class="option">-o
    discard</code> comes with a performance penalty and may negatively
    affect the lifetime of SSDs and is not recommended.
   </p></div></div><div class="sect1 " id="sec_filesystems_info"><div class="titlepage"><div><div><h2 class="title" id="sec_filesystems_info"><span class="number">1.7 </span><span class="name">Additional Information</span> <a title="Permalink" class="permalink" href="cha.filesystems.html#sec_filesystems_info">#</a></h2></div></div></div><p>
   Each of the file system projects described above maintains its own home
   page on which to find mailing list information, further documentation,
   and FAQs:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Btrfs Wiki on Kernel.org:
     <a class="link" href="https://btrfs.wiki.kernel.org/" target="_blank">https://btrfs.wiki.kernel.org/</a>
    </p></li><li class="listitem "><p>
     E2fsprogs: Ext2/3/4 File System Utilities:
     <a class="link" href="http://e2fsprogs.sourceforge.net/" target="_blank">http://e2fsprogs.sourceforge.net/</a>
    </p></li><li class="listitem "><p>
     Introducing Ext3:
     <a class="link" href="http://www.ibm.com/developerworks/linux/library/l-fs7/" target="_blank">http://www.ibm.com/developerworks/linux/library/l-fs7/</a>
    </p></li><li class="listitem "><p>
     XFS: A High-Performance Journaling Filesytem:
     <a class="link" href="http://oss.sgi.com/projects/xfs/" target="_blank">http://oss.sgi.com/projects/xfs/</a>
    </p></li><li class="listitem "><p>
     The OCFS2 Project:
     <a class="link" href="http://oss.oracle.com/projects/ocfs2/" target="_blank">http://oss.oracle.com/projects/ocfs2/</a>
    </p></li></ul></div><p>
   A comprehensive multi-part tutorial about Linux file systems can be found
   at IBM developerWorks in the <em class="citetitle ">Advanced File System
   Implementor’s Guide</em>
   (<a class="link" href="https://www.ibm.com/developerworks/linux/library/l-fs/" target="_blank">https://www.ibm.com/developerworks/linux/library/l-fs/</a>).
  </p><p>
   An in-depth comparison of file systems (not only Linux file systems) is
   available from the Wikipedia project in Comparison of File Systems
   (<a class="link" href="http://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison" target="_blank">http://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison</a>).
  </p></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha.resize_fs.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 2 </span>Resizing File Systems</span></a><a class="nav-link" href="part.filesystems.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Part I </span>File Systems and Mounting</span></a></div><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>