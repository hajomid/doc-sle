<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Software RAID Configuration | Storage Administration Guide | SUSE Linux Enterprise Server 12 SP2</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 2.4.0 using SUSE XSL Stylesheets 2.0.8 (based on DocBook XSL Stylesheets 1.78.1) - chunked" /><meta name="product-name" content="SUSE Linux Enterprise Server" /><meta name="product-number" content="12 SP2" /><meta name="book-title" content="Storage Administration Guide" /><meta name="chapter-title" content="Chapter 7. Software RAID Configuration" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="fs@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE Linux Enterprise Server 12 SP2" /><link rel="home" href="index.html" title="SUSE Linux Enterprise Server Documentation" /><link rel="up" href="part.software_raid.html" title="Part III. Software RAID" /><link rel="prev" href="part.software_raid.html" title="Part III. Software RAID" /><link rel="next" href="cha.raidroot.html" title="Chapter 8. Configuring Software RAID for the Root Partition" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="stor_admin.html">Storage Administration Guide</a><span> › </span><a class="crumb" href="part.software_raid.html">Software RAID</a><span> › </span><a class="crumb" href="cha.raid.html">Software RAID Configuration</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Storage Administration Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="storage.preface.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="part.filesystems.html"><span class="number">I </span><span class="name">File Systems and Mounting</span></a><ol><li class="inactive"><a href="cha.filesystems.html"><span class="number">1 </span><span class="name">Overview of File Systems in Linux</span></a></li><li class="inactive"><a href="cha.resize_fs.html"><span class="number">2 </span><span class="name">Resizing File Systems</span></a></li><li class="inactive"><a href="cha.uuid.html"><span class="number">3 </span><span class="name">Using UUIDs to Mount Devices</span></a></li><li class="inactive"><a href="cha.multitiercache.html"><span class="number">4 </span><span class="name">Multi-tier Caching for Block Device Operations</span></a></li></ol></li><li class="inactive"><a href="part.lvm.html"><span class="number">II </span><span class="name">Logical Volumes (LVM)</span></a><ol><li class="inactive"><a href="cha.lvm.html"><span class="number">5 </span><span class="name">LVM Configuration</span></a></li><li class="inactive"><a href="cha.lvm_snapshots.html"><span class="number">6 </span><span class="name">LVM Volume Snapshots</span></a></li></ol></li><li class="inactive"><a href="part.software_raid.html"><span class="number">III </span><span class="name">Software RAID</span></a><ol><li class="inactive"><a href="cha.raid.html"><span class="number">7 </span><span class="name">Software RAID Configuration</span></a></li><li class="inactive"><a href="cha.raidroot.html"><span class="number">8 </span><span class="name">Configuring Software RAID for the Root Partition</span></a></li><li class="inactive"><a href="cha.raid10.html"><span class="number">9 </span><span class="name">Creating Software RAID 10 Devices</span></a></li><li class="inactive"><a href="cha.raid_degraded.html"><span class="number">10 </span><span class="name">Creating a Degraded RAID Array</span></a></li><li class="inactive"><a href="cha.raid_resize.html"><span class="number">11 </span><span class="name">Resizing Software RAID Arrays with mdadm</span></a></li><li class="inactive"><a href="cha.raid_leds.html"><span class="number">12 </span><span class="name">Storage Enclosure LED Utilities for MD Software RAIDs</span></a></li></ol></li><li class="inactive"><a href="part.net_storage.html"><span class="number">IV </span><span class="name">Network Storage</span></a><ol><li class="inactive"><a href="cha.isns.html"><span class="number">13 </span><span class="name">iSNS for Linux</span></a></li><li class="inactive"><a href="cha.iscsi.html"><span class="number">14 </span><span class="name">Mass Storage over IP Networks: iSCSI</span></a></li><li class="inactive"><a href="cha.fcoe.html"><span class="number">15 </span><span class="name">Fibre Channel Storage over Ethernet Networks: FCoE</span></a></li><li class="inactive"><a href="cha.multipath.html"><span class="number">16 </span><span class="name">Managing Multipath I/O for Devices</span></a></li><li class="inactive"><a href="cha.nfs4_acls.html"><span class="number">17 </span><span class="name">Managing Access Control Lists over NFSv4</span></a></li></ol></li><li class="inactive"><a href="storage.docupdates.html"><span class="number">A </span><span class="name">Documentation Updates</span></a></li><li class="inactive"><a href="bk08apb.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Part III. Software RAID" href="part.software_raid.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 8. Configuring Software RAID for the Root Partition" href="cha.raidroot.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="SUSE Linux Enterprise Server Documentation"><span class="book-icon">SUSE Linux Enterprise Server Documentation</span></a><span> › </span><a class="crumb" href="stor_admin.html">Storage Administration Guide</a><span> › </span><a class="crumb" href="part.software_raid.html">Software RAID</a><span> › </span><a class="crumb" href="cha.raid.html">Software RAID Configuration</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Part III. Software RAID" href="part.software_raid.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 8. Configuring Software RAID for the Root Partition" href="cha.raidroot.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="chapter " id="cha.raid" lang="en"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span></span> <span class="productnumber"><span class="productnumber"><span class="phrase">12 SP2</span></span></span></div><div><h2 class="title"><span class="number">7 </span><span class="name">Software RAID Configuration</span> </h2><div class="doc-status"><ul><li><span class="ds-label">Filename: </span>storage_raid.xml</li><li><span class="ds-label">ID: </span>cha.raid</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha.raid.html#sec.raid.intro"><span class="number">7.1 </span><span class="name">Understanding RAID Levels</span></a></span></dt><dt><span class="sect1"><a href="cha.raid.html#sec.raid.yast"><span class="number">7.2 </span><span class="name">Soft RAID Configuration with YaST</span></a></span></dt><dt><span class="sect1"><a href="cha.raid.html#sec.raid.trouble"><span class="number">7.3 </span><span class="name">Troubleshooting Software RAIDs</span></a></span></dt><dt><span class="sect1"><a href="cha.raid.html#sec.raid.more"><span class="number">7.4 </span><span class="name">For More Information</span></a></span></dt></dl></div></div><p>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </p><div id="idm140028086296752" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: RAID on Cluster File Systems</h6><p>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the High Availability Extension documentation at
   <a class="link" href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_cluster-md.html" target="_blank">https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_cluster-md.html</a>.
  </p></div><p>
  SUSE Linux Enterprise offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <span class="emphasis"><em>RAID levels</em></span>.
 </p><div class="sect1 " id="sec.raid.intro"><div class="titlepage"><div><div><h2 class="title" id="sec.raid.intro"><span class="number">7.1 </span><span class="name">Understanding RAID Levels</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro">#</a></h2></div></div></div><p>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </p><div class="sect2 " id="sec.raid.intro.raid0"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid0"><span class="number">7.1.1 </span><span class="name">RAID 0</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid0">#</a></h3></div></div></div><p>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not really a
    RAID, because it does not provide data backup, but the name
    <span class="emphasis"><em>RAID 0</em></span> for this type of system has become the
    norm. With RAID 0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </p></div><div class="sect2 " id="sec.raid.intro.raid1"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid1"><span class="number">7.1.2 </span><span class="name">RAID 1</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid1">#</a></h3></div></div></div><p>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <span class="emphasis"><em>hard disk
    mirroring</em></span>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </p></div><div class="sect2 " id="sec.raid.intro.raid23"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid23"><span class="number">7.1.3 </span><span class="name">RAID 2 and RAID 3</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid23">#</a></h3></div></div></div><p>
    These are not typical RAID implementations. Level 2 stripes data at
    the bit level rather than the block level. Level 3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </p></div><div class="sect2 " id="sec.raid.intro.raid4"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid4"><span class="number">7.1.4 </span><span class="name">RAID 4</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid4">#</a></h3></div></div></div><p>
    Level 4 provides block-level striping like Level 0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level 4 is sometimes used.
   </p></div><div class="sect2 " id="sec.raid.intro.raid5"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid5"><span class="number">7.1.5 </span><span class="name">RAID 5</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid5">#</a></h3></div></div></div><p>
    RAID 5 is an optimized compromise between Level 0 and
    Level 1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID 0. <span class="emphasis"><em>Parity blocks</em></span>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID 5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </p></div><div class="sect2 " id="sec.raid.intro.raid6"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid6"><span class="number">7.1.6 </span><span class="name">RAID 6</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid6">#</a></h3></div></div></div><p>
    RAID 6 is essentially an extension of RAID 5 that allows for
    additional fault tolerance by using a second independent distributed parity
    scheme (dual parity). Even if two of the hard disks fail during the data
    recovery process, the system continues to be operational, with no data
    loss.
   </p><p>
    RAID 6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </p><p>
    The performance for RAID 6 is slightly lower but comparable to
    RAID 5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID 6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </p><div class="table" id="idm140028086275808"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 7.1: </span><span class="name">Comparison of RAID 5 and RAID 6 </span><a title="Permalink" class="permalink" href="cha.raid.html#idm140028086275808">#</a></h6></div><div class="table-contents"><table summary="Comparison of RAID 5 and RAID 6" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Feature
        </p>
       </th><th>
        <p>
         RAID 5
        </p>
       </th><th>
        <p>
         RAID 6
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         Number of devices
        </p>
       </td><td>
        <p>
         N+1, minimum of 3
        </p>
       </td><td>
        <p>
         N+2, minimum of 4
        </p>
       </td></tr><tr><td>
        <p>
         Parity
        </p>
       </td><td>
        <p>
         Distributed, single
        </p>
       </td><td>
        <p>
         Distributed, dual
        </p>
       </td></tr><tr><td>
        <p>
         Performance
        </p>
       </td><td>
        <p>
         Medium impact on write and rebuild
        </p>
       </td><td>
        <p>
         More impact on sequential write than RAID 5
        </p>
       </td></tr><tr><td>
        <p>
         Fault-tolerance
        </p>
       </td><td>
        <p>
         Failure of one component device
        </p>
       </td><td>
        <p>
         Failure of two component devices
        </p>
       </td></tr></tbody></table></div></div></div><div class="sect2 " id="sec.raid.intro.raid_nested"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.intro.raid_nested"><span class="number">7.1.7 </span><span class="name">Nested and Complex RAID Levels</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.intro.raid_nested">#</a></h3></div></div></div><p>
    Several other RAID levels have been developed, such as RAIDn, RAID 10,
    RAID 0+1, RAID 30, and RAID 50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID 10 configurations can be found in <a class="xref" href="cha.raid10.html" title="Chapter 9. Creating Software RAID 10 Devices">Chapter 9, <em>Creating Software RAID 10 Devices</em></a>.
   </p></div></div><div class="sect1 " id="sec.raid.yast"><div class="titlepage"><div><div><h2 class="title" id="sec.raid.yast"><span class="number">7.2 </span><span class="name">Soft RAID Configuration with YaST</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.yast">#</a></h2></div></div></div><p>
   The YaST soft RAID configuration can be reached from the YaST Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID 10 configurations is explained in <a class="xref" href="cha.raid10.html" title="Chapter 9. Creating Software RAID 10 Devices">Chapter 9, <em>Creating Software RAID 10 Devices</em></a>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Launch YaST and open the <span class="guimenu">Partitioner</span>.
    </p></li><li class="step "><p>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <span class="guimenu">0xFD Linux RAID</span>. When using existing partitions it is
     not necessary to change their partition type—YaST will
     automatically do so. Refer to <span class="intraxref">Book “<em class="citetitle ">Deployment Guide</em>”, Chapter 11 “Advanced Disk Setup”, Section 11.1 “Using the YaST Partitioner”</span>
     for details.
    </p><p>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID 1
     and 5) and to optimize the performance of RAID 0.
    </p><p>
     For RAID 0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID 5. A RAID 6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </p></li><li class="step "><p>
     In the left panel, select <span class="guimenu">RAID</span>.
    </p><p>
     A list of existing RAID configurations opens in the right panel.
    </p></li><li class="step "><p>
     At the lower left of the RAID page, click <span class="guimenu">Add RAID</span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/yast2_raid2_a.png"><img src="images/yast2_raid2_a.png" width="" /></a></div></div></li><li class="step "><p>
     Select a <span class="guimenu">RAID Type</span> and <span class="guimenu">Add</span> an
     appropriate number of partitions from the <span class="guimenu">Available
     Devices</span> dialog.
    </p><p>
     You can optionally assign a <span class="guimenu">RAID Name</span> to your RAID. It
     will make it available as
     <code class="filename">/dev/md/<em class="replaceable ">name</em></code>. See
     <a class="xref" href="cha.raid.html#sec.raid.yast.names" title="7.2.1. RAID Names">Section 7.2.1, “RAID Names”</a> for more information.
    </p><div class="figure" id="fig.yast2.raid3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast2_raid3_a.png"><img src="images/yast2_raid3_a.png" width="" alt="Example RAID 5 Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.1: </span><span class="name">Example RAID 5 Configuration </span><a title="Permalink" class="permalink" href="cha.raid.html#fig.yast2.raid3">#</a></h6></div></div><p>
     Proceed with <span class="guimenu">Next</span>.
    </p></li><li class="step "><p>
     Select the <span class="guimenu">Chunk Size</span> and, if applicable, the
     <span class="guimenu">Parity Algorithm</span>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <a class="link" href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes" target="_blank">https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes</a>
     for more information. More information on parity algorithms can be found
     with <code class="command">man 8 mdadm</code> when searching for the
     <code class="option">--layout</code> option. If unsure, stick with the defaults.
    </p></li><li class="step "><p>
     Choose a <span class="guimenu">Role</span> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <span class="guimenu">Raw Volume
     (Unformatted)</span>.
    </p></li><li class="step "><p>
     Under <span class="guimenu">Formatting Options</span>, select <span class="guimenu">Format
     Partition</span>, then select the <span class="guimenu">File system</span>. The
     content of the <span class="guimenu">Options</span> menu depends on the file system.
     Usually there is no need to change the defaults.
    </p><p>
     Under <span class="guimenu">Mounting Options</span>, select <span class="guimenu">Mount
     partition</span>, then select the mount point. Click <span class="guimenu">Fstab
     Options</span> to add special mounting options for the volume.
    </p></li><li class="step "><p>
     Click <span class="guimenu">Finish</span>.
    </p></li><li class="step "><p>
     Click <span class="guimenu">Next</span>, verify that the changes are listed, then
     click <span class="guimenu">Finish</span>.
    </p></li></ol></div></div><div class="sect2 " id="sec.raid.yast.names"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.yast.names"><span class="number">7.2.1 </span><span class="name">RAID Names</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.yast.names">#</a></h3></div></div></div><p>
    By default, software RAID devices have numeric names following the pattern
    <code class="literal">mdN</code>, where <code class="literal">N</code> is a number. As such
    they can be accessed as, for example, <code class="filename">/dev/md127</code> and
    are listed as <code class="literal">md127</code> in <code class="filename">/proc/mdstat</code>
    and <code class="filename">/proc/partitions</code>. Working with these names can be
    clumsy. <span class="productname"><span class="phrase">SUSE Linux Enterprise Server</span></span> offers two ways to work around this problem:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm140028086205296"><span class="term ">Providing a Named Link to the Device</span></dt><dd><p>
       You can optionally specify a name for the RAID device when creating it
       with YaST or on the command line with <code class="command">mdadm --create
       '/dev/md/</code> <em class="replaceable ">name</em>'. The device name
       will still be <code class="literal">mdN</code>, but a link
       <code class="filename">/dev/md/<em class="replaceable ">name</em></code> will be
       created:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</pre></div><p>
       The device will still be listed as <code class="literal">md127</code> under
       <code class="filename">/proc</code>.
      </p></dd><dt id="idm140028086199072"><span class="term ">Providing a Named Device</span></dt><dd><p>
       In case a named link to the device is not sufficient for your setup, add
       the line CREATE names=yes to <code class="filename">/etc/mdadm.conf</code> by
       running the following command:
      </p><div class="verbatim-wrap"><pre class="screen">sudo echo "CREATE names=yes" &gt;&gt; /etc/mdadm.conf</pre></div><p>
       It will cause names like <code class="literal">myRAID</code> to be used as a
       <span class="quote">“<span class="quote">real</span>”</span> device name. The device will not only be accessible
       at <code class="filename">/dev/myRAID</code>, but also be listed as
       <code class="literal">myRAID</code> under <code class="filename">/proc</code>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDS will continue to use the
       <code class="literal">mdN</code> names until they get stopped and re-assembled.
      </p><div id="idm140028086193120" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Incompatible Tools</h6><p>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <code class="literal">mdN</code>, it will fail to
        identify the devices.
       </p></div></dd></dl></div></div></div><div class="sect1 " id="sec.raid.trouble"><div class="titlepage"><div><div><h2 class="title" id="sec.raid.trouble"><span class="number">7.3 </span><span class="name">Troubleshooting Software RAIDs</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.trouble">#</a></h2></div></div></div><p>
   Check the <code class="filename">/proc/mdstat</code> file to find out whether a RAID
   partition has been damaged. If a disk fails, shut down your Linux system and
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <code class="command">mdadm /dev/mdX --add
   /dev/sdX</code>. Replace <code class="literal">X</code> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID 0).
  </p><p>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </p><div class="sect2 " id="sec.raid.trouble.autorecovery"><div class="titlepage"><div><div><h3 class="title" id="sec.raid.trouble.autorecovery"><span class="number">7.3.1 </span><span class="name">Recovery after Failing Disk is Back Again</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.trouble.autorecovery">#</a></h3></div></div></div><p>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Problems with the disk media.
     </p></li><li class="listitem "><p>
      Disk drive controller failure.
     </p></li><li class="listitem "><p>
      Broken connection to the disk.
     </p></li></ul></div><p>
    In the case of the disk media or controller failure, the device needs to be
    replaced or repaired. If a hot-spare was not configured within the RAID,
    then manual intervention is required.
   </p><p>
    In the last case, the failed device can be automatically re-added by the
    <code class="command">mdadm</code> command after the connection is repaired (which
    might be automatic).
   </p><p>
    Because <code class="command">md</code>/<code class="command">mdadm</code> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </p><p>
    Under some circumstances—such as storage devices with the internal
    RAID array— the connection problems are very often the cause of the
    device failure. In such case, you can tell <code class="command">mdadm</code> that it
    is safe to automatically <code class="option">--re-add</code> the device after it
    appears. You can do this by adding the following line to
    <code class="filename">/etc/mdadm.conf</code>:
   </p><div class="verbatim-wrap"><pre class="screen">POLICY action=re-add</pre></div><p>
    Note that the device will be automatically re-added after re-appearing only
    if the <code class="systemitem">udev</code> rules cause <code class="command">mdadm -I
    <em class="replaceable ">disk_device_name</em></code> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </p><p>
    If you want this policy to only apply to some devices and not to the
    others, then the <code class="literal">path=</code> option can be added to the
    <code class="literal">POLICY</code> line in <code class="filename">/etc/mdadm.conf</code> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <code class="command">man 5 mdadm.conf</code>
    for more information.
   </p></div></div><div class="sect1 " id="sec.raid.more"><div class="titlepage"><div><div><h2 class="title" id="sec.raid.more"><span class="number">7.4 </span><span class="name">For More Information</span> <a title="Permalink" class="permalink" href="cha.raid.html#sec.raid.more">#</a></h2></div></div></div><p>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <em class="citetitle ">The Linux RAID wiki</em>:
     <a class="link" href="https://raid.wiki.kernel.org/index.php/Linux_Raid" target="_blank">https://raid.wiki.kernel.org/index.php/Linux_Raid</a>
    </p></li><li class="listitem "><p>
     <em class="citetitle ">The Software RAID HOWTO</em> in the
     <code class="filename">/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</code>
     file
    </p></li></ul></div><p>
   Linux RAID mailing lists are also available, such as
   <em class="citetitle ">linux-raid</em> at
   <a class="link" href="http://marc.info/?l=linux-raid" target="_blank">http://marc.info/?l=linux-raid</a>.
  </p></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="cha.raidroot.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 8 </span>Configuring Software RAID for the Root Partition</span></a><a class="nav-link" href="part.software_raid.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Part III </span>Software RAID</span></a></div><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>